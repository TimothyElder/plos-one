<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="pmc">plosone</journal-id>
      <journal-title-group>
        <journal-title>PLoS ONE</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1932-6203</issn>
      <publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">PONE-D-12-34944</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pone.0059785</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
          </subj-group>
          <subj-group>
            <subject>Zoology</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Anthropology</subject>
          </subj-group>
          <subj-group>
            <subject>Psychology</subject>
            <subj-group>
              <subject>Cognitive psychology</subject>
              <subject>Experimental psychology</subject>
              <subject>Sensory perception</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Head-Mounted Eye Tracking of a Chimpanzee under Naturalistic Conditions</article-title>
        <alt-title alt-title-type="running-head">Head-Mounted Eye-Tracking of a Chimpanzee</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Kano</surname>
            <given-names>Fumihiro</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Tomonaga</surname>
            <given-names>Masaki</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <label>1</label>
        <addr-line>Primate Research Institute, Kyoto University, Inuyama, Aichi, Japan</addr-line>
      </aff>
      <aff id="aff2">
        <label>2</label>
        <addr-line>Japan Society for Promotion of Science, Chiyoda-ku, Tokyo, Japan</addr-line>
      </aff>
      <contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Warrant</surname>
            <given-names>Eric James</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group>
      <aff id="edit1">
        <addr-line>Lund University, Sweden</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">fkanou@pri.kyoto-u.ac.jp</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: FK MT. Performed the experiments: FK MT. Analyzed the data: FK. Contributed reagents/materials/analysis tools: FK MT. Wrote the paper: FK.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <year>2013</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>27</day>
        <month>3</month>
        <year>2013</year>
      </pub-date>
      <volume>8</volume>
      <issue>3</issue>
      <elocation-id>e59785</elocation-id>
      <history>
        <date date-type="received">
          <day>9</day>
          <month>11</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>18</day>
          <month>2</month>
          <year>2013</year>
        </date>
      </history>
      <permissions>
        <copyright-year>2013</copyright-year>
        <copyright-holder>Kano, Tomonaga</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>This study offers a new method for examining the bodily, manual, and eye movements of a chimpanzee at the micro-level. A female chimpanzee wore a lightweight head-mounted eye tracker (60 Hz) on her head while engaging in daily interactions with the human experimenter. The eye tracker recorded her eye movements accurately while the chimpanzee freely moved her head, hands, and body. Three video cameras recorded the bodily and manual movements of the chimpanzee from multiple angles. We examined how the chimpanzee viewed the experimenter in this interactive setting and how the eye movements were related to the ongoing interactive contexts and actions. We prepared two experimentally defined contexts in each session: a face-to-face greeting phase upon the appearance of the experimenter in the experimental room, and a subsequent face-to-face task phase that included manual gestures and fruit rewards. Overall, the general viewing pattern of the chimpanzee, measured in terms of duration of individual fixations, length of individual saccades, and total viewing duration of the experimenter’s face/body, was very similar to that observed in previous eye-tracking studies that used non-interactive situations, despite the differences in the experimental settings. However, the chimpanzee viewed the experimenter and the scene objects differently depending on the ongoing context and actions. The chimpanzee viewed the experimenter’s face and body during the greeting phase, but viewed the experimenter’s face and hands as well as the fruit reward during the task phase. These differences can be explained by the differential bodily/manual actions produced by the chimpanzee and the experimenter during each experimental phase (i.e., greeting gestures, task cueing). Additionally, the chimpanzee’s viewing pattern varied depending on the identity of the experimenter (i.e., the chimpanzee’s prior experience with the experimenter). These methods and results offer new possibilities for examining the natural gaze behavior of chimpanzees.</p>
      </abstract>
      <funding-group>
        <funding-statement>This research was financially supported by the Japan Society for the Promotion of Science (JSPS) and the Ministry of Education, Culture, Sports, Science and Technology (MEXT) under the Japan Grants-in-Aid for Scientific Research (nos. 16002001, 19300091, 20002001, 212299) and the JSPS/MEXT global COE programs (D07 and A06). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
      <counts>
        <page-count count="9"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Human and nonhuman primates rely primarily on vision to retrieve information from the outside world. To retrieve visual information, primates rely on their eyes, especially on the central foveae, which capture less than 2 degrees of the visual field <xref ref-type="bibr" rid="pone.0059785-Land1">[1]</xref>. Thus, primates must actively move their eyes to select only necessary information from the array of information that exists in a real-life environment. Where do they look in such an environment?</p>
      <p>Eye tracking is a technique that accurately measures these eye movements. In nonhuman primates such as macaques, a magnetic search coil method is commonly used for eye tracking <xref ref-type="bibr" rid="pone.0059785-Robinson1">[2]</xref>–<xref ref-type="bibr" rid="pone.0059785-Keating1">[7]</xref>. However, this method requires the coil to be implanted on the eye surface of the subjects and the heads of the subjects to be firmly fixed in place by a chin rest or a bite bar. Thus, due to both ethical and physical constraints, this method is not applied to large primates such as great apes. A recent study solved this problem using a video-based, table-mounted eye tracker, allowing eye tracking without head restraints <xref ref-type="bibr" rid="pone.0059785-Kano1">[8]</xref>. This eye tracker uses wide-angle camera lenses to search for both corneal and pupil reflections from the eyes and compensates for head movements (indicated by the corneal reflection) when calculating eye movement (indicated by the pupil reflection). This same method is commonly used in human infants <xref ref-type="bibr" rid="pone.0059785-Gredebck1">[9]</xref> and, more recently, in dogs <xref ref-type="bibr" rid="pone.0059785-Somppi1">[10]</xref>, <xref ref-type="bibr" rid="pone.0059785-Tgls1">[11]</xref>.</p>
      <p>However, despite its usability, the table-mounted eye-tracking method has an essential limitation. The experimental stimuli, typically 2D images or movies presented on a computer screen, are presented within the visual field of subjects, who are not able to interact with those stimuli. Thus, this method fails to capture the interactive nature of eye movements in a real-life environment. In human adults, video-based, head-mounted (i.e., wearable) eye trackers are used to examine the eye movements of subjects who are freely moving and interacting with real-life environments <xref ref-type="bibr" rid="pone.0059785-Hayhoe1">[12]</xref>. Previous studies have examined eye movements while participants executed various manual tasks, including making tea <xref ref-type="bibr" rid="pone.0059785-Land2">[13]</xref>, making a sandwich <xref ref-type="bibr" rid="pone.0059785-Hayhoe2">[14]</xref>, washing their hands <xref ref-type="bibr" rid="pone.0059785-Pelz1">[15]</xref>, playing cricket <xref ref-type="bibr" rid="pone.0059785-Land3">[16]</xref>, walking <xref ref-type="bibr" rid="pone.0059785-Turano1">[17]</xref>, and driving <xref ref-type="bibr" rid="pone.0059785-Land4">[18]</xref>. These studies have found that subjects fixate only on areas relevant to the task and do so only at the time at which relevant information is required. That is, the eye movements of subjects are goal directed and strictly dependent on the interactive context and the subjects’ actions.</p>
      <p>The same head-mounted eye-tracking method has not been fully developed with nonhuman primates, and thus their goal-directed or natural eye movements are largely unexplored. One study was conducted on lemurs while they moved freely in a cage with conspecifics <xref ref-type="bibr" rid="pone.0059785-Shepherd2">[19]</xref>, <xref ref-type="bibr" rid="pone.0059785-Shepherd3">[20]</xref>. The lemurs followed the gaze of their conspecifics in this real-life environment and showed differential eye-movement patterns when walking compared with being stationary. This head-mounted eye-tracking method has also been used with human infants <xref ref-type="bibr" rid="pone.0059785-Franchak1">[21]</xref> and with dogs <xref ref-type="bibr" rid="pone.0059785-Williams1">[22]</xref> but not with phylogenetically closer animals (i.e., great apes).</p>
      <p>In this study, we aimed to extend this eye-tracking method to a chimpanzee under naturalistic conditions. First, we evaluated the utility, reliability, and limitations of our method. Second, we examined whether our data on general patterns of eye movements (i.e., duration of individual fixations, length of individual saccades, etc.,) were comparable to the results reported in previous studies relying on table-mounted eye tracking (viewing still images) <xref ref-type="bibr" rid="pone.0059785-Kano1">[8]</xref>, <xref ref-type="bibr" rid="pone.0059785-Kano2">[23]</xref>. Third, we examined how a real-life environment and the chimpanzee’s interaction with that environment affected the chimpanzee’s eye-movement patterns. Finally, we explored how the chimpanzee viewed the social stimuli (i.e. the experimenter’s face and body) under this interactive situation.</p>
      <p>In our experiment we used a daily interactive situation that enhanced face-to-face communication between the chimpanzee and the human experimenter. We devised two experimental settings to alter the quality of the interaction between the subject and experimenter: a face-to-face greeting between the experimenter and the chimpanzee occurring when the experimenter initially appears in the experimental room (“greeting phase”) and subsequent task-related interaction involving both the experimenter’s and the chimpanzee’s manual gestures as well as fruit rewards (“task phase”).</p>
      <p>Based on previous head-mounted eye-tracking studies with humans, we expected that the chimpanzee would view the body parts and scene areas most relevant to the ongoing context and actions. That is, during the greeting phase, we expected that the chimpanzee would view the experimenter’s face and body. In the task phase, we expected that the chimpanzee would view task-relevant areas, such as the experimenter’s gestures and the fruit reward.</p>
      <p>With respect to the chimpanzee’s pattern of viewing the experimenter’s face and body, of particular interest was the pattern of viewing the face. Faces contain a rich store of information vital to their social lives, such as identity, emotion, and gaze direction <xref ref-type="bibr" rid="pone.0059785-Parr1">[24]</xref>–<xref ref-type="bibr" rid="pone.0059785-Tomonaga2">[26]</xref>. The previous table-mounted eye-tracking studies found that chimpanzees primarily viewed the face and eyes when exposed to conspecific and human images <xref ref-type="bibr" rid="pone.0059785-Kano1">[8]</xref>, <xref ref-type="bibr" rid="pone.0059785-Hattori1">[27]</xref>–<xref ref-type="bibr" rid="pone.0059785-MyowaYamakoshi1">[29]</xref>. However, prolonged viewing of the face and eyes is less common in chimpanzees than in human subjects. Instead, chimpanzees view the body and mouth more frequently than do humans. These findings may reflect chimpanzees’ habitual communicative style or their limited use of long-bout facial communication, including making eye contact and reading subtle eye expressions and gaze directions. However, previous studies did not include interactions between subjects and stimuli, and thus subjects may have been less motivated to view faces and eyes. Additionally, because the experimental settings (free observation of images) and the presentation duration of stimuli (several seconds) in the previous studies were limited, the manner in which subjects alter the pattern of face viewing as a function of context and time remains unclear. In this study, we examined the extent to which the chimpanzee would view the experimenter’s face in the interactive situation. We then explored how context and actions would modify the chimpanzee’s pattern of viewing faces.</p>
      <p>Additionally, as the quality of interaction depends on a chimpanzee’s relationship to an experimenter, we examined how the chimpanzee altered her viewing pattern depending on the identity of the experimenter. Although the previous table-mounted studies (using still pictures) examined chimpanzees’ viewing patterns for familiar and unfamiliar individuals, they failed to find differential viewing patterns. However, as mentioned above, their results may have been due to the lack of interaction between the chimpanzees and stimuli and to the short presentation duration of stimuli. We thus re-examined this issue in a real-life setting and expected to observe the chimpanzee’s novelty response (longer viewing) upon encountering the unfamiliar experimenter.</p>
    </sec>
    <sec id="s2" sec-type="methods">
      <title>Methods</title>
      <sec id="s2a">
        <title>Subject</title>
        <p>Pan, a female chimpanzee, aged 27 years old, participated in this study. Pan was a member of a social group comprising 13 individuals living in an enriched environment with a 700-m<sup>2</sup> outdoor compound and an attached indoor residence <xref ref-type="bibr" rid="pone.0059785-Matsuzawa1">[30]</xref>. The outdoor compound was equipped with climbing frames, small streams, and various species of trees. Access to the outdoor compound was available to Pan every other day during the day. Daily meals included a wide variety of fresh fruits and vegetables fed throughout the day supplemented with nutritionally balanced biscuits (fed twice daily) and water available <italic>ad libitum</italic>. Pan has been reared by humans and has experienced various cognitive experiments since youth <xref ref-type="bibr" rid="pone.0059785-Hashiya1">[31]</xref>–<xref ref-type="bibr" rid="pone.0059785-Itakura1">[34]</xref>. The care and use of Pan adhered to the 2002 version of the Guidelines for the Care and Use of Laboratory Primates by the Primate Research Institute, Kyoto University. This experimental protocol was approved by the Animal Welfare and Care Committee of the same institute (no. 2010-023). For the daily experiments, Pan left her social group voluntarily on the request of experimenters, moved into the experimental booth with the guidance of experimenters, and moved back to her social group after the completion of experiments (approx. 1 hour).</p>
      </sec>
      <sec id="s2b">
        <title>Monitoring Eye Movements</title>
        <p>Pan’s eye movements were monitored using a commercial head-mounted eye tracker (<xref ref-type="fig" rid="pone-0059785-g001">Fig. 1A</xref>; 60 Hz, “Omniview”, ISCAN Inc., Woburn, MA, USA). This eye tracker has a temporal resolution of 60 Hz and a spatial resolution of &lt;0.25° in recording the eye image (<xref ref-type="fig" rid="pone-0059785-g001">Fig. 1B</xref>). The accuracy of gaze-in-scene position (gaze position with respect to the world) was approximately 0.5° over a central 40° field when the calibration was accurate. Although this eye tracker is able to record both eyes of subjects, we abandoned the left-eye records of Pan due to the relatively lower position of her left eyelid (i.e., less robust to the eccentric eye movements compared with the right eye). Although tracking a single eye is known to cause a parallax error (i.e., a calibration error), especially in the distance of subject’s hand reach, this error was largely irrelevant to the current experiment, which did not include regions of interest in those areas.</p>
        <fig id="pone-0059785-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0059785.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Experimental apparatuses.</title>
            <p>The eye tracker on Pan’s head (A) and the eye (left top) and scene-camera image (B). Cross mark indicates point-of-regard (POR). Also, see <xref ref-type="supplementary-material" rid="pone.0059785.s002">Video S2</xref>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.g001" position="float" xlink:type="simple"/>
        </fig>
        <p>The eye tracker was attached to goggles mounted on her head non-invasively (<xref ref-type="fig" rid="pone-0059785-g001">Fig. 1A</xref>). The goggles were fixed by four strings that passed along the top and sides of her head and were bound at the back of her head with a plastic clip. Due to the higher position of the ears in chimpanzees than in humans, the original temples of the goggles were replaced by wire temples shaped to fit Pan’s ears. Thus, the eye-tracker goggles were supported at multiple points on her head; her nose, the top and back of her head, and her ears. The goggles were immobile during the recording unless she touched them (data from these failed sessions were removed from the analysis; see below). Note that the goggles were fixed to Pan’s head only tightly enough to remain in place and could be removed by Pan herself any time in the session.</p>
        <p>Two eye cameras were attached to the left- and right-top of the goggles, and they recorded the reflection of the eye image in a half mirror. Pupil and first Purkinje image centroids were extracted from the eye image, and eye-in-head position (the eye position with respect to the head) was calculated based on the vector difference between the two centroids. As this vector difference was independent of the absolute coordinates of the two centroids in the eye cameras, the eye-in-head position was robust to small movements of the goggles on the head. The scene camera was attached to the middle of the top of the goggles and provided a video recording (30 Hz) of the scene from her viewpoint (<xref ref-type="fig" rid="pone-0059785-g001">Fig. 1B</xref>; approx. 70×50° field in width and height). All data were stored in a small digital recorder, which was placed on the floor during recording.</p>
        <p>Because Pan did not hesitate to wear the eye tracker, no habituation was necessary. However, to check the accuracy and increase the stability of recordings, we practiced the calibration procedure and conducted preliminary recordings with Pan for several weeks prior to the testing sessions.</p>
      </sec>
      <sec id="s2c">
        <title>Calibration Procedure</title>
        <p>Two experimenters engaged in the calibration session. One remained inside an experimental booth (E1) and set the eye tracker on Pan’s head, and the other (E1′) remained outside of the booth and set the calibration frame. A five-point calibration was conducted each time before the daily session. The calibration points were set in a 58.5×51 cm frame, which was placed outside the booth at 1.5 m from Pan (22×19° in width and height). E1′ attracted her gaze to each calibration point several times by presenting small objects and rewards at that point (See <xref ref-type="supplementary-material" rid="pone.0059785.s001">Video S1</xref>). During the calibration procedure, Pan’s head was lightly held by E1, thereby preventing large head movements during calibration. Once these calibration procedures were finished, her head was set free, and E1 and E1′ moved away from her; E1′ moved completely away from her sight, and E1 remained in the booth but kept distance from her during the test session (<xref ref-type="fig" rid="pone-0059785-g002">Fig. 2a</xref>).</p>
        <fig id="pone-0059785-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0059785.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Experimental procedures.</title>
            <p>(A) Schematic of the experimental setting. (B) Time flow of an experimental session.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.g002" position="float" xlink:type="simple"/>
        </fig>
        <p>Off-line processing of the calibration data was performed on a PC by selecting the location of each calibration point on the scene-camera image and selecting the time at which Pan fixated on the point, as indicated by the eye-camera images (the eye movements that followed the objects/rewards). After processing the calibration data, the ISCAN system provided the point-of-regard (POR or eye-in-scene position), which was superimposed on the scene-camera image (cross mark in <xref ref-type="fig" rid="pone-0059785-g001">Fig. 1B</xref>). Failures in the calibration process were indicated by the loss of POR data on large sections of scene-camera images. The accuracy of the POR (the distance between the POR and the intended region of interest) was typically very small (within 1°) around the calibration surface (at the 1.5-m distance and within the central 40° field) but was larger when the POR was more distant from the surface. We thus conducted the main recordings around that area (see below). As a result, accuracy was around 0–2° during the recordings when estimated based on Pan’s eye movements following small rewards/objects (see <xref ref-type="supplementary-material" rid="pone.0059785.s001">Video S1</xref>).</p>
      </sec>
      <sec id="s2d">
        <title>Testing Procedure</title>
        <p>Experimenter 2 (E2) entered the room, sat on the floor in front of Pan, and gazed at, talked to, and gestured to her (<xref ref-type="fig" rid="pone-0059785-g002">Fig. 2A, 2B</xref>; “greeting phase”). After approximately 1 minute, E2 began the gesture task on which Pan had been trained for several years. This task lasted for approximately 2 minutes (“task phase”). During this task, E2 showed one of three actions to Pan, touching the nose, touching the palm, or clapping the hands, and gave the fruit reward (pieces of apple) when Pan reproduced that action. The fruit rewards were placed in a transparent box next to the experimenter, and the box and rewards were present at the fixed place throughout the entire session (i.e., during both greeting and task phases).</p>
        <p>Six experimenters played the role of E2 across 2 days (total of 12 sessions). The six experimenters differed from one another in the quality of daily interactions with Pan. Four had been interacting with Pan on a daily basis (familiar), and the other two met her for the first time during the experiment (unfamiliar). Of the familiar four, two interacted daily with Pan in the room where this experiment was conducted (regular), but the other two interacted daily with her elsewhere, such as the chimpanzee residential area or another experimental room (irregular). That is, it was unusual for Pan to see the irregular E2 in the room where this experiment was conducted. The types of experimenters were thus termed “regular/familiar,” “irregular/familiar,” and “irregular/unfamiliar”. The order in which the different types of experimenter appeared within each session was randomized within the 12 sessions.</p>
        <p>The entire procedure, including mounting the eye tracker on Pan and conducting the calibration and testing, lasted for 20–30 minutes. The sessions were terminated if Pan showed any signs of distress or if she touched the eye tracker. Sessions in which calibrations failed were eliminated from the analysis and repeated on another day (in total, 14 sessions, including two failed sessions, were conducted).</p>
      </sec>
      <sec id="s2e">
        <title>Eye-movement Analysis</title>
        <p>Lost eye signals (pupil or first Purkinje image) occurring as a result of blinks or downcast gazes amounted to 24% of the total recording time. Fixation was detected off-line based on the instantaneous velocity of the gaze-in-scene position. This velocity was calculated from the gaze-in-scene vector as a combination of the head and eye-in-head vectors. The head vector was calculated by tracking the coordinates of any object in the scene-camera images, and the eye-in-head vector was calculated from the POR coordinates given by the ISCAN system. A fixation (or smooth pursuit) was defined when the gaze-in-scene velocity was &lt;30°/s. A saccade (velocity &gt;30°/s.) shorter than 20 ms (i.e., one sample) and a fixation shorter than 50 ms (i.e., fewer than three samples) were regarded as noise and were integrated into the surrounding fixation and saccade, respectively, in that order. These criteria were chosen to match the POR movements projected on the scene-camera images. All analyses were conducted using MATLAB (<ext-link ext-link-type="uri" xlink:href="http://www.mathworks.co.jp" xlink:type="simple">www.mathworks.co.jp</ext-link>).</p>
        <p>The following dependent variables were analyzed to represent the characteristics of Pan’s eye movements. 1) Viewing time: sum of all fixation durations (ms). 2) Fixation duration: duration of individual fixations (ms). 3) Fixation number: number of fixations. 4) Saccade length: length of individual saccades (degree).</p>
      </sec>
      <sec id="s2f">
        <title>Coding of the Fixation Target and Actions</title>
        <p>The fixation target was manually coded by inspecting the scene-camera image. The scene was divided into E2’s face (above the neck), hands (from the wrist), feet (from the ankle), and other body parts; the reward; and other areas (<xref ref-type="fig" rid="pone-0059785-g002">Fig. 2A</xref>). Inter-coder reliability was checked by another coder naïve to the experimental hypothesis using part of the coding footage (3 min.) and was categorized as excellent (Cohen’s <italic>Kappa</italic> = 0.84). The actions of E2 and Pan were recorded by three fixed cameras (SONY Handicam, <ext-link ext-link-type="uri" xlink:href="http://www.sony.jp" xlink:type="simple">www.sony.jp</ext-link>) aimed at E2 and at Pan’s front and back and coded by inspecting the footage on a frame-by-frame basis. The gaze data (shown in the scene-camera image) and action data (shown in the fixed-camera images) were temporally matched by inspecting the onset and offset of any actions appearing in both images.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <p>Overall, our method enabled us to record Pan’s eye movements over the course of 3 minutes while Pan engaged in her usual interactions with E2; for example, Pan engaged in overt greeting gestures when E2 appeared (pant-grunting or swaying in five of the 12 sessions), and she performed the task actions and occasionally requested the reward after the task began (<xref ref-type="fig" rid="pone-0059785-g003">Fig. 3</xref> and <xref ref-type="table" rid="pone-0059785-t001">Table 1</xref>). During the task phase (approx. 2 min.), E2 produced the task actions an average of 13.6 times (<italic>SD</italic>: 4.1), and Pan reproduced the actions and obtained the reward an average of 9.9 times (<italic>SD</italic>: 3.1).</p>
      <fig id="pone-0059785-g003" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059785.g003</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Examples of Pan’s fixations and Pan’s and E2’s actions as a function of time (s) during (A) greeting and (B) task phases.</title>
          <p>The ticks on the fixation axis indicate the beginning of each fixation.</p>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.g003" position="float" xlink:type="simple"/>
      </fig>
      <table-wrap id="pone-0059785-t001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059785.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Actions observed during the experiments.</title>
        </caption>
        <alternatives>
          <graphic id="pone-0059785-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.t001" xlink:type="simple"/>
          <table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Chimpanzee</td>
                <td align="left" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">sway trunk (greeting gesture)</td>
                <td align="left" rowspan="1" colspan="1">3</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">pant grunt (greeting gesture)</td>
                <td align="left" rowspan="1" colspan="1">2</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">reach hand (request gesture)</td>
                <td align="left" rowspan="1" colspan="1">89</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">short vocalization (request gesture)</td>
                <td align="left" rowspan="1" colspan="1">49</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">clap hand</td>
                <td align="left" rowspan="1" colspan="1">150</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">touch nose</td>
                <td align="left" rowspan="1" colspan="1">99</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">tap palm</td>
                <td align="left" rowspan="1" colspan="1">73</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">reach for reward</td>
                <td align="left" rowspan="1" colspan="1">130</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">guide reward</td>
                <td align="left" rowspan="1" colspan="1">119</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">self-scratch</td>
                <td align="left" rowspan="1" colspan="1">73</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Human interactive partners</td>
                <td align="left" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">open door</td>
                <td align="left" rowspan="1" colspan="1">24</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">walk</td>
                <td align="left" rowspan="1" colspan="1">24</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">sit, stand up</td>
                <td align="left" rowspan="1" colspan="1">24</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">call subject’s name</td>
                <td align="left" rowspan="1" colspan="1">79</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">clap hand</td>
                <td align="left" rowspan="1" colspan="1">45</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">touch nose</td>
                <td align="left" rowspan="1" colspan="1">85</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">tap palm</td>
                <td align="left" rowspan="1" colspan="1">34</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">reach for reward</td>
                <td align="left" rowspan="1" colspan="1">125</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">guide reward</td>
                <td align="left" rowspan="1" colspan="1">124</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>With respect to the basic patterns of Pan’s eye movements (over all sessions), the average velocities of Pan’s head, eye-in-head, and gaze-in-scene movements were 11.9° (<italic>SD</italic>: 25.9), 56.1° (<italic>SD</italic>: 126.6), and 57.5° (<italic>SD</italic>: 130.5), respectively. This pattern indicates that Pan used eye movements more commonly than she used head movements to shift her gaze. The spatial distribution of Pan’s fixations on scene-camera images (i.e., eye-in-head) was shown in <xref ref-type="fig" rid="pone-0059785-g004">Figure 4</xref>. In general, her fixations were clustered in the middle of the horizontal axis and were more widely distributed along the vertical axis. This pattern indicates that Pan used head movements more frequently when shifting her gaze horizontally than when shifting her gaze vertically. The two peaks along the vertical axis largely correspond to the density of objects or body parts in the scene (i.e., a lack of interesting objects or body parts in the middle of the vertical axis; see <xref ref-type="supplementary-material" rid="pone.0059785.s002">Video S2</xref>).</p>
      <fig id="pone-0059785-g004" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059785.g004</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Spatial distribution of fixations on scene-camera images (i.e., eye-in-head).</title>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.g004" position="float" xlink:type="simple"/>
      </fig>
      <p>The temporal distributions of fixation durations (<xref ref-type="fig" rid="pone-0059785-g005">Figure 5A</xref>) and saccade lengths (<xref ref-type="fig" rid="pone-0059785-g005">Figure 5B</xref>) was similar to those in the previous reports about chimpanzees and humans <xref ref-type="bibr" rid="pone.0059785-Land2">[13]</xref>, <xref ref-type="bibr" rid="pone.0059785-Turano1">[17]</xref>, <xref ref-type="bibr" rid="pone.0059785-Kano2">[23]</xref>, <xref ref-type="bibr" rid="pone.0059785-Kano4">[35]</xref>, <xref ref-type="bibr" rid="pone.0059785-Henderson1">[36]</xref>. That is, she exhibited a skewed distribution with a long right tail for the fixation duration and saccade length, with modes around 200–300 ms and 1–5 degrees, respectively. <xref ref-type="table" rid="pone-0059785-t002">Table 2</xref> shows the mean fixation duration and saccade length during each experimental phase. To examine changes in these variables across experimental phases, we conducted <italic>t</italic>-tests (total of 12 sessions) for each variable. We found no significant changes in either variable across the phases (fixation duration: <italic>t</italic>(11) = 1.60, <italic>P</italic> = 0.13; saccade length: <italic>t</italic>(11) = 0.62, <italic>P</italic> = 0.54).</p>
      <fig id="pone-0059785-g005" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059785.g005</object-id>
        <label>Figure 5</label>
        <caption>
          <title>Probability distribution of (A) fixation duration (ms) and (B) saccade length (degree).</title>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.g005" position="float" xlink:type="simple"/>
      </fig>
      <table-wrap id="pone-0059785-t002" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059785.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Fixation duration (ms) and saccade length (degree) during each experimental phase.</title>
        </caption>
        <alternatives>
          <graphic id="pone-0059785-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.t002" xlink:type="simple"/>
          <table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" rowspan="1" colspan="1">Experimental phase</td>
                <td align="left" rowspan="1" colspan="1">Greeting</td>
                <td align="left" rowspan="1" colspan="1">Task</td>
                <td align="left" rowspan="1" colspan="1">Overall</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Fixation duration (s.e.)</td>
                <td align="left" rowspan="1" colspan="1">235 (10.2)</td>
                <td align="left" rowspan="1" colspan="1">213 (9.4)</td>
                <td align="left" rowspan="1" colspan="1">224 (7.0)</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Saccade length (s.e.)</td>
                <td align="left" rowspan="1" colspan="1">13.7 (0.53)</td>
                <td align="left" rowspan="1" colspan="1">14.1 (0.37)</td>
                <td align="left" rowspan="1" colspan="1">13.9 (0.33)</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>Pan altered the viewing time for each scene area as a function of experimental phase and time (10-s time windows; <xref ref-type="fig" rid="pone-0059785-g006">Figure 6</xref>). In general, Pan decreased the time spent viewing E2 over the course of the greeting phase, increased it again when the task began, and decreased it again as time passed. Pan viewed the reward rarely (&lt;1%) during the greeting phase, whereas she viewed it intensely during the task phase (&gt;30%). To examine Pan’s viewing patterns for each of E2’s body parts according to the experimental phase, we compared the two phases (first 50 s) using a repeated-measures analysis of variance (ANOVA) with phase (2) and body parts (4) as factors (total of 12 sessions) and found a significant interaction between the two factors (<italic>F</italic>(3, 9) = 5.75, <italic>P</italic> = 0.018, <italic>η</italic><sup>2</sup> = 0.65). The additional analyses showed a significant effect of body part during both greeting (<italic>F</italic>(3, 9) = 3.93, <italic>P</italic> = 0.048, <italic>η</italic><sup>2</sup> = 0.56) and task (<italic>F</italic>(3, 9) = 4.04, <italic>P</italic> = 0.045, <italic>η</italic><sup>2</sup> = 0.57) phases. Specifically, Pan viewed E2’s face, feet, and other body parts for a particularly long time during the greeting phase and viewed E2’s face and hands for a particularly long time during the task phase.</p>
      <fig id="pone-0059785-g006" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059785.g006</object-id>
        <label>Figure 6</label>
        <caption>
          <title>Viewing time (ms) for each scene area as a function of time during (A) greeting and (B) task phases.</title>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.g006" position="float" xlink:type="simple"/>
      </fig>
      <p>Pan also altered the number and duration of fixations on each of E2’s body parts as a function of experimental phase (note that <italic>viewing time = fixation duration</italic>×<italic>fixation number</italic>; <xref ref-type="fig" rid="pone-0059785-g007">Figure 7</xref>). To examine changes in these parameters across experimental phases, we conducted a repeated-measures ANOVA with phase (2) and body parts (4) as factors (total of 12 sessions). As with viewing time, we found a significant interaction between the two factors with respect to number of fixations (<italic>F</italic>(3, 9) = 7.90, <italic>P</italic> = 0.007, <italic>η</italic><sup>2</sup> = 0.72), whereas no significant interaction was found with respect to fixation duration (<italic>F</italic>(3, 4) = 0.34, <italic>P</italic> = 0.79, <italic>η</italic><sup>2</sup> = 0.20; note that these data include null values due to no fixation to particular body parts in a few sessions). In terms of fixation duration, we found significant main effects of phase (<italic>F</italic>(1, 6) = 8.26, <italic>P</italic> = 0.028, <italic>η</italic><sup>2</sup> = 0.57) and body parts (<italic>F</italic>(3, 4) = 10.83, <italic>P</italic> = 0.022, <italic>η</italic><sup>2</sup> = 0.89), indicating that Pan exhibited longer fixation durations during the greeting than during the task phase and longer fixation durations for the face than for other body parts during both experimental phases.</p>
      <fig id="pone-0059785-g007" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059785.g007</object-id>
        <label>Figure 7</label>
        <caption>
          <title>Average of (A) fixation duration (ms) and (B) saccade length (degree) for each scene area during the greeting and task phases (first 50 s).</title>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.g007" position="float" xlink:type="simple"/>
      </fig>
      <p>Pan’s viewing pattern for E2 was related to her own and to E2’s actions. Pan offered greeting gestures to E2 five times during the greeting phase (<xref ref-type="table" rid="pone-0059785-t001">Table 1</xref>). While performing these gestures, Pan fixated on E2 exclusively (90% of all fixations), especially on E2’s face and feet (43% and 33% on the face and feet, respectively). Additionally, Pan fixated primarily on E2’s feet when E2 was walking (80% of all fixations on E2, 40% on E2’s feet; 14% on E2’s face). During the task phase, Pan fixated primarily on the face and hands when those parts were cued by E2 during task actions (i.e., touching the nose, clapping the hands, and touching the palm; 59% and 41% of all fixations were targeted to the face and hands, respectively).</p>
      <p>Pan exhibited anticipatory fixation on E2’s actions, particularly reaching for the reward (i.e., fixation on the reward before E2 grasped it; 84% of all reaching actions). On those occasions, Pan fixated the reward an average of 313 ms (<italic>SD</italic>: 146 ms; median: 308 ms) before E2 grasped the reward, and E2’s reaching action lasted an average of 666 ms (i.e., from the onset of the hand movements until the onset of the grasping movements; <italic>SD</italic>: 114 ms; median: 658 ms). These anticipatory fixations were likely triggered by E2’s hand actions rather than by E2’s head-gaze moving toward the reward or by the other task cues because just before those anticipatory fixations, Pan did not fixate on the face (1% of all fixations) but rather was looking elsewhere, and these anticipatory fixations were initiated after the onset of E2’s reaching action (99% of all anticipatory fixations).</p>
      <p>The familiarity status of E2s affected Pan’s viewing time for E2 during the greeting phase (first 50 s; <xref ref-type="fig" rid="pone-0059785-g008">Figure 8</xref>). We conducted a repeated-measures ANOVA with familiarity (3) and body parts (4) as factors (four sessions for each familiarity factor; total of 12 sessions). We found significant main effects for familiarity (<italic>F</italic>(2, 9) = 4.91, <italic>P</italic> = 0.036, <italic>η</italic><sup>2</sup> = 0.52) and body parts (<italic>F</italic>(3, 7) = 6.43, <italic>P</italic> = 0.020, <italic>η</italic><sup>2</sup> = 0.73), indicating that Pan viewed regular/familiar E2s least strongly, irregular/unfamiliar E2s (especially the face and foot) most strongly, and irregular/familiar E2s at an intermediate level.</p>
      <fig id="pone-0059785-g008" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059785.g008</object-id>
        <label>Figure 8</label>
        <caption>
          <title>Viewing time for E2 as a function of familiarity with E2 during the greeting phase (first 50 s). Error bars indicate standard errors.</title>
          <p>*<italic>p</italic>&lt;0.05.</p>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059785.g008" position="float" xlink:type="simple"/>
      </fig>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>Pan wore the lightweight eye tracker on her head for more than 3 minutes in each session, and no physical restraints were necessary. This recording duration is far longer than those in the previous studies that presented still images to chimpanzees (2–3 s) <xref ref-type="bibr" rid="pone.0059785-Kano1">[8]</xref>, <xref ref-type="bibr" rid="pone.0059785-Kano2">[23]</xref>. The eye tracker did not seem to inhibit her head movements in that she moved her head frequently (see <xref ref-type="supplementary-material" rid="pone.0059785.s002">Video S2</xref>). Additionally, the eye tracker did not seem to inhibit her bodily/manual movements given that she also exhibited a wide range of her usual behavioral repertoire (<xref ref-type="table" rid="pone-0059785-t001">Table 1</xref>). One clear limitation of this study is that we obtained the data from a single chimpanzee due to the constraints of the experimental setting (i.e., one experimenter remains with the chimpanzee in the same experimental booth). In the future, we aim to habituate or find more subjects.</p>
      <p>Despite this limitation, the results of this study included several interesting findings that allow us to re-evaluate previous table-mounted eye-tracking studies of chimpanzees. First, the general patterns of eye movements, in terms of fixation duration and saccade length were very similar to those reported by previous studies <xref ref-type="bibr" rid="pone.0059785-Kano1">[8]</xref>, <xref ref-type="bibr" rid="pone.0059785-Kano2">[23]</xref>. That is, the data for both fixation duration and saccade length were characterized by distributions with long right tails. These skewed distributions, demonstrating the variability of Pan’s eye movements, suggest that the she flexibly controlled fixations and saccades, as has been demonstrated in previous studies with humans <xref ref-type="bibr" rid="pone.0059785-Land2">[13]</xref>, <xref ref-type="bibr" rid="pone.0059785-Henderson1">[36]</xref>, <xref ref-type="bibr" rid="pone.0059785-Henderson2">[37]</xref>. It should also be noted that the average fixation duration (254 ms) was close to the value observed in previous studies (∼250 ms) <xref ref-type="bibr" rid="pone.0059785-Kano1">[8]</xref>, <xref ref-type="bibr" rid="pone.0059785-Kano2">[23]</xref>, <xref ref-type="bibr" rid="pone.0059785-Kano3">[28]</xref>.</p>
      <p>Second, the general patterns of Pan’s face viewing were similar to those reported by previous table-mounted eye-tracking studies <xref ref-type="bibr" rid="pone.0059785-Kano1">[8]</xref> despite the fact that different experimental settings were adopted in each study. That is, Pan viewed the face for a longer period of time than she viewed the other body parts, and she demonstrated longer fixations on the face than on other body parts (<xref ref-type="fig" rid="pone-0059785-g006">Fig. 6A</xref>). However, long-bout face viewing, which has been commonly reported among humans in previous studies (long duration of fixations &gt;500 ms or successive fixations on the faces), was not frequently observed in this study. Instead, Pan typically alternated her gaze between the face and the other body parts (e.g., feet and hands) when attending to the experimenter.</p>
      <p>However, these results do not suggest that the interactive contexts did not play a role in Pan’s viewing patterns. Indeed, Pan flexibly modified her viewing patterns for faces and other scene areas depending on the ongoing context or action. For example, Pan rarely viewed the fruit rewards and instead viewed the experimenter during the greeting phase, although she focused on the fruit rewards after task initiation. Additionally, Pan concentrated on the face and feet during the greeting phase and on the face and hands during the task phase. These differences can be explained by the differential bodily/manual actions produced by Pan and the experimenter during each experimental phase. That is, Pan viewed the face and feet when she was performing greeting gestures (e.g., pant grunting, swaying) directed toward the experimenter, and Pan viewed the face and hands when the experimenter was gesturing toward her in the task phase (e.g., touching the nose and hands). Thus, overall, although Pan viewed the experimenter when she and the experimenter were engaged in actions directed toward each other, on those occasions, Pan viewed the experimenter’s whole body (e.g., feet, hands) and not necessarily the face. Future studies should examine chimpanzees’ viewing patterns in situations that facilitate production of a wider variety of actions, such as interactions with conspecifics, to further clarify chimpanzees’ habitual communicative styles.</p>
      <p>In this study, we also found that Pan’s viewing pattern was dependent on her prior experiences with the experimenter (i.e., familiar/unfamiliar, regular/irregular) during the greeting phase. This finding is particularly interesting given that previous studies that presented images of familiar and unfamiliar people to chimpanzees (including this subject) did not find any significant effect of familiarity <xref ref-type="bibr" rid="pone.0059785-Kano1">[8]</xref>. This difference between studies may be explained in terms of habituation speed. That is, this study observed the effect of familiarity 10 seconds after the appearance of the experimenter. However, the previous study presented the images for only a short duration (3 s.). This suggests the importance of using extended time scales to examine chimpanzees’ viewing response to social stimuli.</p>
      <p>Apart from the pattern of face/body viewing, the method employed in this study can be used for other research purposes in the future. We suggest two directions for future research. The first is interspecies comparisons with humans with respect to basic eye-movement controls. It remains unclear how chimpanzees differ from humans in the duration of individual fixations, the length of individual saccades (<xref ref-type="fig" rid="pone-0059785-g005">Fig. 5</xref>), and the use of head and eye movements in shifting gaze (<xref ref-type="fig" rid="pone-0059785-g004">Fig. 4</xref>) in a real-life environment. In this study, the chimpanzee did not alter her overall pattern of fixation duration and saccade length depending on context (<xref ref-type="table" rid="pone-0059785-t002">Table 2</xref>). This is consistent with the results of some studies conducted with human adults <xref ref-type="bibr" rid="pone.0059785-Castelhano1">[38]</xref> but not with those of others <xref ref-type="bibr" rid="pone.0059785-DeAngelus1">[39]</xref>. Additionally, previous studies have found that chimpanzees engage in shorter fixations and longer saccades than do humans when scanning scenes (i.e., chimpanzees scanned scenes more quickly and more widely than did humans). It is unclear how this finding applies to real-life situations. Furthermore, due to the limited contexts and lack of actions in previous studies, the functions involved in shorter/longer duration of fixations of each species remain unclear.</p>
      <p>The second direction for future research relates to gaze following and anticipatory looking in chimpanzees. A number of behavioral studies have been conducted on how monkeys and great apes use experimenter-given social directional cues such as gazing, pointing, and reaching for an object in a choice task <xref ref-type="bibr" rid="pone.0059785-Itakura1">[34]</xref>, <xref ref-type="bibr" rid="pone.0059785-Anderson1">[40]</xref>–<xref ref-type="bibr" rid="pone.0059785-Ruiz1">[44]</xref>. Although monkeys and great apes are able to use these directional cues in a task, they are limited in their ability to use gaze cues, especially eye-only cues (no head direction). However, these previous studies did not clarify how subjects looked at the experimenter’s actions with anticipation. In this study, the chimpanzee frequently (more than 80% of all occasions) looked at the experimenter’s reaching action with anticipation, but she did not show clear evidence of following the experimenter’s gaze. Further studies are necessary to examine this issue more thoroughly.</p>
      <p>In conclusion, this study offers a new method for examining the bodily, manual, and eye movements of a chimpanzee at the micro-level while the chimpanzee interacts with a real-life environment. We found that the general viewing patterns, such as the duration of individual fixations, the length of individual saccades, and the pattern of face viewing, were similar to those reported by previous eye-tracking studies despite differences in experimental settings. However, ongoing context and actions were critically related to the chimpanzee’s eye movements. These methods and results offer new possibilities for examining the natural gaze behavior of chimpanzees.</p>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pone.0059785.s001" mimetype="video/x-ms-wmv" xlink:href="info:doi/10.1371/journal.pone.0059785.s001" position="float" xlink:type="simple">
        <label>Video S1</label>
        <caption>
          <p>Calibration procedure. Cross mark indicates point-of-regard (POR) after the calibration procedure, which suggests the accuracy of POR.</p>
          <p>(WMV)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pone.0059785.s002" mimetype="video/x-ms-wmv" xlink:href="info:doi/10.1371/journal.pone.0059785.s002" position="float" xlink:type="simple">
        <label>Video S2</label>
        <caption>
          <p>Scene-camera image with point-of-regard (POR; cross mark). The first half of image was recorded during the greeting phase, and the latter half was recorded during the task phase.</p>
          <p>(WMV)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Dr. Matsuzawa, Adachi, and Hirata for their comments. We also thank the Center for Human Evolution Modeling Research at the Primate Research Institute for the daily care of the chimpanzees.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pone.0059785-Land1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Land</surname><given-names>MF</given-names></name> (<year>2006</year>) <article-title>Eye movements and the control of actions in everyday life</article-title>. <source>Progress in Retinal and Eye Research</source> <volume>25</volume>: <fpage>296</fpage>–<lpage>324</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Robinson1">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robinson</surname><given-names>DA</given-names></name> (<year>1963</year>) <article-title>A method of measuring eye movemnent using a scieral search coil in a magnetic field</article-title>. <source>IEEE Transactions Biomedical Engineering</source> <volume>10</volume>: <fpage>137</fpage>–<lpage>145</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Nahm1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nahm</surname><given-names>FKD</given-names></name>, <name name-style="western"><surname>Perret</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Amaral</surname><given-names>DG</given-names></name>, <name name-style="western"><surname>Albright</surname><given-names>TD</given-names></name> (<year>1997</year>) <article-title>How do monkeys look at faces?</article-title> <source>Journal of Cognitive Neuroscience</source> <volume>9</volume>: <fpage>611</fpage>–<lpage>623</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Deaner1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deaner</surname><given-names>RO</given-names></name>, <name name-style="western"><surname>Platt</surname><given-names>ML</given-names></name> (<year>2003</year>) <article-title>Reflexive Social Attention in Monkeys and Humans</article-title>. <source>Current Biology</source> <volume>13</volume>: <fpage>1609</fpage>–<lpage>1613</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Berg1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berg</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Boehnke</surname><given-names>SE</given-names></name>, <name name-style="western"><surname>Marino</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Munoz</surname><given-names>DP</given-names></name>, <name name-style="western"><surname>Itti</surname><given-names>L</given-names></name> (<year>2009</year>) <article-title>Free viewing of dynamic stimuli by humans and monkeys</article-title>. <source>Journal of Vision</source> <volume>9</volume>: <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Shepherd1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shepherd</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Steckenfinger</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Hasson</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Ghazanfar</surname><given-names>AA</given-names></name> (<year>2010</year>) <article-title>Human-monkey gaze correlations reveal convergent and divergent patterns of movie viewing</article-title>. <source>Current Biology</source> <volume>20</volume>: <fpage>649</fpage>–<lpage>656</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Keating1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keating</surname><given-names>CF</given-names></name>, <name name-style="western"><surname>Keating</surname><given-names>EG</given-names></name> (<year>1993</year>) <article-title>Monkeys and mug shots: Cues used by rhesus monkeys (<italic>Macaca Mulatta</italic>) to recognize a human face</article-title>. <source>Journal of Comparative Psychology</source> <volume>107</volume>: <fpage>131</fpage>–<lpage>139</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Kano1">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kano</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Tomonaga</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>How chimpanzees look at pictures: a comparative eye-tracking study</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source> <volume>276</volume>: <fpage>1949</fpage>–<lpage>1955</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Gredebck1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gredebäck</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>S</given-names></name>, <name name-style="western"><surname>von Hofsten</surname><given-names>C</given-names></name> (<year>2009</year>) <article-title>Eye tracking in infancy research</article-title>. <source>Developmental neuropsychology</source> <volume>35</volume>: <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Somppi1">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Somppi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Törnqvist</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hänninen</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Krause</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Vainio</surname><given-names>O</given-names></name> (<year>2012</year>) <article-title>Dogs do look at images: eye tracking in canine cognition research</article-title>. <source>Animal Cognition</source> <volume>15</volume>: <fpage>163</fpage>–<lpage>174</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Tgls1">
        <label>11</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Téglás</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Gergely</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kupán</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Miklósi</surname><given-names>Á</given-names></name>, <name name-style="western"><surname>Topál</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Dogs’ gaze following is tuned to human communicative signals</article-title>. <source>Current Biology</source> <volume>22</volume>: <fpage>1</fpage>–<lpage>4</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Hayhoe1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hayhoe</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ballard</surname><given-names>D</given-names></name> (<year>2005</year>) <article-title>Eye movements in natural behavior</article-title>. <source>Trends in Cognitive Sciences</source> <volume>9</volume>: <fpage>188</fpage>–<lpage>194</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Land2">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Land</surname><given-names>MF</given-names></name>, <name name-style="western"><surname>Mennie</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Rusted</surname><given-names>J</given-names></name> (<year>1999</year>) <article-title>The roles of vision and eye movements in the control of activities of daily living</article-title>. <source>Perception</source> <volume>28</volume>: <fpage>1311</fpage>–<lpage>1328</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Hayhoe2">
        <label>14</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hayhoe</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Shrivastava</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mruczek</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Pelz</surname><given-names>JB</given-names></name> (<year>2003</year>) <article-title>Visual memory and motor planning in a natural task</article-title>. <source>Journal of Vision</source> <volume>3</volume>: <fpage>49</fpage>–<lpage>63</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Pelz1">
        <label>15</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pelz</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Canosa</surname><given-names>R</given-names></name> (<year>2001</year>) <article-title>Oculomotor behavior and perceptual strategies in complex tasks</article-title>. <source>Vision Research</source> <volume>41</volume>: <fpage>3587</fpage>–<lpage>3596</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Land3">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Land</surname><given-names>MF</given-names></name>, <name name-style="western"><surname>McLeod</surname><given-names>P</given-names></name> (<year>2000</year>) <article-title>From eye movements to actions: how batsmen hit the ball</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>: <fpage>1340</fpage>–<lpage>1345</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Turano1">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turano</surname><given-names>KA</given-names></name>, <name name-style="western"><surname>Geruschat</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Baker</surname><given-names>FH</given-names></name> (<year>2003</year>) <article-title>Oculomotor strategies for the direction of gaze tested with a real-world activity</article-title>. <source>Vision Research</source> <volume>43</volume>: <fpage>333</fpage>–<lpage>346</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Land4">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Land</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Horwood</surname><given-names>J</given-names></name> (<year>1995</year>) <article-title>Which parts of the road guide steering?</article-title> <source>Nature</source> <volume>377</volume>: <fpage>339</fpage>–<lpage>340</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Shepherd2">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shepherd</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Deaner</surname><given-names>RO</given-names></name>, <name name-style="western"><surname>Platt</surname><given-names>ML</given-names></name> (<year>2006</year>) <article-title>Social status gates social attention in monkeys</article-title>. <source>Current Biology</source> <volume>16</volume>: <fpage>119</fpage>–<lpage>120</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Shepherd3">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shepherd</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Platt</surname><given-names>ML</given-names></name> (<year>2006</year>) <article-title>Noninvasive telemetric gaze tracking in freely moving socially housed prosimian primates</article-title>. <source>Methods</source> <volume>38</volume>: <fpage>185</fpage>–<lpage>194</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Franchak1">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Franchak</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Kretch</surname><given-names>KS</given-names></name>, <name name-style="western"><surname>Soska</surname><given-names>KC</given-names></name>, <name name-style="western"><surname>Adolph</surname><given-names>KE</given-names></name> (<year>2011</year>) <article-title>Head-mounted eye tracking: A new method to describe infant looking</article-title>. <source>Child Development</source> <volume>82</volume>: <fpage>1738</fpage>–<lpage>1750</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Williams1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname><given-names>FJ</given-names></name>, <name name-style="western"><surname>Mills</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Guo</surname><given-names>K</given-names></name> (<year>2011</year>) <article-title>Development of a head-mounted, eye-tracking system for dogs</article-title>. <source>Journal of Neuroscience Methods</source> <volume>194</volume>: <fpage>259</fpage>–<lpage>265</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Kano2">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kano</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Tomonaga</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Species difference in the timing of gaze movement between chimpanzees and humans</article-title>. <source>Animal Cognition</source> <volume>14</volume>: <fpage>879</fpage>–<lpage>892</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Parr1">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parr</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Dove</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hopkins</surname><given-names>WD</given-names></name> (<year>1998</year>) <article-title>Why faces may be special: evidence of the inversion effect in chimpanzees</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>10</volume>: <fpage>615</fpage>–<lpage>622</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Tomonaga1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tomonaga</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Visual search for orientation of faces by a chimpanzee (<italic>Pan troglodytes</italic>): face-specific upright superiority and the role of facial configural properties</article-title>. <source>Primates</source> <volume>48</volume>: <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Tomonaga2">
        <label>26</label>
        <mixed-citation publication-type="other" xlink:type="simple">Tomonaga M (2010) Chimpanzee eyes have it? Social cognition on the basis of gaze and attention from the comparative-cognitive-developmental perspective. In: Lornsdorf E, Ross S, Matsuzawa T, editors. The mind of the chimpanzee: Ecological and empirical perspectives. Chicago: University of Chicago Press.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Hattori1">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hattori</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kano</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Tomonaga</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Differential sensitivity to conspecific and allospecific cues in chimpanzees and humans: A comparative eye-tracking study</article-title>. <source>Biology Letters</source> <volume>6</volume>: <fpage>610</fpage>–<lpage>613</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Kano3">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kano</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Tomonaga</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Face scanning in chimpanzees and humans: Continuity and discontinuity</article-title>. <source>Animal Behaviour</source> <volume>79</volume>: <fpage>227</fpage>–<lpage>235</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-MyowaYamakoshi1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Myowa-Yamakoshi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Scola</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Hirata</surname><given-names>S</given-names></name> (<year>2012</year>) <article-title>Humans and chimpanzees attend differently to goal-directed actions</article-title>. <source>Nature Communications</source> <volume>3</volume>: <fpage>693</fpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Matsuzawa1">
        <label>30</label>
        <mixed-citation publication-type="other" xlink:type="simple">Matsuzawa T, Tomonaga M, Tanaka M (2006) Cognitive development in chimpanzees. Tokyo: Springer.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Hashiya1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hashiya</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kojima</surname><given-names>S</given-names></name> (<year>1997</year>) <article-title>Auditory-visual intermodal matching by a chimpanzee (<italic>Pan troglodytes</italic>)</article-title>. <source>Japanese Psychological Research</source> <volume>39</volume>: <fpage>182</fpage>–<lpage>190</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Izumi1">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izumi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kojima</surname><given-names>S</given-names></name> (<year>2004</year>) <article-title>Matching vocalizations to vocalizing faces in a chimpanzee (<italic>Pan troglodytes</italic>)</article-title>. <source>Animal Cognition</source> <volume>7</volume>: <fpage>179</fpage>–<lpage>184</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Kojima1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kojima</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Izumi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ceugniet</surname><given-names>M</given-names></name> (<year>2003</year>) <article-title>Identification of vocalizers by pant hoots, pant grunts and screams in a chimpanzee</article-title>. <source>Primates</source> <volume>44</volume>: <fpage>225</fpage>–<lpage>230</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Itakura1">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itakura</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Tanaka</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Use of experimenter-given cues during object-choice tasks by chimpanzees (<italic>Pan troglodytes</italic>), an orangutan (<italic>Pongo pygmaeus</italic>), and human infants (<italic>Homo sapiens</italic>)</article-title>. <source>Journal of Comparative Psychology</source> <volume>112</volume>: <fpage>119</fpage>–<lpage>126</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Kano4">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kano</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Hirata</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Call</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Tomonaga</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>The visual strategy specific to humans among hominids: A study using the gap-overlap paradigm</article-title>. <source>Vision Research</source> <volume>51</volume>: <fpage>2348</fpage>–<lpage>2355</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Henderson1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Henderson</surname><given-names>JM</given-names></name> (<year>2003</year>) <article-title>Human gaze control during real-world scene perception</article-title>. <source>Trends in Cognitive Sciences</source> <volume>7</volume>: <fpage>498</fpage>–<lpage>504</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Henderson2">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Henderson</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Hollingworth</surname><given-names>A</given-names></name> (<year>1999</year>) <article-title>High-level scene perception</article-title>. <source>Annual Review of Psychology</source> <volume>50</volume>: <fpage>243</fpage>–<lpage>271</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Castelhano1">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Castelhano</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Mack</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Henderson</surname><given-names>JM</given-names></name> (<year>2009</year>) <article-title>Viewing task influences eye movement control during active scene perception</article-title>. <source>Journal of Vision</source> <volume>9</volume>: <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-DeAngelus1">
        <label>39</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeAngelus</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pelz</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Top-down control of eye movements: Yarbus revisited</article-title>. <source>Visual Cognition, 17</source> <volume>6</volume>: <fpage>790</fpage>–<lpage>811</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Anderson1">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Montant</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Schmitt</surname><given-names>D</given-names></name> (<year>1996</year>) <article-title>Rhesus monkeys fail to use gaze direction as an experimenter-given cue in an object-choice task</article-title>. <source>Behavioural Processes</source> <volume>37</volume>: <fpage>47</fpage>–<lpage>55</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Anderson2">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Sallaberry</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Barbier</surname><given-names>H</given-names></name> (<year>1995</year>) <article-title>Use of experimenter-given cues during object-choice tasks by capuchin monkeys</article-title>. <source>Animal Behaviour</source> <volume>49</volume>: <fpage>201</fpage>–<lpage>208</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Peignot1">
        <label>42</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peignot</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>JR</given-names></name> (<year>1999</year>) <article-title>Use of experimenter-given manual and facial cues by gorillas (<italic>Gorilla gorilla</italic>) in an object-choice task</article-title>. <source>Journal of Comparative Psychology</source> <volume>113</volume>: <fpage>253</fpage>–<lpage>260</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Poss1">
        <label>43</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poss</surname><given-names>SR</given-names></name>, <name name-style="western"><surname>Kuhar</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Stoinski</surname><given-names>TS</given-names></name>, <name name-style="western"><surname>Hopkins</surname><given-names>WD</given-names></name> (<year>2006</year>) <article-title>Differential use of attentional and visual communicative signaling by orangutans (<italic>Pongo pygmaeus</italic>) and gorillas (<italic>Gorilla gorilla</italic>) in response to the attentional status of a human</article-title>. <source>American Journal of Primatology</source> <volume>68</volume>: <fpage>978</fpage>–<lpage>992</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059785-Ruiz1">
        <label>44</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ruiz</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Gómez</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Roeder</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Byrne</surname><given-names>RW</given-names></name> (<year>2009</year>) <article-title>Gaze following and gaze priming in lemurs</article-title>. <source>Animal Cognition</source> <volume>12</volume>: <fpage>427</fpage>–<lpage>434</lpage>.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>