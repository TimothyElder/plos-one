<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PONE-D-12-00517</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0039384</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Neuroimaging</subject>
              <subj-group>
                <subject>fMRI</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Medicine</subject>
          <subj-group>
            <subject>Mental health</subject>
            <subj-group>
              <subject>Psychology</subject>
              <subj-group>
                <subject>Social psychology</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Anthropology</subject>
            <subj-group>
              <subject>Social anthropology</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Psychology</subject>
            <subj-group>
              <subject>Human relations</subject>
              <subject>Neuropsychology</subject>
              <subject>Sensory perception</subject>
              <subject>Social psychology</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Sociology</subject>
            <subj-group>
              <subject>Social research</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Mental Health</subject>
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories><title-group><article-title>Neural Basis of Moral Elevation Demonstrated through Inter-Subject Synchronization of Cortical Activity during Free-Viewing</article-title><alt-title alt-title-type="running-head">Neural Basis of Moral Elevation</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Englander</surname>
            <given-names>Zoë A.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Haidt</surname>
            <given-names>Jonathan</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Morris</surname>
            <given-names>James P.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">          <addr-line>Department of Psychology, University of Virginia, Charlottesville, Virginia, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Paul</surname>
            <given-names>Friedemann</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Charité University Medicine Berlin, Germany</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">jpmorris@virginia.edu</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: JPM JH. Performed the experiments: ZE. Analyzed the data: ZE. Contributed reagents/materials/analysis tools: ZE JPM. Wrote the paper: ZE JH JPM.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>20</day>
        <month>6</month>
        <year>2012</year>
      </pub-date><volume>7</volume><issue>6</issue><elocation-id>e39384</elocation-id><history>
        <date date-type="received">
          <day>3</day>
          <month>1</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>24</day>
          <month>5</month>
          <year>2012</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Englander et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <sec>
          <title>Background</title>
          <p>Most research investigating the neural basis of social emotions has examined emotions that give rise to negative evaluations of others (e.g. anger, disgust). Emotions triggered by the virtues and excellences of others have been largely ignored. Using fMRI, we investigated the neural basis of two “other-praising" emotions – Moral Elevation (a response to witnessing acts of moral beauty), and Admiration (which we restricted to admiration for physical skill).</p>
        </sec>
        <sec>
          <title>Methodology/Principal Findings</title>
          <p>Ten participants viewed the same nine video clips. Three clips elicited moral elevation, three elicited admiration, and three were emotionally neutral. We then performed pair-wise voxel-by-voxel correlations of the BOLD signal between individuals for each video clip and a separate resting-state run. We observed a high degree of inter-subject synchronization, regardless of stimulus type, across several brain regions during free-viewing of videos. Videos in the elevation condition evoked significant inter-subject synchronization in brain regions previously implicated in self-referential and interoceptive processes, including the medial prefrontal cortex, precuneus, and insula. The degree of synchronization was highly variable over the course of the videos, with the strongest synchrony occurring during portions of the videos that were independently rated as most emotionally arousing. Synchrony in these same brain regions was not consistently observed during the admiration videos, and was absent for the neutral videos.</p>
        </sec>
        <sec>
          <title>Conclusions/Significance</title>
          <p>Results suggest that the neural systems supporting moral elevation are remarkably consistent across subjects viewing the same emotional content. We demonstrate that model-free techniques such as inter-subject synchronization may be a useful tool for studying complex, context dependent emotions such as self-transcendent emotion.</p>
        </sec>
      </abstract><funding-group><funding-statement>JP Morris was supported by a National Institute of Mental Health Pathway to Independence Award, R00MH079617. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="8"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Scientific investigations of emotion typically focus on the original six “basic" emotions first described by Ekman and Friesen <xref ref-type="bibr" rid="pone.0039384-Ekman1">[1]</xref>. These emotions – happiness, sadness, anger, fear, surprise and disgust – each have a distinctive cross-culturally recognizable facial expression and can be easily elicited in the lab. However, emotional experience is not limited to these six emotions. Ekman himself has long said that there are many more emotions that should be studied, only a few of which may turn out to have distinctive facial signals or clear analogues in other animals <xref ref-type="bibr" rid="pone.0039384-Ekman2">[2]</xref>.</p>
      <p>Most emotions involve an evaluation–a rapid appraisal of a person, action, or event as either good or bad <xref ref-type="bibr" rid="pone.0039384-Ortony1">[3]</xref>. There have been many studies of emotions that give rise to the negative evaluations of others (anger, disgust, and contempt), and of the self (shame and guilt). However, research on the positive social emotions appears much less in the literature than research on the negative emotions <xref ref-type="bibr" rid="pone.0039384-Haidt1">[4]</xref>.</p>
      <p>Recent work by Haidt and colleagues has sought to demonstrate the existence of a class of emotions they call “other-praising" emotions. These emotions are elicited by a positive appraisal of another person’s actions. They generally motivate people to praise others to third parties <xref ref-type="bibr" rid="pone.0039384-Algoe1">[5]</xref>–<xref ref-type="bibr" rid="pone.0039384-Haidt2">[6]</xref>. This class of emotions includes moral elevation, which is defined as a reaction to witnessing acts of moral beauty. Moral elevation has been found to motivate people to want to emulate the virtuous role model and do virtuous deeds themselves <xref ref-type="bibr" rid="pone.0039384-Haidt2">[6]</xref>–<xref ref-type="bibr" rid="pone.0039384-Schnall1">[7]</xref>. Another “other-praising" emotion is admiration. Because the word “admiration" is a broad term that is sometimes used to refer to admiration for moral virtue (which would overlap with elevation), we restrict the meaning of admiration to mean the positive emotional response to witnessing another person exceed normal standards of skill or talent. Admiration for skills/talents has been shown to be quite distinct from moral elevation. For example, Algoe and Haidt <xref ref-type="bibr" rid="pone.0039384-Algoe1">[5]</xref> found that admiration was generally reported by participants to be energizing. Admiration motivates people to work harder on their <italic>own</italic> goals and projects, and it was shown to be more likely than elevation to elicit self-reported feelings of chills or tingling in the skin. (The “other-praising" emotions also include the emotion of gratitude, which we will not discuss because it cannot be elicited by a video. Therefore, it cannot be studied using the methods we describe below.).</p>
      <p>Elevation and admiration are unusual among emotions because they are not elicited by events directly relevant to the self. Cognitive theories of emotion typically categorize emotions based on the appraisals that trigger them. As Lazarus <xref ref-type="bibr" rid="pone.0039384-Lazarus1">[8]</xref> put it, an appraisal is “an evaluation of the significance of knowledge about what is happening for our personal well-being. Only the recognition that we have something to gain or lose, that is, that the outcome of a transaction is relevant to goals and well-being, generates an emotion" <xref ref-type="bibr" rid="pone.0039384-Ortony1">[3]</xref>, <xref ref-type="bibr" rid="pone.0039384-Smith1">[9]</xref>. Therefore, it is puzzling–and it requires some investigation– to understand how and why people can experience chills, and can be moved to tears, just by seeing other people do extraordinary things with no implications for the self. In fact, the “other-praising" emotions are sometimes referred to as “self-transcendent" emotions because they seem to take people out of their ordinary everyday focus on their own goals and projects. These emotions make people feel “uplifted" or “inspired" <xref ref-type="bibr" rid="pone.0039384-Haidt3">[10]</xref>.</p>
      <p>Techniques that have been successful in identifying the neural underpinnings of the basic emotions (such as the use of standardized still photographs to elicit emotions) are likely to be less successful when studying the “other-praising emotions". Unlike basic emotions such as fear and disgust, elevation is generally induced by narratives with plotlines that build slowly. It cannot be induced by a simple photograph (e.g., of a person giving money to a beggar). While admiration can perhaps be induced by a photograph, it too is generally enhanced by a story that sets up the background and the challenge that the admirable person overcame. Therefore, these emotions present a unique challenge: how can we present stimuli with enough context to evoke them, and how can we design an analysis strategy that is compatible with such complex stimuli?</p>
      <p>Due to these difficulties, there were no documented empirical attempts to characterize the neural basis of these emotions until very recently. In an fMRI study, Immordino-Yang and colleagues <xref ref-type="bibr" rid="pone.0039384-ImmordinoYang1">[11]</xref> elicited admiration and compassion across four different categories – admiration for virtue (which we call “moral elevation"), admiration for skill, compassion for social/psychological pain and compassion for physical pain. The authors relied on a “reminder" approach to present the emotional stimuli. This approach involved rigorously pre-testing each emotion by allowing participants to view full-length videos before entering the scanner. While in the scanner, participants viewed reminders of the stimuli while attempting to self-induce a similar emotional state to that experienced in pre-testing. Using this approach, the authors were able build a temporal model of the expected BOLD response necessary for traditional fMRI analytic techniques (general linear model analysis) while also using complex stimuli to evoke the target emotion. Compelling evidence for the role of brain systems supporting interoceptive processes in the subjective experience of admiration and compassion was presented. Unfortunately, the study only reported results for pairs of conditions merged together, with each “admiration" condition being paired with a “compassion" condition, which doesn’t allow for distinctions between moral elevation and admiration for skill. Furthermore, the “reminder" experimental approach does not allow for a direct measure of the emotional experience as it is naturally evoked, rather it asks the subject to re-induce an emotional state that was experienced in the past.</p>
      <p>Due to the limitations associated with applying typical fMRI experimental design and analysis procedures to study slow-building emotional experience, we turned to a non-traditional approach that has been previously successful in identifying neural systems involved in shared sensory experience. Specifically, Hasson and colleagues <xref ref-type="bibr" rid="pone.0039384-Hasson1">[12]</xref> have developed an analytic technique where the BOLD response in an individual brain is used to model the activity in another brain. By applying this inter-subject correlation procedure, they have demonstrated extensive voxel-to-voxel synchrony between individuals freely-viewing the same movie. This approach has provided fascinating evidence for how individuals process complex audio- visual stimuli in the same way (how individuals <italic>see</italic> the world in the same way) and has led us to consider its utility in understanding to what degree humans <italic>feel</italic> the world in the same way, or at least during the experience of “other-praising" emotions.</p>
      <p>In the present study we adopted this inter-subject correlation approach to characterize neural systems supporting moral elevation and admiration. This analytic approach allowed us to use a free-viewing paradigm where video clips were presented to subjects in the scanner. These video clips have been demonstrated to be effective in eliciting the target emotions. We predicted that a high-degree of synchrony in the BOLD signal across individuals would be induced during the experience of “other-praising"emotion. We hypothesized that the inter-subject synchronization would be especially pronounced in brain regions involved with interoceptive processes and self-referential thought, as well as in the insula, as was shown by Immordino-Yang and colleagues <xref ref-type="bibr" rid="pone.0039384-Haidt1">[4]</xref>, <xref ref-type="bibr" rid="pone.0039384-ImmordinoYang1">[11]</xref>.</p>
    </sec>
    <sec id="s2" sec-type="methods">
      <title>Methods</title>
      <sec id="s2a">
        <title>Selection of Stimuli</title>
        <p>The video clips used in this study were selected from a collection of YouTube clips previously validated by a study on <ext-link ext-link-type="uri" xlink:href="http://www.YourMorals.org" xlink:type="simple">www.YourMorals.org</ext-link>. The clips were shown to effectively elicit the target emotions. In a large and diverse population. 2,815 participants were shown one of 30 short video clips (10 candidate clips for each of the three emotions; elevation, admiration, and neutral). To assess the distinct characteristics of each video, a web survey was created that asked about each of the main components of the emotional content of the video, as well as a few additional questions about the participants’ overall response to the video. We selected the videos to be between 3 and 5 minutes in length to ensure that large variations in duration wouldn’t confound our analysis.</p>
        <p>Of the videos that fit the criteria for length, we chose three videos rated highly for ‘moral good’ and ‘positive emotion’ for the moral elevation videos. One video, for example, was the story of the “subway hero", a New York construction worker who endangered himself to save, a young man who had suffered a seizure and fallen onto the tracks, from being struck by a New York City subway train.</p>
        <p>Additionally, we chose three videos rated highly for ‘great skill or talent’ and ‘positive emotion’ for our admiration for physical skill videos. One video, for example, showed the highlights of a professional basketball player’s career. Finally, we selected two control videos that were rated as interesting but did not contain content rated as morally good or depicting great skill. As an example, one video was an informational clip about how a flute is constructed. See <xref ref-type="table" rid="pone-0039384-t001">Table 1</xref> for more information about each of the videos.</p>
        <table-wrap id="pone-0039384-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0039384.t001</object-id><label>Table 1</label><caption>
            <title>Video Stimuli.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pone-0039384-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0039384.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">Plot Summary</td>
                <td align="left" colspan="1" rowspan="1">Duration</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="3" rowspan="1">
                  <bold>Moral Elevation</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">EL1</td>
                <td align="left" colspan="1" rowspan="1">A young man who was born blind and crippled learns to play the piano beautifully and is able to participate in a marching band.</td>
                <td align="left" colspan="1" rowspan="1">303 s</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">EL2</td>
                <td align="left" colspan="1" rowspan="1">A boy with autism participates in a basketball game.</td>
                <td align="left" colspan="1" rowspan="1">130 s</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">EL3</td>
                <td align="left" colspan="1" rowspan="1">A man saves a stranger’s life on the New York City subway.</td>
                <td align="left" colspan="1" rowspan="1">124 s</td>
              </tr>
              <tr>
                <td align="left" colspan="3" rowspan="1">
                  <bold>Admiration</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">AP1</td>
                <td align="left" colspan="1" rowspan="1">A child sings somewhere over the rainbow for a talent competition.</td>
                <td align="left" colspan="1" rowspan="1">177 s</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">AP2</td>
                <td align="left" colspan="1" rowspan="1">Dancers compete on a talent competition.</td>
                <td align="left" colspan="1" rowspan="1">192 s</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">AP3</td>
                <td align="left" colspan="1" rowspan="1">A hall of fame basketball player’s career.</td>
                <td align="left" colspan="1" rowspan="1">246 s</td>
              </tr>
              <tr>
                <td align="left" colspan="3" rowspan="1">
                  <bold>Neutral</bold>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">N1</td>
                <td align="left" colspan="1" rowspan="1">Interview with an actress about her TV show.</td>
                <td align="left" colspan="1" rowspan="1">120 s</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">N2</td>
                <td align="left" colspan="1" rowspan="1">Interview on talk show.</td>
                <td align="left" colspan="1" rowspan="1">256 s</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">N3</td>
                <td align="left" colspan="1" rowspan="1">Tutorial about how a flute is constructed</td>
                <td align="left" colspan="1" rowspan="1">299 s</td>
              </tr>
            </tbody>
          </table></alternatives></table-wrap>
      </sec>
      <sec id="s2b">
        <title>fMRI Experiment</title>
        <sec id="s2b1">
          <title>Participants</title>
          <p>In the fMRI experiment, 10 healthy volunteers (age 20 to 27, 7 female, 3 male) participated. All volunteers had normal or corrected-to-normal vision, and were screened against neurological and psychiatric diseases. Volunteers provided written informed consent prior to participation.</p>
        </sec>
        <sec id="s2b2">
          <title>Stimuli</title>
          <p>Each participant watched the same nine video clips, three belonging to the moral elevation condition, three belonging to the admiration condition, and three belonging to the neutral condition (See <xref ref-type="table" rid="pone-0039384-t001">Table 1</xref>). The order of stimulus presentation was counterbalanced across participants such that videos were presented in three blocks, with one video clip of each condition in each block. The last video clip of each block belonged to the neutral condition. One of the video clips in the neutral condition was not analyzed because the scanner stopped before the narrative had concluded for some of the subjects. Therefore, all of the analysis contains only two examples of neutral videos. Subjects were also instructed to lie still with their eyes closed while no visual or auditory stimuli were presented during one scan three minutes in length. This data was used to identify areas of correlation that may have been introduced by scanner noise or data processing procedures, as no relevant correlation is expected in the absence of a stimulus.</p>
          <p>Using Psychophysics toolbox for MATLAB, the stimuli were presented using an LCD projector (AVOTEC) that projected images onto a screen located behind the subject’s head. Participants viewed the stimuli through a mirror attached to the head coil, and listened to the audio through headphones. Participants were not told to induce a specific emotional state, they were told only to attend to the stimuli as they were presented. During the resting state scan, participants were asked to lie still with their eyes closed. Timing of the video clips was synchronized to the TTL pulse received with the acquisition of the first TR.</p>
        </sec>
        <sec id="s2b3">
          <title>Imaging</title>
          <p>Scanning was performed on a Siemens 3 Tesla MAGNETOM Trio with a 12-channel head coil. 176 high-resolution T1 weighted images were acquired using Siemens’ MPRAGE pulse sequence (TR, 1900 ms; TE, 2.53 ms; FOV, 250 mm; voxel size, 1 mm×1 mm×1 mm) and used for co-registration with functional data. Whole-brain functional images were acquired using a T2* weighted EPI sequence (repetition time = 2000 ms, echo time = 40, FOV = 192 mm, image matrix = 64×64, voxel size = 3.0×3.0 × 4.2 mm; flip angle = 90°, 28 axial slices).</p>
        </sec>
        <sec id="s2b4">
          <title>Pre-Processing</title>
          <p>FMRI data processing was carried out using FEAT (FMRI Expert Analysis Tool) Version 5.98, part of FSL (FMRIB’s Software Library, <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl" xlink:type="simple">www.fmrib.ox.ac.uk/fsl</ext-link>). Motion was detected by center of mass measurements implemented using automated scripts developed for quality assurance purposes and packaged with the BXH/XCEDE suite of tools, available through the Bioinformatics Information Research Network (BIRN). Participants that had greater than a 3-mm deviation in the center of mass in the x-, y-, or z-dimensions were excluded from further analysis. The following pre-statistics processing was applied; motion correction using MCFLIRT <xref ref-type="bibr" rid="pone.0039384-Jenkinson1">[13]</xref>, slice-timing correction using Fourier-space time-series phase-shifting; non-brain removal using BET <xref ref-type="bibr" rid="pone.0039384-Smith2">[14]</xref>, spatial smoothing using a Gaussian kernel of FWHM 8.0 mm; grand-mean intensity normalization of the entire 4D dataset by a single multiplicative factor; high-pass temporal filtering (Gaussian-weighted least-squares straight line fitting, with sigma = 50.0 s). Additionally, each functional volume was registered to the participant’s anatomical image, and then to the standard Montreal Neurologic Institute (MNI) template brain using FLIRT <xref ref-type="bibr" rid="pone.0039384-Jenkinson1">[13]</xref>. Each individual anatomical image was also registered to standard space and segmented into gray matter, white matter, and CSF components using FAST <xref ref-type="bibr" rid="pone.0039384-Zhang1">[15]</xref>. The preprocessed data were then used in the analysis procedures described below.</p>
        </sec>
        <sec id="s2b5">
          <title>Inter-subject correlation</title>
          <p>The primary analysis followed the methods of Hasson and colleagues <xref ref-type="bibr" rid="pone.0039384-Hasson1">[12]</xref>, and consisted of an assessment of the temporal synchronization in the BOLD signal between different individuals’ brains that occurred in response to the stimuli. A voxel should show a high degree of correlation with a corresponding voxel in another brain if the two time-courses show similar temporal dynamics, time-locked to the stimuli. As demonstrated by Hasson and colleagues, extensive synchronization is observed in visual and auditory regions as participants freely view complex stimuli. However, no inter-subject synchronization would be expected in data sets where the participants were scanned in the absence of stimuli. Inter-subject temporal correlation would not be expected in the situation where there is no stimulus to induce the time-locking of the neural response. We extended this methodology beyond the study of visual and auditory processing, to the investigation of the experience of “other-praising" emotions.</p>
          <p>In order to quantify the degree of synchronization in the BOLD signal between corresponding voxels in different individuals’ brains, the time course of each voxel in a template brain was used to predict activity in a target brain, resulting in a map of correlation coefficients. Using the segmented and standardized anatomical images, we restricted this procedure only to voxels that were classified as gray matter in both the template and target brains. Overall, there were 45 pair-wise comparisons for each video clip and the resting state run between 10 individuals. After maps of correlation coefficients were generated for each pair, the correlation maps were concatenated into a 4-D data set (x×y×z×correlation coefficient for each pair). To determine which voxels showed overall correlation across all pair-wise comparisons, a non-parametric permutation method as implemented by FSL randomise was used for thresholding and correction for multiple comparisons using FWE (family-wise error) correction <xref ref-type="bibr" rid="pone.0039384-Nichols1">[16]</xref>. This resulted in a single image for each video clip describing which voxels have correlation coefficients that are significantly different from zero with p&lt;0.05. This method of determining probability was used because the null distributions for these datasets were assumed to be non-normal.</p>
        </sec>
      </sec>
      <sec id="s2c">
        <title>Peak-Moment Video Ratings</title>
        <p>In order to establish the portions of the movie clips that were most likely to evoke strong emotions, we conducted a separate behavioral study intended to provide moment-by-moment ratings of positive and negative emotion for each of our video clips. We were looking to establish which portions of the video clips individuals found to be most emotionally arousing. Twenty-one volunteers (age 18–32, 13 females) who did not previously participate in the fMRI portion of the experiment participated in a behavioral rating experiment. In this experiment, the participants moved a slider up and down to reflect positive or negative feelings while viewing the videos. Participants controlled the slider with arrow keys on a computer keyboard and were instructed to move it up when feeling positive and to move it down when feeling negative. Twenty-five data points per second were collected on an arbitrary scale that ranged from −500 (most negative) to 500 (most positive). From this study we were able to generate a time-series for each participant of moment-by-moment video ratings. We then averaged and normalized the ratings time-course for each video across participants in order to identify “peak-moments" of positive ratings for each of the videos. Peak moments were identified as windows around areas of the time-course in which the normalized rating exceeded z = 2.3. The average length of the peak moments was 10 TRs (20 seconds). There were no windows in the normalized time-courses of the neutral videos that met the criteria for a “peak moment."</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <p>We observed extensive regions of gray matter that showed temporal synchronization in the BOLD signal between brains for all video clips. In contrast, very little synchronized activity was observed in the resting-state data. This result is demonstrated in <xref ref-type="fig" rid="pone-0039384-g001">Figure 1</xref> and <xref ref-type="table" rid="pone-0039384-t002">Table 2</xref> show the mean percentage of gray matter voxels in the brain for each of the three types of video clips and in the resting state that fit the following criteria: (i) the correlation coefficient of that voxel was greater than 0.2 in an individual pair-wise correlation map, (ii) and that voxel showed a significant (P&lt;0.05) correlation across all 45 pair-wise comparisons Voxels that fit these criteria were considered to be temporally synchronized between brains. Overall, the elevation clips induced a greater amount of inter-subject temporal synchronization across all gray matter voxels relative to the other three conditions. <xref ref-type="fig" rid="pone-0039384-g002">Figure 2</xref> shows the extent of the significantly correlated BOLD signal for each video clip.</p>
      <fig id="pone-0039384-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0039384.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Percentage of correlated gray matter voxels for each condition.</title>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0039384.g001" xlink:type="simple"/>
      </fig>
      <table-wrap id="pone-0039384-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0039384.t002</object-id><label>Table 2</label><caption>
          <title>Percentage of correlated gray matter voxels for each condition.</title>
        </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pone-0039384-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0039384.t002" xlink:type="simple"/><table>
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <td align="left" colspan="1" rowspan="1"/>
              <td align="left" colspan="1" rowspan="1">Mean % correlated</td>
              <td align="left" colspan="1" rowspan="1">Std Dev.</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" colspan="1" rowspan="1">Elevation</td>
              <td align="left" colspan="1" rowspan="1">11.41</td>
              <td align="left" colspan="1" rowspan="1">2.55</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Admiration</td>
              <td align="left" colspan="1" rowspan="1">3.65</td>
              <td align="left" colspan="1" rowspan="1">0.89</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Neutral</td>
              <td align="left" colspan="1" rowspan="1">4.54</td>
              <td align="left" colspan="1" rowspan="1">0.51</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">In darkness</td>
              <td align="left" colspan="1" rowspan="1">1.39</td>
              <td align="left" colspan="1" rowspan="1">0.61</td>
            </tr>
          </tbody>
        </table></alternatives></table-wrap>
      <fig id="pone-0039384-g002" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0039384.g002</object-id>
        <label>Figure 2</label>
        <caption>
          <title>Significantly correlated voxels across all pair-wise comparisons for each condition.</title>
          <p>Color maps indicate the significance level at each voxel (<italic>p</italic>&lt;.05–<italic>p</italic>&lt;1×10<sup>−3</sup>).</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0039384.g002" xlink:type="simple"/>
      </fig>
      <p>Next, we found which voxels responded to the experience of an other-praising emotion, regardless of whether it was elevation or admiration. For this analysis, we created a mask of voxels that were significantly correlated for both the elevation and admiration conditions, but not the neutral condition. The results revealed small clusters of voxels within the bilateral middle temporal gyrus and fusiform gyrus that showed significant inter-subject synchronization across all pair-wise correlation maps for both elevation and admiration (<xref ref-type="table" rid="pone-0039384-t003">Table 3</xref>).</p>
      <table-wrap id="pone-0039384-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0039384.t003</object-id><label>Table 3</label><caption>
          <title>Clusters of voxels correlated in both the admiration for physical skill and moral elevation conditions.</title>
        </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pone-0039384-t003-3" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0039384.t003" xlink:type="simple"/><table>
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <td align="left" colspan="1" rowspan="1">Anatomical Region</td>
              <td align="left" colspan="1" rowspan="1">k</td>
              <td align="left" colspan="1" rowspan="1">Hem</td>
              <td align="left" colspan="1" rowspan="1">x</td>
              <td align="left" colspan="1" rowspan="1">y</td>
              <td align="left" colspan="1" rowspan="1">z</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" colspan="1" rowspan="1">Middle Temporal Gyrus</td>
              <td align="left" colspan="1" rowspan="1">262</td>
              <td align="left" colspan="1" rowspan="1">R</td>
              <td align="left" colspan="1" rowspan="1">59.8</td>
              <td align="left" colspan="1" rowspan="1">−46.8</td>
              <td align="left" colspan="1" rowspan="1">5</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1"/>
              <td align="left" colspan="1" rowspan="1">18</td>
              <td align="left" colspan="1" rowspan="1">L</td>
              <td align="left" colspan="1" rowspan="1">−58.2</td>
              <td align="left" colspan="1" rowspan="1">−40.6</td>
              <td align="left" colspan="1" rowspan="1">−4</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Occipital Pole</td>
              <td align="left" colspan="1" rowspan="1">64</td>
              <td align="left" colspan="1" rowspan="1">L</td>
              <td align="left" colspan="1" rowspan="1">−11</td>
              <td align="left" colspan="1" rowspan="1">−97.6</td>
              <td align="left" colspan="1" rowspan="1">−9</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Fusiform Gyrus</td>
              <td align="left" colspan="1" rowspan="1">34</td>
              <td align="left" colspan="1" rowspan="1">R</td>
              <td align="left" colspan="1" rowspan="1">19.6</td>
              <td align="left" colspan="1" rowspan="1">−90.2</td>
              <td align="left" colspan="1" rowspan="1">−18.4</td>
            </tr>
          </tbody>
        </table></alternatives><table-wrap-foot>
          <fn id="nt101">
            <label/>
            <p>X,Y,Z coordinates reflect the center of gravity of the cluster in MNI space.</p>
          </fn>
        </table-wrap-foot></table-wrap>
      <p>To isolate voxels that responded uniquely to elevation, we created a mask of voxels that displayed significant correlation across participants in all three elevation videos, but not any of the neutral or admiration videos. From this analysis, we identified clusters of voxels in the anterior and posterior cingulate, precuneus, bilateral temporoparietal junction, superior frontal gyrus, inferior frontral gyrus and angular gyrus that appeared to be responding consistently and uniquely to the elevation videos (<xref ref-type="table" rid="pone-0039384-t004">Table 4</xref>; <xref ref-type="fig" rid="pone-0039384-g003">Figure 3</xref>). We classified and reported each cluster of voxels based on its center of gravity, however the clusters identified as the angular gyrus in the left hemisphere and the superior frontal gyrus in the right hemisphere include the bilateral parietal operculum.</p>
      <table-wrap id="pone-0039384-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0039384.t004</object-id><label>Table 4</label><caption>
          <title>Clusters of voxels significantly correlated in all moral elevation video clips but none of the other clips.</title>
        </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pone-0039384-t004-4" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0039384.t004" xlink:type="simple"/><table>
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
            <col align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <td align="left" colspan="1" rowspan="1">Anatomical Region</td>
              <td align="left" colspan="1" rowspan="1">k</td>
              <td align="left" colspan="1" rowspan="1">Hem</td>
              <td align="left" colspan="1" rowspan="1">x</td>
              <td align="left" colspan="1" rowspan="1">y</td>
              <td align="left" colspan="1" rowspan="1">z</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" colspan="1" rowspan="1">Precuneus</td>
              <td align="left" colspan="1" rowspan="1">3147</td>
              <td align="left" colspan="1" rowspan="1">R/L</td>
              <td align="left" colspan="1" rowspan="1">26.2</td>
              <td align="left" colspan="1" rowspan="1">−61.4</td>
              <td align="left" colspan="1" rowspan="1">37.4</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Angular Gyrus</td>
              <td align="left" colspan="1" rowspan="1">402</td>
              <td align="left" colspan="1" rowspan="1">L</td>
              <td align="left" colspan="1" rowspan="1">−51.2</td>
              <td align="left" colspan="1" rowspan="1">−57</td>
              <td align="left" colspan="1" rowspan="1">21.8</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Superior Frontal Gyrus</td>
              <td align="left" colspan="1" rowspan="1">305</td>
              <td align="left" colspan="1" rowspan="1">R</td>
              <td align="left" colspan="1" rowspan="1">27.2</td>
              <td align="left" colspan="1" rowspan="1">4.8</td>
              <td align="left" colspan="1" rowspan="1">57.8</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Frontal Pole</td>
              <td align="left" colspan="1" rowspan="1">175</td>
              <td align="left" colspan="1" rowspan="1">R</td>
              <td align="left" colspan="1" rowspan="1">38</td>
              <td align="left" colspan="1" rowspan="1">47.8</td>
              <td align="left" colspan="1" rowspan="1">−18.4</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Anterior Cingulate Gyrus</td>
              <td align="left" colspan="1" rowspan="1">166</td>
              <td align="left" colspan="1" rowspan="1">R</td>
              <td align="left" colspan="1" rowspan="1">4.6</td>
              <td align="left" colspan="1" rowspan="1">40.4</td>
              <td align="left" colspan="1" rowspan="1">26.8</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Fusiform Gyrus</td>
              <td align="left" colspan="1" rowspan="1">136</td>
              <td align="left" colspan="1" rowspan="1">L</td>
              <td align="left" colspan="1" rowspan="1">−14.2</td>
              <td align="left" colspan="1" rowspan="1">−81.6</td>
              <td align="left" colspan="1" rowspan="1">−20.8</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Inferior Frontal Gyrus</td>
              <td align="left" colspan="1" rowspan="1">133</td>
              <td align="left" colspan="1" rowspan="1">R</td>
              <td align="left" colspan="1" rowspan="1">52.6</td>
              <td align="left" colspan="1" rowspan="1">12.2</td>
              <td align="left" colspan="1" rowspan="1">20.6</td>
            </tr>
            <tr>
              <td align="left" colspan="1" rowspan="1">Posterior Cingulate</td>
              <td align="left" colspan="1" rowspan="1">53</td>
              <td align="left" colspan="1" rowspan="1">L</td>
              <td align="left" colspan="1" rowspan="1">−9.4</td>
              <td align="left" colspan="1" rowspan="1">−35.2</td>
              <td align="left" colspan="1" rowspan="1">39.6</td>
            </tr>
          </tbody>
        </table></alternatives><table-wrap-foot>
          <fn id="nt102">
            <label/>
            <p>X,Y,Z coordinates reflect the center of gravity of the cluster in MNI space.</p>
          </fn>
        </table-wrap-foot></table-wrap>
      <fig id="pone-0039384-g003" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0039384.g003</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Mask of voxels significantly correlated in all moral elevation videos, but not correlated during admiration for physical skill or neutral videos.</title>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0039384.g003" xlink:type="simple"/>
      </fig>
      <p>In order to demonstrate that the high inter-subject synchronization evoked by the elevation videos was related to feelings of positive emotion; we conducted an inter-subject synchronization analysis on the peak moments of the video, the moments that were rated most positive during the behavioral study described previously in this manuscript. For each of the three elevation videos, we repeated the inter-subject synchronization procedure to measure the average correlation coefficient within gray matter voxels during the peak moments. The average correlation coefficient was found to be higher during peak moments relative to the average correlation coefficient for the whole video for each of the three moral elevation (ME) videos (ME1–<italic>t</italic><sub>(44)</sub> = 11.94, <italic>p&lt;.</italic>001; ME2–<italic>t</italic><sub>(44)</sub> = 11.37, <italic>p&lt;.</italic>001; ME3–<italic>t</italic><sub>(44)</sub> = 9.128, <italic>p&lt;</italic>.001) (<xref ref-type="fig" rid="pone-0039384-g004">Figure 4</xref>).</p>
      <fig id="pone-0039384-g004" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0039384.g004</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Average correlation of gray matter voxels during peak moments vs. whole videos for the three moral elevation videos.</title>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0039384.g004" xlink:type="simple"/>
      </fig>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>The current study was concerned with brain systems supporting feelings of moral elevation (admiration for moral virtue) and admiration for the physical skill or talent of others. These “other-praising" emotions appear to be catalysts for positive behavioral change <xref ref-type="bibr" rid="pone.0039384-Algoe1">[5]</xref>, yet little is known about the brain mechanisms involved in the experience of these emotions. Unlike the basic emotions, the other-praising emotions cannot be easily elicited with static photographs displaying arousing content. Stimuli that are capable of evoking “other-praising" emotions are often complex and story-like, making the temporal model typically employed in fMRI analysis difficult to derive. In this study we used an inter-subject synchronization approach that has previously been used in fMRI studies featuring free-viewing of movies to demonstrate that audio-visual processing of complex stimuli occurs similarly across individuals. We adopted this approach to investigate the neural systems supporting the emotional response to excellence in others, specifically to test whether neural activity associated with moral elevation is different than the admiration for physical skill.</p>
      <p>Using the inter-subject synchronization technique, we discovered a striking amount of correlation in the BOLD signal across individuals. Replicating Hasson and colleagues <xref ref-type="bibr" rid="pone.0039384-Hasson1">[12]</xref>, we observed extensive correlation in the audio-visual cortex when participants freely-viewed video clips, regardless of the emotional content. Elevating videos were especially effective in eliciting high synchronization between subjects. Along with the audio-visual cortex, these videos produced significant correlation in anterior and posterior cingulate, precuneus, bilateral temporoparietal junction, superior frontal gyrus, inferior frontal gyrus, angular gyrus and insula. In contrast, admiration videos did not elicit synchrony in the BOLD signal as consistently, with the exception of those brain regions associated with audio-visual processing, and synchrony in the insula was observed for one of the admiration videos.</p>
      <p>In a subsequent analysis, we found voxels that were significantly correlated in all moral elevation video clips, but not in any of the other video clips. From this analysis, we found that regions in the anterior and posterior cingulate, precuneus, bilateral temporoparietal junction, superior frontal gyrus, inferior frontal gyrus, angular gyrus, and bilateral areas of the parietal operculum uniquely responded in the elevation condition. While many of the regions detected by this analysis are often implicated in “default" mode processing, it should be noted that the intersubject synchronization technique is not optimized to detect intrinsic connectivity <italic>within</italic> participants, but rather correlations <italic>between</italic> participants during stimulation. Therefore, activity in these regions is likely related to processing of the content of the video.</p>
      <p>Our findings closely parallel those found in a recent study by Immordino-Yang and colleagues. Here, the authors used brief narratives designed to evoke admiration and compassion for others. They report involvement of the posterior cingulate, retrosplinial cortex and precuneus – referred to as the posteromedial cortices (PMC) – when subjects experienced either admiration for the virtues of others or compassion for others’ psychological pain <xref ref-type="bibr" rid="pone.0039384-ImmordinoYang1">[11]</xref>. However, these same areas are not involved in the admiration for physical skill or compassion for physical pain. In their study, Immordino-Yang and colleagues used a recall paradigm where subjects were presented with emotional films before entering the scanner, shown refresher clips during scanning, and told specifically to induce the same emotional state as they experienced in the preparation session. Here we find a similar pattern of effects when subjects are presented with the emotional stimuli in the scanner and are not told to specifically attend to their own emotional state.</p>
      <p>While we are unable to pinpoint the precise role of these highly synchronized brain regions evoked by elevation videos, our findings do provide some unique insight into the mental operations that may lead to feelings of elevation. First, regions uniquely associated with peak moments of elevation are often implicated in mentalizing behavior – that is, the ability to make inferences about the beliefs and mind states of others <xref ref-type="bibr" rid="pone.0039384-Gallagher1">[17]</xref>–<xref ref-type="bibr" rid="pone.0039384-Saxe1">[18]</xref>. Second, midline structures including precuneus, posterior and anterior cingulate have been proposed to play a role in self-referential processes including constructing a social narrative, integrating emotional experience and action planning <xref ref-type="bibr" rid="pone.0039384-Fletcher1">[19]</xref>–<xref ref-type="bibr" rid="pone.0039384-Greene1">[21]</xref>. Furthermore, our moral elevation videos also evoked synchrony in bilateral parietal operculum, which is part of the secondary somatosensory cortex and thought to play a role in emotional experience. Because these same structures seem to support both self-referential processes and mentalizing abilities, some have speculated that one’s own self is modeled when making inferences about others – a process referred to as simulation. Our data suggest that moral elevation engages these same processes, leading to an interesting paradox. While moral elevation may involve feelings of self-transcendence – a <italic>reduction</italic> in attention to the self- the actual feeling of elevation is dependent upon an <italic>increase</italic> of self-referential processes.</p>
      <p>A potential limitation of the current investigation is that we did not collect any behavioral data from the group of subjects that were scanned. Because we wanted to replicate procedures previously successful in evoking high inter-subject synchronization during free-viewing, we chose to allow our subjects to freely view the stimulus without an overt task. Furthermore, we also wanted to test if the same pattern of activation previously reported in brain regions supporting self-referential processes would replicate even when subjects were not explicitly told to attend to their own emotional state. Future studies would be well served to combine the two approaches featured here. That is, moment-by-moment ratings of emotion could be collected while subjects view the videos in the scanner and used as a predictor of BOLD activity in candidate brain regions. This type of analysis would give us further evidence that the regions we report in this study are directly related to positive aspects of moral elevation.</p>
      <p>Overall, using this inter-subject correlation technique, we were able to present complex emotional stimuli to individuals undergoing fMRI. We demonstrated that a high degree of inter-subject correlation occurred in voxels responding to the emotional content of the videos, specifically within regions associated with interoceptive processing in the moral elevation condition.</p>
    </sec>
  </body>
  <back>
    <ack>
      <p>The authors would like to thank Meghan Cronk for assisting with participant scheduling and data collection.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pone.0039384-Ekman1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ekman</surname><given-names>P</given-names></name><name name-style="western"><surname>Friesen</surname><given-names>WV</given-names></name></person-group>             <year>1969</year>             <article-title>The repertoire of nonverbal behavior: Categories, origins, usage, and coding.</article-title>             <source>Semiotica</source>             <volume>1(1)</volume>             <fpage>49</fpage>             <lpage>98</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Ekman2">
        <label>2</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ekman</surname><given-names>PE</given-names></name><name name-style="western"><surname>Davidson</surname><given-names>RJ</given-names></name></person-group>             <year>1994</year>             <article-title>The Nature of Emotion: Fundamental Questions.</article-title>             <source>Oxford University Press</source>          </element-citation>
      </ref>
      <ref id="pone.0039384-Ortony1">
        <label>3</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ortony</surname><given-names>A</given-names></name><name name-style="western"><surname>Clore</surname><given-names>GL</given-names></name><name name-style="western"><surname>Collins</surname><given-names>A</given-names></name></person-group>             <year>1988</year>             <article-title>The cognitive structure of emotions.</article-title>             <source>Cambridge University Press</source>          </element-citation>
      </ref>
      <ref id="pone.0039384-Haidt1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haidt</surname><given-names>J</given-names></name><name name-style="western"><surname>Morris</surname><given-names>JP</given-names></name></person-group>             <year>2009</year>             <article-title>Finding the self in self-transcendent emotions.</article-title>             <source>Proceedings of the National Academy of Sciences</source>             <volume>106(19)</volume>             <fpage>7687</fpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Algoe1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Algoe</surname><given-names>SB</given-names></name><name name-style="western"><surname>Haidt</surname><given-names>J</given-names></name></person-group>             <year>2009</year>             <article-title>Witnessing excellence in action: The “other-praising" emotions of elevation, gratitude, and admiration.</article-title>             <source>The Journal of Positive Psychology</source>             <volume>4(2)</volume>             <fpage>105</fpage>             <lpage>127</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Haidt2">
        <label>6</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haidt</surname><given-names>J</given-names></name></person-group>             <year>2003</year>             <article-title>The moral emotions. In R. J. Davidson, K. R. Scherer, &amp; H. H. Goldsmith (Eds.), Handbook of affective sciences.</article-title>             <source>Oxford: Oxford University Press</source>             <comment>(852–870).</comment>          </element-citation>
      </ref>
      <ref id="pone.0039384-Schnall1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schnall</surname><given-names>S</given-names></name><name name-style="western"><surname>Roper</surname><given-names>J</given-names></name><name name-style="western"><surname>Fessler</surname><given-names>DMT</given-names></name></person-group>             <year>2010</year>             <article-title>Elevation leads to altruistic behavior.</article-title>             <source>Psychological Science</source>             <volume>21(3)</volume>             <fpage>315</fpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Lazarus1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lazarus</surname><given-names>RS</given-names></name></person-group>             <year>1991</year>             <article-title>Progress on a cognitive-motivational-relational theory of emotion.</article-title>             <source>American Psychologist</source>             <volume>46(8)</volume>             <fpage>819</fpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Smith1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>CA</given-names></name><name name-style="western"><surname>Ellsworth</surname><given-names>PC</given-names></name></person-group>             <year>1985</year>             <article-title>Patterns of cognitive appraisal in emotion.</article-title>             <source>Journal of Personality and Social Psychology</source>             <volume>48(4)</volume>             <fpage>813</fpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Haidt3">
        <label>10</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Haidt</surname><given-names>J</given-names></name></person-group>             <year>2006</year>             <article-title>The happiness hypothesis: Finding modern truth in ancient wisdom.</article-title>             <source>New York, NY: Basic Books</source>          </element-citation>
      </ref>
      <ref id="pone.0039384-ImmordinoYang1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Immordino-Yang</surname><given-names>MH</given-names></name><name name-style="western"><surname>McColl</surname><given-names>A</given-names></name><name name-style="western"><surname>Damasio</surname><given-names>H</given-names></name><name name-style="western"><surname>Damasio</surname><given-names>A</given-names></name></person-group>             <year>2009</year>             <article-title>Neural correlates of admiration and compassion.</article-title>             <source>Proceedings of the National Academy of Sciences</source>             <volume>106(19)</volume>             <fpage>8021</fpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Hasson1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hasson</surname><given-names>U</given-names></name><name name-style="western"><surname>Nir</surname><given-names>Y</given-names></name><name name-style="western"><surname>Levy</surname><given-names>I</given-names></name><name name-style="western"><surname>Fuhrmann</surname><given-names>G</given-names></name><name name-style="western"><surname>Malach</surname><given-names>R</given-names></name></person-group>             <year>2004</year>             <article-title>Intersubject synchronization of cortical activity during natural vision.</article-title>             <source>Science</source>             <volume>303(5664)</volume>             <fpage>1634</fpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Jenkinson1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jenkinson</surname><given-names>M</given-names></name><name name-style="western"><surname>Bannister</surname><given-names>P</given-names></name><name name-style="western"><surname>Brady</surname><given-names>M</given-names></name><name name-style="western"><surname>Smith</surname><given-names>S</given-names></name></person-group>             <year>2002</year>             <article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images.</article-title>             <source>Neuroimage</source>             <volume>17(2)</volume>             <fpage>825</fpage>             <lpage>841</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Smith2">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>SM</given-names></name></person-group>             <year>2002</year>             <article-title>Fast robust automated brain extraction.</article-title>             <source>Human Brain Mapping</source>             <volume>17(3)</volume>             <fpage>143</fpage>             <lpage>155</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Zhang1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Brady</surname><given-names>M</given-names></name><name name-style="western"><surname>Smith</surname><given-names>S</given-names></name></person-group>             <year>2001</year>             <article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm.</article-title>             <source>Medical Imaging, IEEE Transactions</source>             <volume>20(1)</volume>             <fpage>45</fpage>             <lpage>57</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Nichols1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nichols</surname><given-names>TE</given-names></name><name name-style="western"><surname>Holmes</surname><given-names>AP</given-names></name></person-group>             <year>2002</year>             <article-title>Nonparametric Permutation Tests for Functional Neuroimaging: A Primer with Examples.</article-title>             <source>Human Brain Mapping</source>             <volume>15(1)</volume>             <fpage>1</fpage>             <lpage>25</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Gallagher1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gallagher</surname><given-names>HL</given-names></name><name name-style="western"><surname>Frith</surname><given-names>CD</given-names></name></person-group>             <year>2004</year>             <article-title>Dissociable neural pathways for the perception and recognition of expressive and instrumental gestures.</article-title>             <source>Neuropsychologia</source>             <volume>42(13)</volume>             <fpage>1725</fpage>             <lpage>1736</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Saxe1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Saxe</surname><given-names>R</given-names></name><name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name></person-group>             <year>2003</year>             <article-title>People thinking about thinking people: The role of the temporo-parietal junction in.</article-title>             <source>Neuroimage</source>             <volume>19(4)</volume>             <fpage>1835</fpage>             <lpage>1842</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Fletcher1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fletcher</surname><given-names>PC</given-names></name><name name-style="western"><surname>Happe</surname><given-names>F</given-names></name><name name-style="western"><surname>Frith</surname><given-names>U</given-names></name><name name-style="western"><surname>Baker</surname><given-names>SC</given-names></name><name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name><etal/></person-group>             <year>1995</year>             <article-title>Other minds in the brain: A functional imaging study of “theory of mind" in story comprehension.</article-title>             <source>Cognition</source>             <volume>57(2)</volume>             <fpage>109</fpage>             <lpage>128</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Maddock1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Maddock</surname><given-names>RJ</given-names></name></person-group>             <year>1999</year>             <article-title>The retrosplenial cortex and emotion: new insights from functional neuroimaging of the human brain.</article-title>             <source>Trends in Neurosciences</source>             <volume>22(7)</volume>             <fpage>310</fpage>             <lpage>316</lpage>          </element-citation>
      </ref>
      <ref id="pone.0039384-Greene1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Greene</surname><given-names>J</given-names></name><name name-style="western"><surname>Haidt</surname><given-names>J</given-names></name></person-group>             <year>2002</year>             <article-title>How (and where) does moral judgment work?</article-title>             <source>Trends in Cognitive Sciences</source>             <volume>6(12)</volume>             <fpage>517</fpage>             <lpage>523</lpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>