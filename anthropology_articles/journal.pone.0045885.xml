<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta><journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><issn pub-type="epub">1932-6203</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PONE-D-12-13424</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0045885</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Natural language processing</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Learning and memory</subject>
              <subject>Neurolinguistics</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Computer science</subject>
          <subj-group>
            <subject>Natural language processing</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Medicine</subject>
          <subj-group>
            <subject>Mental health</subject>
            <subj-group>
              <subject>Psychology</subject>
              <subj-group>
                <subject>Cognitive psychology</subject>
                <subj-group>
                  <subject>Learning</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Anthropology</subject>
            <subj-group>
              <subject>Cultural anthropology</subject>
              <subj-group>
                <subject>Natural language</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Linguistic anthropology</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Communications</subject>
            <subj-group>
              <subject>Natural language</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Linguistics</subject>
            <subj-group>
              <subject>Natural language</subject>
              <subject>Neurolinguistics</subject>
              <subject>Psycholinguistics</subject>
              <subject>Structural linguistics</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Psychology</subject>
            <subj-group>
              <subject>Behavior</subject>
              <subj-group>
                <subject>Human performance</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Cognitive psychology</subject>
              <subj-group>
                <subject>Learning</subject>
                <subject>Memory</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Mental Health</subject>
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
          <subject>Computer Science</subject>
        </subj-group>
      </article-categories><title-group><article-title>Implicit Learning of Recursive Context-Free Grammars</article-title><alt-title alt-title-type="running-head">Implicit Learning of Recursion</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Rohrmeier</surname>
            <given-names>Martin</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Fu</surname>
            <given-names>Qiufang</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Dienes</surname>
            <given-names>Zoltan</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">
        <label>1</label>
        <addr-line>Cluster Languages of Emotion, Freie Universität Berlin, Berlin, Germany</addr-line>
      </aff><aff id="aff2">
        <label>2</label>
        <addr-line>State Key Laboratory of Brain and Cognitive Science, Institute of Psychology, Chinese Academy of Sciences, Beijing, China</addr-line>
      </aff><aff id="aff3">
        <label>3</label>
        <addr-line>Sackler Centre for Consciousness Science and School of Psychology, University of Sussex, Brighton, United Kingdom</addr-line>
      </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Bolhuis</surname>
            <given-names>Johan J.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">
        <addr-line>Utrecht University, The Netherlands</addr-line>
      </aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">mrohrmeier@cantab.net</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: MR QF. Performed the experiments: MR QF. Analyzed the data: MR QF. Contributed reagents/materials/analysis tools: MR. Wrote the paper: MR QF ZD. Implemented experiments and regression analyses: MR.</p>
        </fn>
      </author-notes><pub-date pub-type="collection">
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>19</day>
        <month>10</month>
        <year>2012</year>
      </pub-date><volume>7</volume><issue>10</issue><elocation-id>e45885</elocation-id><history>
        <date date-type="received">
          <day>11</day>
          <month>5</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>27</day>
          <month>8</month>
          <year>2012</year>
        </date>
      </history><permissions>
        
        <copyright-holder>Rohrmeier et al</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions><abstract>
        <p>Context-free grammars are fundamental for the description of linguistic syntax. However, most artificial grammar learning experiments have explored learning of simpler finite-state grammars, while studies exploring context-free grammars have not assessed awareness and implicitness. This paper explores the implicit learning of context-free grammars employing features of hierarchical organization, recursive embedding and long-distance dependencies. The grammars also featured the distinction between left- and right-branching structures, as well as between centre- and tail-embedding, both distinctions found in natural languages. People acquired unconscious knowledge of relations between grammatical classes even for dependencies over long distances, in ways that went beyond learning simpler relations (e.g. n-grams) between individual words. The structural distinctions drawn from linguistics also proved important as performance was greater for tail-embedding than centre-embedding structures. The results suggest the plausibility of implicit learning of complex context-free structures, which model some features of natural languages. They support the relevance of artificial grammar learning for probing mechanisms of language learning and challenge existing theories and computational models of implicit learning.</p>
      </abstract><funding-group>
        <funding-statement>This research was supported in part by grants from 973 Program of Chinese Ministry of Science and Technology (2011CB302201), the National Natural Science Foundation of China (30900395). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group><counts>
        <page-count count="15"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Humans seem to possess a remarkable facility to grasp new structures from the environment and generalise the use of this knowledge to other stimuli and domains <xref ref-type="bibr" rid="pone.0045885-Reber1">[1]</xref>. We are able to master complex everyday activities such as steering a car along bends and in traffic <xref ref-type="bibr" rid="pone.0045885-Land1">[2]</xref>, intercepting thrown objects <xref ref-type="bibr" rid="pone.0045885-Reed1">[3]</xref>, playing sports <xref ref-type="bibr" rid="pone.0045885-Masters1">[4]</xref>, hearing words in continuous speech <xref ref-type="bibr" rid="pone.0045885-Saffran1">[5]</xref>, or improvising music in an ensemble <xref ref-type="bibr" rid="pone.0045885-Bigand1">[6]</xref>–<xref ref-type="bibr" rid="pone.0045885-Rohrmeier1">[9]</xref> without full awareness of the knowledge enabling such activities. Similarly, native speakers of a language are able to understand and produce sentences without being able to fully articulate the grammatical rules they are applying. Children acquire and use grammatical knowledge from mere exposure or interaction with very little explicit input or teaching <xref ref-type="bibr" rid="pone.0045885-Chomsky1">[10]</xref>. First and second language acquisition thus constitutes a prototypical case and one main example for implicit learning (e.g. <xref ref-type="bibr" rid="pone.0045885-Cleeremans1">[11]</xref>–<xref ref-type="bibr" rid="pone.0045885-Rebuschat2">[15]</xref>).</p>
      <p>Most artificial grammar learning studies apply finite-state grammars, using letters, syllable sequences, tones, melodies, timbres, or visual symbols as terminals, producing strong evidence for implicit learning <xref ref-type="bibr" rid="pone.0045885-Altmann1">[16]</xref>–<xref ref-type="bibr" rid="pone.0045885-Rohrmeier2">[24]</xref>. Similarly, numerous studies have investigated the acquisition of linguistic phrase structure using finite state grammars <xref ref-type="bibr" rid="pone.0045885-Braine1">[25]</xref>–<xref ref-type="bibr" rid="pone.0045885-Opitz2">[32]</xref>. In the same context many studies have found that adults and children are able to learn different features of language based on their statistical properties and features such as word segmentation and word categories, without needing to refer to phrase structure grammar per se <xref ref-type="bibr" rid="pone.0045885-Saffran1">[5]</xref>, <xref ref-type="bibr" rid="pone.0045885-Aslin1">[33]</xref>–<xref ref-type="bibr" rid="pone.0045885-Redington3">[40]</xref>. Such research raises the question of what needs to be shown to demonstrate the implicit acquisition of linguistic syntax. Are finite state grammars and statistical learning sufficient to account for human implicit learning and language learning? However, as argued by <xref ref-type="bibr" rid="pone.0045885-Chomsky2">[41]</xref>–<xref ref-type="bibr" rid="pone.0045885-Chomsky3">[42]</xref>, finite state grammars are not sufficiently expressive to capture linguistic syntactic recursion and the modularity and hierarchical organisation of constituents and phrases. The complexity of at least context-free grammars is required to capture these features (see e.g. <xref ref-type="bibr" rid="pone.0045885-Chomsky2">[41]</xref>, <xref ref-type="bibr" rid="pone.0045885-Pullum1">[43]</xref>–<xref ref-type="bibr" rid="pone.0045885-Lobina3">[46]</xref> for a discussion). To our knowledge, only a few studies have explored implicit learning beyond finite-state complexity (see below, <xref ref-type="bibr" rid="pone.0045885-Dienes2">[47]</xref>–<xref ref-type="bibr" rid="pone.0045885-Jiang1">[51]</xref>).</p>
      <p>The purpose of this study is to investigate implicit learning of a linguistic context-free grammar above finite-state grammar complexity. The structure of the sequences produced by the grammars used in the study embody distinctive features of recursion <xref ref-type="bibr" rid="pone.0045885-Tomalin1">[52]</xref>, in particular, nested or tail recursion (see below), and hence involve hierarchical organisation and long-distance dependencies (note that we use the notion of hierarchical structure in the sense of <italic>hierarchically nested dependency</italic> relationships according with the definitions of recursion in <xref ref-type="bibr" rid="pone.0045885-Martins1">[53]</xref> and the distinctions between formal languages drawn by <xref ref-type="bibr" rid="pone.0045885-Fitch2">[54]</xref>). In this context the study links to the recent debates about the learnability of recursive, centre-embedded structures in the cognitive sciences (see below).</p>
    </sec>
    <sec id="s2">
      <title>Background</title>
      <p>One central aspect of language syntax concerns the organisation of words, constituents and phrases in nested, recursive ways <xref ref-type="bibr" rid="pone.0045885-Pullum1">[43]</xref>, <xref ref-type="bibr" rid="pone.0045885-Chomsky4">[55]</xref>. An example would be “the old garden at the rear of the house” which acts as a noun phrase like the single “the garden”; as a noun phrase both could fit the context “… is beautiful”. Similarly, the words in the sentence “the Labrador which chased the poodle that was hiding barked” fall into recursively dependent constituents: the Labrador [which chased the poodle [that was hiding]] barked. The understanding of a sentence like the above requires the correct parse of the syntactic and semantic dependencies to reconstruct the appropriate sentence meaning (which of the two dogs barked?). Generally, however, parsing of semantic and syntactic dependencies interact (see also <xref ref-type="bibr" rid="pone.0045885-Ferreira1">[56]</xref>).</p>
      <p>Another example of a recursive German sentence would be: “Hans sagte, dass Peter Maria dem Mann den Zaun streichen helfen sah.” (“Hans said that Peter saw Maria help the man paint the fence.”). Embedded relative clauses like the English or German examples involve recursive nested hierarchical embedding and nonadjacent dependencies (e.g. “the Labrador …. barked”, “Maria … helfen”). In the English example there are two instances of nested tail-recursion (each embedded sequence adjacent to the end of a sequence, e.g. “the poodle [that was hiding]”; we henceforth refer to embedded structures generated by tail-recursion as “tail-embedding”) and one instance of centre-embedding (“the Labrador […] barked”). In contrast the German sentence features three instances of recursive nested centre-embedding (“[Peter [Maria [dem Mann … streichen] helfen] sah]”). The fact that the dependencies in either language are generated recursively entails that they are potentially unbounded and infinite in the sense that there is no theoretical upper limit for the number of tail- or centre-embedded structures if the pattern would be continued (not considering limitations of performance such as working memory). These potentially unbounded nested dependencies constitute the core of the argument for recursion at the heart of the human language faculty (<xref ref-type="bibr" rid="pone.0045885-Chomsky2">[41]</xref>, cf <xref ref-type="bibr" rid="pone.0045885-Lobina1">[44]</xref>–<xref ref-type="bibr" rid="pone.0045885-Lobina3">[46]</xref>, <xref ref-type="bibr" rid="pone.0045885-Hauser1">[57]</xref>–<xref ref-type="bibr" rid="pone.0045885-Zwart1">[58]</xref>).</p>
      <p>Finite-state grammars can express simple forms of tail-recursion and limited nonadjacent dependencies. This constitutes a difference between finite-state grammars (as defined by rewrite rules that only add elements to one side) and Markov models (i.e. as represented by a table of transition probabilities). While they largely overlap, they are different <xref ref-type="bibr" rid="pone.0045885-Chomsky2">[41]</xref>. demonstrates that a finite-state grammar (in notable contrast to n-th order Markov or n-gram models) can express an unbounded nonlocal dependency using tail recursion (e.g. AX*B | CX*D). In words, the expression means: a set of sequences in which either A is followed by any number of X and B or C followed by any number of X and D. Hence an initial A implies B after any number of X, and the same for C and D. Therefore any Markov or n-gram model of finite length will not be able to express this (unlimited) nonlocal dependency although it easily be constituted by a simple finite-state grammar.</p>
      <p>Context-free grammars in contrast can express forms of nested dependencies that can be proven not to be finite-state. Unbounded recursively nested dependencies like UA<sup>n</sup>VB<sup>n</sup>W (where U, V, W may be any sequence of terminal events or empty) can be expressed by context-free, but not finite-state grammars. The German example above constitutes a sentence that exhibits this type of dependency “Peter<sub>1</sub> Maria<sub>2</sub> dem Mann<sub>3</sub> streichen<sub>3</sub> helfen<sub>2</sub> sah<sub>1</sub>”.</p>
      <p>Research exploring learning or processing of recursion and context-free grammars bears one particular caveat: The difference between context-free and finite-state grammars relates to potentially unbounded dependencies while, trivially, a finite set of sequences can be expressed by an all encompassing finite-state grammar. However, an unlimited number of dependencies cannot be explored experimentally. On the other hand, finite-state grammars expressing nonadjacent dependencies or finite examples of context-free sequences are redundant: for instance, AX*B | CX*D encodes the identical intermitting X* twice, and a finite-state grammar encoding general nonlocal dependencies between <italic>n</italic> pairs of symbols has to represent the intermitting sequences <italic>n</italic> times (second or third order embeddings would accordingly let the number of multiple representations grow exponentially). Thus, although it may be possible to express such bounded structures by a finite-state grammar, a context-free grammar achieves a more parsimonious representation. Sometimes the simpler psychological explanation for what has been learned will involve a grammar higher up the Chomsky hierarchy.</p>
      <p>Various research has been performed in this line of research linking the field of implicit learning with syntax acquisition and recursion (cf <xref ref-type="bibr" rid="pone.0045885-deVries1">[59]</xref>–<xref ref-type="bibr" rid="pone.0045885-Friederici4">[60]</xref>). In order to put our study in context we systematically review existing research on (not necessarily implicit) learning of recursion, nonadjacent dependencies and word classes.</p>
      <sec id="s2a">
        <title>Recursion and context-free structure</title>
        <p>The exploration of the learning of realistic features of context-free grammars is linked with one current cognitive debate concerning the processing and learnability of recursive structures. Recursion is argued to be situated at the heart of the human faculty of language (e.g. <xref ref-type="bibr" rid="pone.0045885-Fitch2">[54]</xref>, <xref ref-type="bibr" rid="pone.0045885-Hauser1">[57]</xref>, <xref ref-type="bibr" rid="pone.0045885-Pinker1">[61]</xref>. Hierarchically nested structures and recursion in various forms of human communication, such as language, music <xref ref-type="bibr" rid="pone.0045885-Jackendoff1">[62]</xref>–<xref ref-type="bibr" rid="pone.0045885-Patel1">[66]</xref> as well as planned action have been argued to be unique to human cognition <xref ref-type="bibr" rid="pone.0045885-Fitch1">[48]</xref>–<xref ref-type="bibr" rid="pone.0045885-Friederici3">[49]</xref>, <xref ref-type="bibr" rid="pone.0045885-Jackendoff2">[67]</xref>. <xref ref-type="bibr" rid="pone.0045885-Jackendoff2">[67]</xref>–<xref ref-type="bibr" rid="pone.0045885-Jackendoff3">[68]</xref> situates the hierarchical organisation of language and music within a broader human capacity of recursion, a position that is similarly argued by <xref ref-type="bibr" rid="pone.0045885-Steedman2">[69]</xref>. In this context, the question of how humans form, acquire and manage complex recursively embedded hierarchical structures constitutes a core question in the area. Again, in line with <xref ref-type="bibr" rid="pone.0045885-Martins1">[53]</xref>, hierarchical organisation entails the representation of dependency relationships between constituents (at multiple levels but not necessarily based on the same principles or rules). Recursive embedding entails the representation of dependency relationships based on the same rule or principle; the recursive nature of the embedding step further entails that the resulting hierarchical organisation generalises to levels of embedding that are potentially unbounded and may not be observed.</p>
        <p>Context-free grammars, or phrase-structure grammars, constitute the simplest form of grammars to embody features of unbounded nested embeddings in the Chomsky hierarchy <xref ref-type="bibr" rid="pone.0045885-Chomsky2">[41]</xref>, <xref ref-type="bibr" rid="pone.0045885-Chomsky5">[70]</xref>. The Chomsky hierarchy characterises four types of formal languages <xref ref-type="bibr" rid="pone.0045885-Chomsky5">[70]</xref>–<xref ref-type="bibr" rid="pone.0045885-Hopcroft1">[71]</xref> of increasing complexity: regular or finite state languages, context-free languages, context-sensitive languages and recursively enumerable languages. The types of grammar which produce these languages differ by systematic steps of generalisation of the form of the rewrite rules. Whereas finite-state grammar rules embody the most restrictions, the top level (type-0) rules are entirely unrestricted. By virtue of dropping restrictions, every more complex grammar and language becomes a superset of the less complex grammar or language. Accordingly, context-free grammars include all finite-state grammars, and context-sensitive grammars include all context-free grammars and finite state grammars. Thus, there are grammars employing context-free rules which are in fact expressible by finite-state grammars. The core differences between finite-state and context-free languages lie in the features of recursive, centre-embedded structures <xref ref-type="bibr" rid="pone.0045885-Hopcroft1">[71]</xref>.</p>
        <p>The current empirical evidence about the learning and perception of hierarchical recursive structures is ambiguous, and, as a result, discussion in the area is ongoing. Several studies employed very simple grammars of the type A<sup>n</sup>B<sup>n</sup> and variants of it: <xref ref-type="bibr" rid="pone.0045885-Fitch1">[48]</xref> argued to have found evidence for learning of simple regular (finite-state) and nonregular structures (AB)<sup>n</sup> vs. A<sup>n</sup>B<sup>n</sup> in two species. <xref ref-type="bibr" rid="pone.0045885-Friederici5">[72]</xref>–<xref ref-type="bibr" rid="pone.0045885-Bahlmann2">[75]</xref>, using similar methodology, found two different brain regions are associated with the acquisition of finite-state and context-free grammars. In contrast <xref ref-type="bibr" rid="pone.0045885-Hochmann1">[76]</xref>, did not find that participants were able to acquire specific features of the grammar used by Fitch and Hauser. Similarly <xref ref-type="bibr" rid="pone.0045885-Perruchet1">[77]</xref>, argued that Fitch and Hauser's original experiments contained a methodological flaw and found people could not learn the grammar A<sub>1</sub>A<sub>2</sub>A<sub>3</sub>B<sub>3</sub>B<sub>2</sub>B<sub>1</sub> which forced hierarchical embedding for its recognition under incidental learning conditions (the grammar A<sup>n</sup>B<sup>n</sup> could be simply distinguished based on mere word class counting). Similarly <xref ref-type="bibr" rid="pone.0045885-deVries2">[78]</xref> argued that participants using the Friederici et al material engaged explicitly in counting strategies and did not learn the hierarchical structure per se. Thus, the simplistic and reduced case of an A<sup>n</sup>B<sup>n</sup> language may not provide sufficient context and grammatical complexity for people to generalise a genuine context-free grammar. In a recent study, however <xref ref-type="bibr" rid="pone.0045885-Lai1">[79]</xref>, (see also <xref ref-type="bibr" rid="pone.0045885-Poletiek1">[80]</xref>) argued that sufficient exposure to exemplars without embedding (zero level embedding) and staged input may explain found differences regarding the learnability of A<sup>n</sup>B<sup>n</sup> grammars <xref ref-type="bibr" rid="pone.0045885-Thompson1">[81]</xref>. argued that an increase in complexity could help rather than hinder people to learn grammatical structures. This provides the motivation why the present study adopted more complex context-free grammatical structures which employed more features of a natural syntax as materials.</p>
        <p>In an impressive study <xref ref-type="bibr" rid="pone.0045885-Uddn1">[50]</xref>, trained subjects for 30 minutes on letter strings instantiating crossed or nested dependencies (as indexed variants of A<sup>n</sup>B<sup>n</sup>) on each of nine days. People could discriminate grammatical from non-grammatical strings after this extensive training, yet could not say which letters were paired as dependents. While their results may be due to people unconsciously learning hierarchical structure, there remains a confound in their materials. We know already that people learn the repetition structures of letter strings, i.e. in which positions of a string letters are repeats of which other positions <xref ref-type="bibr" rid="pone.0045885-Brooks1">[82]</xref>–<xref ref-type="bibr" rid="pone.0045885-Tunney1">[84]</xref>. Grammaticality was completely confounded with repetition structure, and if people had consciously learned repetition structure, it would explain people's classification performance and poor verbal report of the hierarchical dependencies. Thus, the issue of whether people can unconsciously learn hierarchical structure remains open.</p>
        <p>As discussed above, there are several other studies which employ sequences which are produced from context-free grammars: the structures used by <xref ref-type="bibr" rid="pone.0045885-Friederici5">[72]</xref> and by <xref ref-type="bibr" rid="pone.0045885-Saffran2">[34]</xref>, <xref ref-type="bibr" rid="pone.0045885-Saffran4">[85]</xref>. Whereas the above A<sup>n</sup>B<sup>n</sup> structures were proved to be irreducibly context-free, the grammars used by <xref ref-type="bibr" rid="pone.0045885-Saffran2">[34]</xref> and <xref ref-type="bibr" rid="pone.0045885-Saffran4">[85]</xref> can be expressed by a finite-state grammar. For instance <xref ref-type="bibr" rid="pone.0045885-Opitz1">[31]</xref>, showed a finite-state representation of the grammar they used in several studies (called BROCANTO) and a similar step could be done for the studies by Saffran <xref ref-type="bibr" rid="pone.0045885-Saffran2">[34]</xref>, <xref ref-type="bibr" rid="pone.0045885-Saffran4">[85]</xref> (see <xref ref-type="supplementary-material" rid="pone.0045885.s001">Appendix S1</xref>). Accordingly, although Saffran's grammars and BROCANTO incorporate features of realistic sequential linguistic word order, they do not incorporate the prototypical context-free features of nested centre-embedding and multiple (potentially unbounded) long-distance dependencies that are required for context-free grammar complexity and that characterise one distinctive feature of human linguistic structures. Moreover, most of the studies relevant to learning context free grammars do not integrate measures of awareness to investigate the extent to which the acquired knowledge is implicit (unconscious) or explicit (conscious) into their methodology <xref ref-type="bibr" rid="pone.0045885-Rebuschat2">[15]</xref>. Further, only a few studies relating to second language acquisition employ conditions known to be conducive to implicit learning (for good examples see <xref ref-type="bibr" rid="pone.0045885-Williams1">[13]</xref>, <xref ref-type="bibr" rid="pone.0045885-Guo1">[86]</xref>–<xref ref-type="bibr" rid="pone.0045885-Williams4">[91]</xref>.</p>
      </sec>
      <sec id="s2b">
        <title>Learning long-distance dependencies</title>
        <p>A feature that is closely related to the above debate is nonadjacent dependencies, as centre-embedding context free grammars imply long distance dependencies <xref ref-type="bibr" rid="pone.0045885-Williams2">[14]</xref>. pointed out that nonadjacent dependencies have not been sufficiently explored yet from a statistical or implicit learning perspective. Using letters as stimuli, people can learn repetition patterns across stimuli <xref ref-type="bibr" rid="pone.0045885-Tunney1">[84]</xref>, <xref ref-type="bibr" rid="pone.0045885-Vokey1">[92]</xref>, a simple form of nonadjacent dependency. However, under the standard conditions used in artificial grammar learning studies, people do not implicitly learn nonlocal distance associations between letters which are not repeats (in the biconditional grammars of <xref ref-type="bibr" rid="pone.0045885-Johnstone1">[93]</xref>, and <xref ref-type="bibr" rid="pone.0045885-Mathews1">[94]</xref>. <xref ref-type="bibr" rid="pone.0045885-Newport1">[95]</xref> also did not find evidence of learning of nonadjacent dependencies in syllable sequences. However, they found adults could acquire long-distance dependency relationships, only between literally nonadjacent vowels (or consonants) which actually constituted successive vowels (or consonants). <xref ref-type="bibr" rid="pone.0045885-Creel1">[96]</xref> found learning of nonadjacent dependencies in tone sequences only when the relevant structures were separated from the surrounding structures by auditory streaming. Consistently <xref ref-type="bibr" rid="pone.0045885-Gebhart1">[97]</xref>, also found that nonadjacent dependencies between non-musical noises could be learnt only when perceptual similarity cues were introduced.</p>
        <p>In sum, it has been difficult to find learning of long distance dependencies in the lab when simple perceptual cues did not direct attention to corresponding elements. Such research does not bode well for finding implicit learning of phrase structure in the lab, as the long distance dependencies in the research just reviewed were not even as complex as those instantiating phrase structure grammar. However, when dependencies have been put into a context of more structure, long distance dependencies have been learned in the lab. <xref ref-type="bibr" rid="pone.0045885-Gomez1">[98]</xref> found leaning across an intervening element when the intervening element was variable. <xref ref-type="bibr" rid="pone.0045885-Dienes2">[47]</xref> and <xref ref-type="bibr" rid="pone.0045885-Kuhn1">[99]</xref> found that when the long distance dependencies were structured (namely, by forming a musical inversion, retrograde or transpose, which cannot be expressed through finite state grammars), they were learned (see also <xref ref-type="bibr" rid="pone.0045885-Jiang1">[51]</xref>, <xref ref-type="bibr" rid="pone.0045885-Dienes3">[100]</xref>, <xref ref-type="bibr" rid="pone.0045885-Dienes4">[101]</xref>). Perhaps placing long distance dependencies in certain ecological context-free structures actually helps learning.</p>
      </sec>
      <sec id="s2c">
        <title>Implicit learning of word classes</title>
        <p>Learning a natural phrase-structure grammar involves not only the learning of the syntactic dependency structure, but also distinguishing terminal elements (i.e. the elements forming the sequence: words in a sentence, notes in a melody, etc) from grammatical classes and acquiring knowledge about the relationships between terminals and grammatical class. For example, when learning English, one needs to infer which word class (i.e. noun, verb, adjective etc) each word (the terminals) belongs to, and the relationship between the word classes. Several studies have explored learning of word classes. A few studies have investigated the learning of classes in the artificial grammar learning paradigm. For example <xref ref-type="bibr" rid="pone.0045885-Reeder1">[102]</xref>, applied a simple finite state grammar (Q)AXB(B) in which any of the categories were realised by two or three words each. They found that participants trained on sequences from that system were able to generalise to new (unseen) strings that conformed to the abstract classes. However, the study did not test for awareness or implicitness of the acquired knowledge. One of our aims will be to explore whether relations between grammatical classes can be implicitly learned.</p>
      </sec>
      <sec id="s2d">
        <title>Motivation</title>
        <p>To explore whether people can <italic>unconsciously</italic> learn context-free structures that are more advanced than A<sup>n</sup>B<sup>n</sup> and reflect some natural linguistic patterns (following <xref ref-type="bibr" rid="pone.0045885-Thompson1">[81]</xref>, as above), the present study adopts simplified linguistic context-free grammars, which generate recursive, centre-embedded structures. The artificial context-free grammars were designed to resemble some natural linguistic structures in an abstract way and also to feature a set of different word classes and terminals. To condense the discussed linguistic features into a small set of artificial grammar rules, we chose grammars similar to <xref ref-type="bibr" rid="pone.0045885-Saffran2">[34]</xref>, <xref ref-type="bibr" rid="pone.0045885-Saffran4">[85]</xref>. However, the aim of this study was to model specifically embedded structures such as “The dog [who chased the cat [that was hiding]] barked”. The surface structures were chosen to be sentences of monosyllabic words in the auditory domain to correspond roughly to ecological listening conditions.</p>
        <p>A key difference between phrase-structure grammars is whether they are right branching (as in English) or left branching (as in Chinese). Thus, we will use two variants of a grammar, i.e. a right branching grammar and a left branching grammar, to explore the relevance of this distinction for adult implicit learning. In addition, we will have two further variants of each of these grammars, reflecting another distinction between natural language grammars <xref ref-type="bibr" rid="pone.0045885-Hawkins1">[103]</xref>, <xref ref-type="bibr" rid="pone.0045885-Hawkins2">[104]</xref>, namely centre-embedding or tail-embedding (or nested recursion vs. tail recursion). The structures we are using will feature up to three levels of embedding, which we refer to henceforth as “layer” 1,2 and 3. Their difference amounts to whether or not the third layer is centre or tail embedded in the second. For example, consider the English sentence “The dog, [who chased the cat, [who caught the mouse]], barked”. “The dog barked” would be the first layer; “who chased the cat” would be the second, and “who caught the mouse” would be the third. Note the third relative clause is tail-embedded and hence adjacent to the second (in terms of word order) rather than being centre-embedded. Now consider its German equivalent “Der Hund, [der die Katze, [die die Maus fing], jagte], bellte”(transliteration: “The dog, [who the cat, [who the mouse caught], chased], barked”), where the third relative clause is embedded in the middle of the second. From a cognitive perspective we would predict that the word order of the simpler former (adjacent) case would be easier to learn than the latter (centre-embedded) structure. With respect to our grammars, we will refers to this difference as “tail-embedding” versus “centre embedding” grammars.</p>
        <p>The following grammar was chosen as the tail embedding, right branching grammar (in analogy with English):</p>
        <list list-type="order">
          <list-item>
            <p>S→NP VP</p>
          </list-item>
          <list-item>
            <p>VP→V<sub>1</sub> | V<sub>2</sub> NP</p>
          </list-item>
          <list-item>
            <p>NP→N | N CP</p>
          </list-item>
          <list-item>
            <p>CP→R VP</p>
          </list-item>
        </list>
        <p>The rules of this abstract grammar intend to model simple linguistic relationships: it describes main sentence (S), consisting of a noun phrase (NP) and a verbal phrase (VP), and a simple complementiser phrase (CP). The grammar contains three classes of words that we have glossed as verbs (V), nouns (N) and a relative-clause marker/relative pronoun (R), which model the corresponding natural language classes in an abstract way. Here, S, VP, NP, and CP denote nonterminals and V, N, R denote terminals. Rule 1 indicates that the sentence consists of a noun phrase and a verbal phrase. Rule 2 indicates a verbs can be combined with an additional noun (modelling a distinction similar to transitive or intransitive verbs). The rule distinguishes verbs that entail another NP (V<sub>2</sub>) or verbs that do not (V<sub>1</sub>). Rule 3 indicates that noun phrases can consist of a single noun or a noun with a complementiser phrase attached. Rule 4 indicates that a complementiser phrase is made by a verbal phrase and a marker R which creates the potential of recursive generation, as it enables a VP to recursively be attached to an NP. This optional recursive production ensures that the production process terminates. Thus, the grammatical rules are similar to realistic structures in an abstract way.</p>
        <p>When rules (4) and (2) are used to rewrite rule (3), there are three forms of NP with increasing complexity: NP→N | N R V | N [R V NP]. The third form shows clearly the centre-embedding of the relative clause with respect to the main clause (only). This structure is exemplified in the sequence “[The dog [who chased the cat [that was hiding]] barked]”. The sentence is made by an NP “The dog who chased [the cat that was hiding]” with the structure of “N [R V N [R V]]” and a VP which is made by a V “barked”. <xref ref-type="fig" rid="pone-0045885-g001">Figure 1</xref> displays two different sequences created by the grammar. The tree structure illustrates how non-adjacent dependencies are produced in this grammar, and how the structure “R V (N)” is generated. The grammar creates right-branching dependencies as the relative clause “R V N” is joined to the right of a noun (like “The boy who kissed the girl”).</p>
        <fig id="pone-0045885-g001" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0045885.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Right-branching grammatical structure trees allowed by the tail-embedding right branching grammar.</title>
            <p>Each subordinate CP corresponds with an embedded layer ( e.g. layer 2 on the left tree, and layer 2 &amp; 3 on the right tree). Note that there is no centre-embedding on the left tree, while there is centre-embedding in the right tree with respect to the top NV structure).</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.g001" xlink:type="simple"/>
        </fig>
        <p>For the tail embedding left-branching condition, the corresponding rules are:</p>
        <list list-type="order">
          <list-item>
            <p>S→NP VP<sub>1</sub></p>
          </list-item>
          <list-item>
            <p>VP<sub>1</sub>→V<sub>1</sub> | V<sub>2</sub> NP</p>
          </list-item>
          <list-item>
            <p>VP<sub>2</sub>→V<sub>1</sub> | NP V<sub>2</sub></p>
          </list-item>
          <list-item>
            <p>NP→N | CP N</p>
          </list-item>
          <list-item>
            <p>CP→VP<sub>2</sub> R</p>
          </list-item>
        </list>
        <p>Now consider the centre embedding grammars. For the right-branching grammar (in analogy with embedding structures in German), the corresponding rules are:</p>
        <list list-type="order">
          <list-item>
            <p>S→NP VP<sub>1</sub></p>
          </list-item>
          <list-item>
            <p>(2a) VP<sub>1</sub>→V<sub>1</sub> | V<sub>2</sub> N</p>
          </list-item>
          <list-item>
            <p>(2b) VP<sub>2</sub>→V<sub>1</sub> | NP V<sub>2</sub></p>
          </list-item>
          <list-item>
            <p>NP→N | N CP</p>
          </list-item>
          <list-item>
            <p>CP→R VP<sub>2</sub></p>
          </list-item>
        </list>
        <p>To generate centre embedding left-branching grammatical structures (in analogy with embedding structures in Chinese) the corresponding rules are:</p>
        <list list-type="order">
          <list-item>
            <p>S→NP VP</p>
          </list-item>
          <list-item>
            <p>VP→V<sub>1</sub> | V<sub>2</sub> NP</p>
          </list-item>
          <list-item>
            <p>NP→N | CP N</p>
          </list-item>
          <list-item>
            <p>CP→VP R</p>
          </list-item>
        </list>
        <p><xref ref-type="fig" rid="pone-0045885-g002">Figure 2</xref> displays two different sequences created by the centre-embedding left-branching grammar. The tree structure illustrates the way in which nonlocal dependencies are produced in the left-branching grammar, and how the structure “V NP R” is recursively embedded. To generate the final surface sentences, each of the terminal symbols V, N, R in each abstract structure was randomly replaced by one of a set of corresponding monosyllabic words for each class.</p>
        <fig id="pone-0045885-g002" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0045885.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Left-branching grammatical structure trees allowed by the centre-embedding left branching grammar.</title>
            <p>(note that the subordinate CP embedding creates a nonlocal dependencies on the superordinate level).</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.g002" xlink:type="simple"/>
        </fig>
        <p>As outlined above, the purpose of the experiment was to use these four different artificial grammars to investigate whether people can become unconsciously sensitive to different types of recursive context-free grammars.To explore whether people can be sensitive to violations of the nested recursive structure, two thirds of the ungrammatical structures were designed violating only one embedded structure with the other levels remaining grammatically intact. Accordingly, a violation may span across an embedding. Sensitivity to such a violation would provide prima facie evidence of learning long-distance dependencies created by a recursive hierarchical grammar. However, we already know from past research that people are sensitive to bigram and trigram frequencies <xref ref-type="bibr" rid="pone.0045885-Reber4">[105]</xref>–<xref ref-type="bibr" rid="pone.0045885-Knowlton1">[107]</xref> and to repetition structure <xref ref-type="bibr" rid="pone.0045885-Brooks1">[82]</xref>–<xref ref-type="bibr" rid="pone.0045885-Tunney1">[84]</xref>. Thus, we statistically control these variables (contrast e.g. <xref ref-type="bibr" rid="pone.0045885-Uddn1">[50]</xref>, <xref ref-type="bibr" rid="pone.0045885-Mueller1">[108]</xref>; further, a preliminary computational analysis suggested that it was not possible to balance grammatical and ungrammatical stimuli for indistinguishable bi- and trigram frequencies). The variables will be controlled at both the level of terminals (e.g. the actual word bigrams people were exposed to) and classes (e.g. the sequence noun-verb is a particular bigram). Implicit sensitivity to class-level n-grams and repetitions independent of terminal-level n-grams and repetition is itself an interesting independent question important for implicit learning research. In this context the present experiment contributes to research on the limits of what can be learned implicitly, as well as the role of implicit learning in first and second language learning (cf <xref ref-type="bibr" rid="pone.0045885-Williams1">[13]</xref>–<xref ref-type="bibr" rid="pone.0045885-Rebuschat2">[15]</xref>). In addition, secondary questions concerned whether branching type and grammatical complexity would influence the acquisition of the phrase structures and whether participants' native language (Chinese) would affect the proficiency of learning of the type of grammar. The centre embedding left-branching structures were consistent with the grammatical structures of participants' native language while the right-branching and tail embedding structures were not. On the other hand, finding that people can incidentally and implicitly learn context-free grammars will be an interesting challenge for computational models of implicit learning (cf <xref ref-type="bibr" rid="pone.0045885-Kuhn2">[109]</xref>), since they predominantly tend to be good at learning chunks and associations <xref ref-type="bibr" rid="pone.0045885-Cleeremans2">[110]</xref>.</p>
        <p>We are interested in the structures that can be learnt <italic>implicitly</italic> or <italic>unconsciously</italic>. The fact that people learn programming languages intentionally and consciously means there is little novelty in showing people can <italic>consciously</italic> learn artificial context-free grammars. People manifestly do this every day. However, whether structures more complex than chunks, and in this case produced by context-free grammars, can be learned <italic>implicitly</italic> by adults remains an important open question. For this purpose we chose to employ the Process Dissociation Procedure (cf <xref ref-type="bibr" rid="pone.0045885-Jacoby1">[111]</xref>) as well as additional confidence judgments for assessing the conscious status of the acquired knowledge.</p>
      </sec>
    </sec>
    <sec id="s3" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <sec id="s3a">
        <title>Participants</title>
        <p>We recruited four groups to be trained on either tail or centre embedding structures and either left-branching or right-branching grammatical structures in the training phase. Correspondingly, we recruited four control groups for these conditions. One hundred and sixty-one undergraduate students (77 male, 84 female) in Beijing participated in the experiment. The mean age of the group was 22.3 years. The participants were randomly assigned to experimental or control groups for one of the four conditions combining tail or centre embedding left-branching and right-branching (<italic>n</italic> = 20 or 21 for each condition). Each participant was paid a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0045885.e001" xlink:type="simple"/></inline-formula>20 attendance fee (about three US dollars). The experimental protocol was approved by the institutional review board of the Institute of Psychology, Chinese Academy of Sciences, China. All participants provided informed consent prior to the experiment.</p>
      </sec>
      <sec id="s3b">
        <title>Materials</title>
        <sec id="s3b1">
          <title>Stimulus structures</title>
          <p>With a maximum number of three embeddings, each of the four grammars produced 18 different abstract structures with a length from two to nine words. To generate the final surface sentences, each of the terminal symbols V, N, R in each abstract structure was subsequently replaced by one of a set of corresponding monosyllabic words for each class. There were four words for the V class, four words for the N class, and one word for the R class. An exhaustive recursive enumeration of all possible surface structures created a pool of several thousand different terminal sequences for each of the grammars.</p>
          <p>To assess participants' ability to recognise <italic>abstract</italic> grammatical structures (order of word classes) independently of whether they belonged to the training set, we divided the abstract grammatical structures into <italic>old-grammatical</italic> structures which were presented in both training and test phases and <italic>new-grammatical</italic> structures which were presented only in the testing phase. The old and new grammatical structures each featured five 2-layer structures and three 3-layer structures. The two 1-layer structures were assigned purely to the old-grammatical set because of the small number of structure types. The 2- and 3-layer structures were randomly and equally assigned to old- and new-grammatical structures.</p>
          <p>The training set consisted of 168 different sentences which included 16 1-layer structures (i.e. two old-grammatical structures instantiated randomly with terminals in eight different ways each), 80 2-layer structures (i.e. five 2-layer old grammatical structures instantiated randomly with terminals in 16 different ways) and 72 3-layer structures (i.e. three 3-layer old grammatical structures instantiated randomly with terminals in 24 different ways). The 2- and 3-layer structures were repeated two and three times because they are plausibly more difficult to learn than 1-layer structures.</p>
          <p>In order to assess the acquisition of structural knowledge, we constructed two kinds of ungrammatical structures: layer-ungrammatical structures which violated only one layer of the grammatical structures and random-ungrammatical structures. The length of ungrammatical stimuli always matched that of the corresponding grammatical structure. There were 22 random-ungrammatical structures which matched six 1-layer structures (i.e. the two 1-layer structures repeated twice), ten 2-layer structures and six 3-layer structures (i.e. each abstract 2-layer and 3-layer structure repeated once). There were 44 layer-violating structures which featured the systematic violation of one of the embedded layers in each grammar. For instance, if the grammar belongs to the centre embedding left-branching, its first layer would be either “N V N” or “N V” and its second and third layer would be either “V R” or “V N R”. The violation of the first layer of “N V N” was either “V N N” or “N R V”; similarly,“N V” became either “R V” or “V N”. Thus, a grammatical structure “N V (V N R) N”, in which the first layer “N V N” is to be violated, became either “V N (V N R) N” or “N R (V N R) V”. Similarly, the violation of a second or third layer of “V N R” was either “V V R” or “N R V”; and “V R” became either “R V” or “N R”. Thus, a grammatical structure “N V (V N R) N”, in which the second layer “V N R” was to be violated, became either “N V (V V R) N” or “N V (N R V) N”. Accordingly, the violations of the layer 1 and layer 2 could induce long-distance (nonlocal) dependencies when there was (correct) centre-embedding because the superordinate ungrammatical layer would be intermitted. Altogether the layer-violating structures for each condition included six 1-layer structures (i.e. the two 1-layer structures violated in their one layer in different ways), 20 2-layer structures (i.e. ten 2-layer structures violated in their first or second layer, respectively) and 18 3-layer structures (i.e. six 3-layer structures violated in their first, second and third layer, respectively). <xref ref-type="supplementary-material" rid="pone.0045885.s002">Appendix S2</xref> lists the stimulus sequences used for training and testing.</p>
        </sec>
        <sec id="s3b2">
          <title>Stimulus rendering</title>
          <p>In analogy to the paradigms by Saffran et al., as well as for the sake of simplicity, monosyllabic words were used as terminals. All terminal monosyllabic words were recorded from a professional Chinese Native speaker. The words used were “wao”, “yai”, “piu”, “shin”, “bam”, “fai”, “ti”, “ra”, “ki”. These phonemes/words were pronounced without tone (words were pronounced without tone in order to make a future experiment with Western participants possible). The combination of phonemes in a syllabic word violated Chinese rules for sequencing. Hence the words were not meaningful, in Chinese. Four words were randomly chosen for the V class, four words for the N class, and one word for the R class. For the construction of the stimulus sentences, the monosyllabic words were computationally concatenated to the respective auditory sequences using CSOUND. The sequences were automatically concatenated in order to avoid speaker produced intonation patterns, timing, etc. (cf. <xref ref-type="bibr" rid="pone.0045885-Langendoen1">[112]</xref> for a detailed formal analysis of potential interactions between intonational effects with parsing). The CSOUND score files which specified the respective order of syllables were created using a MATLAB script that converted the randomly chosen set of terminal sequence structures into CSOUND score file format.</p>
        </sec>
      </sec>
      <sec id="s3c">
        <title>Procedure</title>
        <p>The experiment was run using a Flash-environment. There were two phases in the experimental procedure: a training phase and a testing phase.</p>
        <sec id="s3c1">
          <title>Training phase</title>
          <p>Participants were exposed to the set of 168 training stimuli under incidental learning conditions using a word counting distractor task. Before participants gave their word count, the sentence could be repeated as often as the participant wanted. The possibility of repeating stimuli according to the participant is analogous to one standard method in artificial grammar learning to let participants repeat stimuli (e.g. letter sequences) until they could recall them correctly (e.g., <xref ref-type="bibr" rid="pone.0045885-Reber1">[1]</xref>). All training sentences were randomly divided into 8 blocks; each block included 21 sequences. There was an interval of at least 30 seconds between any two blocks. For the experimental group, the sentences were all old-grammatical structures. For the control group, the sentences were all random-ungrammatical structures.</p>
        </sec>
        <sec id="s3c2">
          <title>Testing phase</title>
          <p>Both experimental and control groups received identical instructions throughout the entire experiment. Following the Process Dissociation Procedure (PDP), the testing phase involved two tests: an inclusion and an exclusion test (While conscious and unconscious knowledge both contribute to picking the familiar item in inclusion performance, they conflict in the exclusion condition (as explicit knowledge would lead the participant to choose the unfamiliar item). Hence unconscious but not conscious knowledge of the grammaticality of the item could lead to the grammatical item nonetheless being chosen in exclusion; the difference between inclusion and exclusion performance makes it possible to estimate the amount of conscious knowledge, cf <xref ref-type="bibr" rid="pone.0045885-Jacoby1">[111]</xref>, <xref ref-type="bibr" rid="pone.0045885-Marcel1">[113]</xref>–<xref ref-type="bibr" rid="pone.0045885-Fu1">[114]</xref>). In the inclusion test, participants listened to 66 pairs of sentences. Each trial pair featured one grammatical and one ungrammatical sentence. Participants were asked to choose the one that sounded familiar to them with respect to whether it appeared in the training phase. Subsequently participants specified their level of confidence on a scale from 50% to 100%, where 50% meant completely guessing, and 100% meant absolutely certain (i.e. to give a confidence rating; see <xref ref-type="bibr" rid="pone.0045885-Dienes5">[115]</xref>–<xref ref-type="bibr" rid="pone.0045885-Dienes7">[117]</xref>, for discussion of such subjective measures of awareness). The subsequent exclusion test was carried out precisely as the inclusion test, except that participants were asked to pick the one sentence which sounded unfamiliar to them. Following the methodological conclusions by <xref ref-type="bibr" rid="pone.0045885-Jimenez1">[118]</xref> the exclusion test always followed the inclusion test (<xref ref-type="bibr" rid="pone.0045885-Wilkinson1">[119]</xref> found that the order did not affect performance). There were 132 sequences in each test for both the experimental and control groups. As outlined above, half of the sequences were ungrammatical including 22 random-ungrammatical and 44 layer-ungrammatical structures; half were grammatical including 39 old-grammatical and 27 new-grammatical structures. Each grammatical structure matched a corresponding ungrammatical structure in length. The stimulus pairs appeared in a different random order for each participant.</p>
          <p>Finally, participants were given a category identification test. They were told that the words in the training contained words from different categories such as nouns or verbs. They were then given 14 trials, in each trial they were presented with three words (two belonging to the same category) and were asked to try their best to choose two out of the three words which belonged to the same category.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s4">
      <title>Results</title>
      <p>We will consider the following questions in order: what have people learned, as shown by what violations they can detect? In particular, can people learn long distance dependencies? Is such knowledge modulated by the type of grammar (left vs. right branching, tail versus centre embedding)? Is the knowledge conscious or unconscious? And if we control for chunking and repetition structure, are people still sensitive to the long distance dependencies inherent in the grammars? Finally, can people classify the words that belong to one class? We focus the results section on these key questions.</p>
      <sec id="s4a">
        <title>What was learned?</title>
        <p><xref ref-type="fig" rid="pone-0045885-g003">Figure 3</xref> shows mean accuracy rates for old vs. new grammatical structure and random- vs. layer-ungrammatical structures. <xref ref-type="table" rid="pone-0045885-t001">Table 1</xref> shows mean accuracy rates organised according to the old-/new-grammatical distinction (novelty) or the violation type (layer violations vs. random) under inclusion and exclusion instructions for each group. The difference between inclusion and exclusion will be analyzed below, as will the difference between the different grammars. The analyses in this section are on just the inclusion items, pooling over different types of grammar. Firstly, to examine whether people can generalize the knowledge they acquired in the training to new grammatical structures, we divided the performance for test pairs into performance for old and new grammatical items. A mixed model ANOVA on accuracy rates with grammatical (new vs. old) as a within-participant variable and training (trained vs. control) as a between-participants variable revealed that overall the trained participants classified more accurately than the control participants, <italic>F</italic>(1, 159) = 88.83, <italic>p</italic>&lt;.001, η<italic><sub>p</sub></italic><sup>2</sup> = .36, indicating that training result in learning something about the structure of the grammars. It is important to note here that the range of the results (between 50 and 70%) may appear low with respect to traditional (explicit) learning measures. For experiments exploring unconscious, implicit knowledge, these results are relatively high (cf <xref ref-type="bibr" rid="pone.0045885-Reber1">[1]</xref>, <xref ref-type="bibr" rid="pone.0045885-Dienes1">[18]</xref>). There was no main effect of old versus new, <italic>F</italic>(1, 153) = .51, <italic>p</italic> = .48; further, for just the trained participants, there was no difference detected between new and old items, <italic>t</italic>(79) = .57, <italic>p</italic> = .57. Old versus new did not interact with any effect of interest, so we will not explicitly consider this factor further.</p>
        <fig id="pone-0045885-g003" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0045885.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Accuracy rates for old vs. new-grammatical sequences and random- vs. layer-ungrammatical structures under inclusion.</title>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.g003" xlink:type="simple"/>
        </fig>
        <table-wrap id="pone-0045885-t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0045885.t001</object-id><label>Table 1</label><caption>
            <title>Accuracy Rates for Grammatical and Ungrammatical Structures under Inclusion and Exclusion in Each Group.</title>
          </caption><alternatives>
            <graphic id="pone-0045885-t001-1" mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.t001" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="3" rowspan="1"/>
                  <td align="left" colspan="4" rowspan="1">Trained</td>
                  <td align="left" colspan="4" rowspan="1">Control</td>
                </tr>
                <tr>
                  <td align="left" colspan="3" rowspan="1"/>
                  <td align="left" colspan="2" rowspan="1">Grammatical</td>
                  <td align="left" colspan="2" rowspan="1">Ungrammatical</td>
                  <td align="left" colspan="2" rowspan="1">Grammatical</td>
                  <td align="left" colspan="2" rowspan="1">Ungrammatical</td>
                </tr>
                <tr>
                  <td align="left" colspan="3" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">Old</td>
                  <td align="left" colspan="1" rowspan="1">New</td>
                  <td align="left" colspan="1" rowspan="1">Layer</td>
                  <td align="left" colspan="1" rowspan="1">Random</td>
                  <td align="left" colspan="1" rowspan="1">Old</td>
                  <td align="left" colspan="1" rowspan="1">New</td>
                  <td align="left" colspan="1" rowspan="1">Layer</td>
                  <td align="left" colspan="1" rowspan="1">Random</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Tail embedding</td>
                  <td align="left" colspan="1" rowspan="1">Left-branching</td>
                  <td align="left" colspan="1" rowspan="1">Inclusion</td>
                  <td align="left" colspan="1" rowspan="1">.59(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.62(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.56(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.67(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.52(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.52(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.53(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.50(.02)</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">Exclusion</td>
                  <td align="left" colspan="1" rowspan="1">.60(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.60(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.56(.01)</td>
                  <td align="left" colspan="1" rowspan="1">.68(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.51.02)</td>
                  <td align="left" colspan="1" rowspan="1">.54(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.52(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.52(.02)</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">Right-branching</td>
                  <td align="left" colspan="1" rowspan="1">Inclusion</td>
                  <td align="left" colspan="1" rowspan="1">.64(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.62(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.59(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.73(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.51(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.55(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.52(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.53(.03)</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">Exclusion</td>
                  <td align="left" colspan="1" rowspan="1">.62(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.61(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.59(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.68(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.54(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.55(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.55(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.55(.03)</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Centre embedding</td>
                  <td align="left" colspan="1" rowspan="1">Left-branching</td>
                  <td align="left" colspan="1" rowspan="1">Inclusion</td>
                  <td align="left" colspan="1" rowspan="1">.67(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.66(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.63(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.74(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.51(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.55(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.51(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.54(.02)</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">Exclusion</td>
                  <td align="left" colspan="1" rowspan="1">.64(.01)</td>
                  <td align="left" colspan="1" rowspan="1">.59(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.57(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.73(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.54(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.59(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.54(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.62(.02)</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">Right-branching</td>
                  <td align="left" colspan="1" rowspan="1">Inclusion</td>
                  <td align="left" colspan="1" rowspan="1">.69(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.65(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.62(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.78(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.51(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.51(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.52(.01)</td>
                  <td align="left" colspan="1" rowspan="1">.48(.02)</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">Exclusion</td>
                  <td align="left" colspan="1" rowspan="1">.65(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.65(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.62(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.73(.03)</td>
                  <td align="left" colspan="1" rowspan="1">.51(.01)</td>
                  <td align="left" colspan="1" rowspan="1">.53(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.52(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.51(.03)</td>
                </tr>
              </tbody>
            </table>
          </alternatives></table-wrap>
        <p>To explore whether people can detect the specific violations in layer-ungrammatical items as well as the gross violations in random-ungrammatical ones, a mixed model ANOVA with ungrammatical (layer- vs. random-ungrammatical) and training (trained vs. control) as independent variables revealed that overall the trained participants classified more accurately than the control participants, <italic>F</italic> (1, 159) = 114.02, <italic>p</italic>&lt;.001, η<italic><sub>p</sub></italic><sup>2</sup> = .42, confirming that learning about the grammatical structure did occur. Further analysis revealed that performance in the trained was greater than that in the control condition for both layer-ungrammatical, <italic>t</italic> (159) = 5.66, <italic>p</italic>&lt;.001, <italic>d</italic> = .90, and random-ungrammatical, <italic>t</italic> (159) = 11.45, <italic>p</italic>&lt;.001, <italic>d</italic> = 1.82, indicating that participants in the trained condition acquired not only broad differences between grammatical and non-grammatical items, as indicated by the sensitivity to random baseline structures, but subtle differences as well, as indicated by the sensitivity to layer violations. We will explore these subtle differences further.</p>
        <p><xref ref-type="fig" rid="pone-0045885-g004">Figure <italic>4</italic></xref> shows accuracy rates for the different types of structural violation under Inclusion. Trained participants were more accurate than controls for when the violation occurred in each of the first and second layers, <italic>t</italic> (159) = 4.67, <italic>p</italic>&lt;.001, <italic>d</italic> = .74, <italic>t</italic> (159) = 3.67, <italic>p</italic>&lt;.001, <italic>d</italic> = .58, respectively (both significant after Hochberg's, 1988, sequential Bonferroni correction), though not in the third layer. The sensitivity to violations in different layers does not necessarily imply that participants must have parsed the sequence into the embedded parts per se, nor that participants have learnt the long distance dependencies created by the recursive nature of the grammar. Crucially, however, stimuli with layer violations can be divided into local and nonlocal dependency structures based on whether or not the ungrammatical stimulus involves a nonlocal violation (of a nonlocal structure). Trained participants performed better than controls on both local dependencies, <italic>t</italic> (159) = 3.86, <italic>p</italic>&lt;.001, <italic>d</italic> = .61, and nonlocal dependencies, <italic>t</italic> (159) = 5.16, <italic>p</italic>&lt;.001, <italic>d</italic> = .82 (see <xref ref-type="table" rid="pone-0045885-t002">Table 2</xref>). This key result is explored further below. The performance for nonlocal dependencies surprisingly turns out to be higher than for local dependencies. This might be because non-local violations have an intermitting structure and therefore potentially an irregular transition at more than one location (We should not presume that local n-gram structure is always the easiest. In a yet unpublished study, where sequences of letters were explicitly constructed as obeying or violating either global repetition structure or bigrams, participants learnt the repetition structure considerably better than the bigram structure).</p>
        <fig id="pone-0045885-g004" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0045885.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Accuracy rates for different types of structural violation under inclusion.</title>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.g004" xlink:type="simple"/>
        </fig>
        <table-wrap id="pone-0045885-t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0045885.t002</object-id><label>Table 2</label><caption>
            <title>Accuracy Rates for the different types of structures: number of layers (i.e. complexity), the layer where the violation occurred, and dependency type (local vs. nonlocal) under Inclusion.</title>
          </caption><alternatives>
            <graphic id="pone-0045885-t002-2" mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.t002" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="3" rowspan="1">Violated layer</td>
                  <td align="left" colspan="2" rowspan="1">Dependency</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">First</td>
                  <td align="left" colspan="1" rowspan="1">Second</td>
                  <td align="left" colspan="1" rowspan="1">Third</td>
                  <td align="left" colspan="1" rowspan="1">local</td>
                  <td align="left" colspan="1" rowspan="1">nonlocal</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Experimental</td>
                  <td align="left" colspan="1" rowspan="1">.61(.01)</td>
                  <td align="left" colspan="1" rowspan="1">.61(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.56(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.58(.01)</td>
                  <td align="left" colspan="1" rowspan="1">.65(.02)</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Control</td>
                  <td align="left" colspan="1" rowspan="1">.52(.01)</td>
                  <td align="left" colspan="1" rowspan="1">.53(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.51(.02)</td>
                  <td align="left" colspan="1" rowspan="1">.52(.01)</td>
                  <td align="left" colspan="1" rowspan="1">.53(.02)</td>
                </tr>
              </tbody>
            </table>
          </alternatives></table-wrap>
      </sec>
      <sec id="s4b">
        <title>Were some types of grammars easier than others?</title>
        <p><xref ref-type="fig" rid="pone-0045885-g005">Figure 5</xref> shows the means and standard deviations for proportion of correct classifications of long range dependencies in the inclusion test, according to type of grammar. We subjected the classification of long-distance dependencies in the inclusion test to a 2 (branching: left vs. right)×2 (tail vs. centre embedding) between participants ANOVA. It revealed a significant centre embedding effect, <italic>F</italic> (1, 76) = 4.03, <italic>p</italic>&lt;.05, η<italic><sub>p</sub></italic><sup>2</sup> = .05, indicating that the tail embedding grammar was better learned than the centre embedding one. The interaction of centre embedding by branching reached significance, <italic>F</italic> (1, 76) = 5.77, <italic>p</italic>&lt;.05, η<italic><sub>p</sub></italic><sup>2</sup> = .07. Further analysis revealed that the tail embedding grammar was better learned than the centre embedding grammar when the branching was left-branching, <italic>t</italic> (38) = 3.37, <italic>p</italic>&lt;.01, <italic>d</italic> = 1.09; and the left-branching was better learned than the right-branching when the grammar was tail embedding, <italic>t</italic> (38) = 2.15, <italic>p</italic>&lt;.05, <italic>d</italic> = .70.</p>
        <fig id="pone-0045885-g005" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0045885.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Accuracy rates for nonlocal and local dependencies in the inclusion test, according to type of grammar.</title>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.g005" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s4c">
        <title>Was the knowledge conscious or unconscious?</title>
        <p>The conscious status of the knowledge can be investigated by the difference between inclusion and exclusion performance <xref ref-type="bibr" rid="pone.0045885-Jacoby1">[111]</xref> and also by the relation of confidence to performance <xref ref-type="bibr" rid="pone.0045885-Dienes6">[116]</xref>. We consider each method in turn. Although the exclusion instruction was opposite to inclusion instruction, we computed accuracy rates of both inclusion and exclusion performance on the basis of the rate with which grammatical stimuli were chosen.</p>
        <p><xref ref-type="fig" rid="pone-0045885-g006">Figure 6</xref> shows the means and standard errors for proportion correct separated by inclusion and exclusion. An ANOVA on totaracy rates with instruction (inclusion vs. exclusion) a awihin-subject variable and training (trained vs. control), as between-subjects revealed a significant instruction by training interaction, <italic>F</italic> (1, 159) = 8.07, <italic>p</italic>&lt;.01, η<italic><sub>p</sub></italic><sup>2</sup> = .05. Overall, trained participants selected more grammatical items in inclusion than exclusion, <italic>t</italic> (79) = 2.60, <italic>p</italic>&lt;.05, <italic>dz</italic> = .29, indicating some control over the use of their knowledge. The effect, though significant, is small. Crucially, exclusion performance was still significantly better than the control group, <italic>t</italic> (159) = 7.46, <italic>p</italic>&lt;.001. <italic>d</italic> = 1.09. That is, when asked to pick the non-grammatical items the trained participants still picked the grammatical items, a result inconsistent with participants consciously knowing that the grammatical items were grammatical.</p>
        <fig id="pone-0045885-g006" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0045885.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Accuracy rates comparing inclusion and exclusion performance with respect to left- and right-branching grammars.</title>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.g006" xlink:type="simple"/>
        </fig>
        <p>In terms of the confidence measures of the conscious status of knowledge, when participants said they were purely guessing, inclusion performance was better than that of the control group (<italic>M</italic> = .60, <italic>SE</italic> = .03 vs. <italic>M</italic> = .50, <italic>SE</italic> = .03), <italic>t</italic> (131) = 2.44, <italic>p</italic>&lt;.05, <italic>d</italic> = .43, indicating again that participants were not aware of knowing the grammatical status of the items. When participants had some confidence (&gt;50%), inclusion performance was better than when people said they were guessing (<italic>M</italic> = .66, <italic>SE</italic> = .01 vs <italic>M</italic> = .60, <italic>SE</italic> = .03), <italic>t</italic> (62) = 2.51, <italic>p</italic>&lt;.05, <italic>d</italic> = .64, indicating that people were sometimes aware of knowing an item was grammatical, consistent with the small amount of control that participants exerted. However, when participants had some confidence, the inclusion-exclusion difference was not greater than when participants said they were guessing (<italic>M</italic> = .02, <italic>SE</italic> = .01 vs. <italic>M</italic> = .06, <italic>SE</italic> = .04), <italic>t</italic> (58) = 1.09, <italic>p</italic> = .28, consistent with people not knowing when they had control over the use of their knowledge.</p>
        <p>For the classification of long-distance dependencies, a comparable ANOVA revealed a significant instruction by training interaction, <italic>F</italic> (1, 159) = 8.13, <italic>p</italic>&lt;.01, η<italic><sub>p</sub></italic><sup>2</sup> = .05. Trained participants selected more grammatical items in inclusion than exclusion, <italic>t</italic> (79) = 2.92, <italic>p</italic>&lt;.01, <italic>dz</italic> = .32, indicating some control over the use of their knowledge. Importantly, exclusion performance was also significantly better than that of the control group, <italic>t</italic> (159) = 3.07, <italic>p</italic>&lt;.01, <italic>d</italic> = .49, indicating that participants did not consciously know that the grammatical items were grammatical. There were only about 14, 18, 12 and 8 long distance dependency trials per subject in the centre and tail embedding left- and branching groups, respectively, so they were not subdivided further into confidence bins. Nonetheless, the exclusion performance demonstrates participants' knowledge of long distance dependencies was largely unconscious.</p>
      </sec>
      <sec id="s4d">
        <title>What was learnt, controlling for n-grams and repetition structure at word and class levels?</title>
        <p>Although the stimuli were generated based on the discussed context-free grammars, participants' performance may not necessarily based on the knowledge of the grammar but on other acquired structures <xref ref-type="bibr" rid="pone.0045885-Reber3">[22]</xref>, <xref ref-type="bibr" rid="pone.0045885-Dulan1">[120]</xref>. In particular, we aimed to explore the extent to which participants' responses indicated sensitivity to nonlocal dependencies when other factors were controlled for. For this purpose we employed a (logistic) regression analysis as is common in implicit learning research (e.g. <xref ref-type="bibr" rid="pone.0045885-Dienes2">[47]</xref>, <xref ref-type="bibr" rid="pone.0045885-Dienes3">[100]</xref>, <xref ref-type="bibr" rid="pone.0045885-Lotz1">[121]</xref>). In order to establish that participants' sensitivity to long distance structure (violations which involved a layer that was intermitted) was not (just) based on knowledge of bigrams or trigrams of words in the training phase we determined for each test item the total (summed) frequency of its bigrams and trigrams of words (word chunk strength) as well as bigrams and trigrams of grammatical classes (class chunk strength) representing grammatical class. Anchor positions (stimulus beginnings and endings) are known to provide important cues which participants pick up <xref ref-type="bibr" rid="pone.0045885-Johnstone1">[93]</xref>, <xref ref-type="bibr" rid="pone.0045885-Endress1">[122]</xref>. These features were controlled for by the fact that our n-gram analysis coded stimulus beginnings and endings with two different padding symbols, so that anchor positions would be accounted for as potential predictors. We also controlled repetition structure, which can be coded in a number of ways <xref ref-type="bibr" rid="pone.0045885-Scott1">[83]</xref>. showed a particularly strong predictor of responding in artificial grammar learning was adjacent repetition structure <xref ref-type="bibr" rid="pone.0045885-Mathews2">[123]</xref>. Adjacent repetition structure reflects the similarity of a given terminal element to that immediately preceding it, for example, the adjacent repetition structure of AABBCC is 10101. The initial 1 represents the fact that the second letter is the same as the first letter; the following 0 indicates that the third letter is different to the second letter, and so forth. The frequency of the adjacent repetition pattern of words as well as classes of each test item in the training phase was determined. In addition we also controlled for global repetition structure in a similar way. For instance, the item AABBCC has global repetition structure 112233, meaning the first element also appears in the second position, the third position contains a new type of element, which repeats in the fourth, and so on. The two structures ABABC and BCBCD share the identical global repetition structure: viz 12123.</p>
        <p>For all test choices and for each participant, the participant's choice (correct/incorrect) was logistically regressed on the difference between the grammatical and non-grammatical items in: Word and class chunk strength, word and class local repetition structure, word and class global repetition structure, as well as a dummy predictor variable which coded whether the violation in the ungrammatical stimulus involved a local (rather than a long-distance) violation (1 for local vs. 0 for long distance). Since local violation was a controlled dummy variable complementary to nonlocal dependencies, the intercept encodes the effect of nonlocal dependencies (see <xref ref-type="table" rid="pone-0045885-t003">Table 3</xref>). Results in the exclusion condition were scored as if they were under inclusion instructions, i.e. “correct” means selecting the grammatical item. The intercept represents the person's ability to classify long distance dependencies with chunk strength, local and global repetition structure controlled on word and class levels, i.e. the intercept is the predicted performance when all these other variables are zero. T-tests over participants showed that participants were not sensitive to word chunk strength as far as we could detect, <italic>t</italic> (79) = 1.11, <italic>p</italic> = .27, but were sensitive to class chunk strength, <italic>t</italic> (79) = 2.30, <italic>p</italic>&lt;.05, <italic>dz</italic> = .26, word local repetition structure, <italic>t</italic> (79) = 3.15, <italic>p</italic>&lt;.01, <italic>dz</italic> = .35, word global repetition structure, <italic>t</italic> (79) = 3.98, <italic>p</italic>&lt;.001, <italic>dz</italic> = .44. Crucially we show that people were sensitive to long distance dependencies with other factors controlled, <italic>t</italic> (79) = 2.43, <italic>p</italic>&lt;.05, <italic>dz</italic> = .27. The latter result is the key result and key reason for performing the analysis: We show learning of long distance dependencies controlling a range of other structures we already know people can implicitly learn. In particular, while global repetition structure is a type of long distance dependency, we show that people are sensitive to the long distance dependencies in the grammar in a way that goes beyond sensitivity to global repetition structure as such. That is, our interpretation of the performance results above in terms of local vs. nonlocal dependencies remains after controlling relevant variables.</p>
        <table-wrap id="pone-0045885-t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0045885.t003</object-id><label>Table 3</label><caption>
            <title>Logistic Regression Analyses Regressing Participants' Responses Applying Surface and Deep-structure Chunk Strength, Local and Global Repetition Structure and Local Dependencies as Predictors.</title>
          </caption><alternatives>
            <graphic id="pone-0045885-t003-3" mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.t003" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">Regression coefficient</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>p</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1"><italic>t</italic> (79)</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Nonlocal dependency (intercept)</td>
                  <td align="left" colspan="1" rowspan="1">0.150<xref ref-type="table-fn" rid="nt101">*</xref></td>
                  <td align="left" colspan="1" rowspan="1">0.018</td>
                  <td align="left" colspan="1" rowspan="1">2.43</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">2–3-grams (word)</td>
                  <td align="left" colspan="1" rowspan="1">0.004</td>
                  <td align="left" colspan="1" rowspan="1">0.270</td>
                  <td align="left" colspan="1" rowspan="1">1.11</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">2–3-grams (class)</td>
                  <td align="left" colspan="1" rowspan="1">0.002<xref ref-type="table-fn" rid="nt101">*</xref></td>
                  <td align="left" colspan="1" rowspan="1">0.024</td>
                  <td align="left" colspan="1" rowspan="1">2.30</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Local dependencies</td>
                  <td align="left" colspan="1" rowspan="1">0.163</td>
                  <td align="left" colspan="1" rowspan="1">0.071</td>
                  <td align="left" colspan="1" rowspan="1">1.83</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Global repetition (word)<xref ref-type="table-fn" rid="nt101">*</xref></td>
                  <td align="left" colspan="1" rowspan="1">0.032</td>
                  <td align="left" colspan="1" rowspan="1">0.000</td>
                  <td align="left" colspan="1" rowspan="1">3.98</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Global repetition (class)</td>
                  <td align="left" colspan="1" rowspan="1">−0.004</td>
                  <td align="left" colspan="1" rowspan="1">0.285</td>
                  <td align="left" colspan="1" rowspan="1">−1.08</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Local repetition (word)</td>
                  <td align="left" colspan="1" rowspan="1">0.170<xref ref-type="table-fn" rid="nt101">*</xref></td>
                  <td align="left" colspan="1" rowspan="1">0.002</td>
                  <td align="left" colspan="1" rowspan="1">3.15</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Local repetition (class)</td>
                  <td align="left" colspan="1" rowspan="1">−0.002</td>
                  <td align="left" colspan="1" rowspan="1">0.280</td>
                  <td align="left" colspan="1" rowspan="1">−1.09</td>
                </tr>
              </tbody>
            </table>
          </alternatives><table-wrap-foot>
            <fn id="nt101">
              <label>*</label>
              <p>: p&lt;.05.</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <p>Note we demonstrate sensitivity to class chunk strength controlling for word chunk strength, a finding that goes beyond the now common demonstration that people are sensitive to chunks of terminal elements, e.g. letters (e.g. <xref ref-type="bibr" rid="pone.0045885-ServanSchreiber1">[106]</xref>; cf <xref ref-type="bibr" rid="pone.0045885-Reber2">[21]</xref>). However, the effect size is tiny. The evidence from this analysis therefore that people learnt classes is people's sensitivity to long distance dependencies between word classes controlling for a range of word level variables. Indeed a Bayes factor was run comparing the null hypothesis of no effect to a theory that expected class chunk strength to affect classification by up to 3%. The Bayes factor showed the evidence was 100 times stronger for the null! The predictions of the theory was represented by a half-normal with a standard deviation of 1.5%, i.e. the theory allowed effects between 0 and about 3%; see <xref ref-type="bibr" rid="pone.0045885-Dienes8">[124]</xref>–<xref ref-type="bibr" rid="pone.0045885-Dienes9">[125]</xref> for the technique. This is an example of a case where an effect is so tiny that a significant result is actually evidence for the null over a theory predicting a difference as small as is often picked up with abstract implicit learning (cf <xref ref-type="bibr" rid="pone.0045885-Dienes1">[18]</xref>).</p>
        <p>However, the sensitivity to nonlocal dependencies might arise because of sensitivity to nonlocal dependencies between words and not classes. Accordingly the purpose of the next analysis was to examine whether fixed nonlocal chunks (i.e. chunks which would be intermitted) rather than flexible nonlocal dependencies would be potential predictors for participants' responses. The span of intervening material for nonlocal dependencies was 2 or 3 words. Thus we conducted another regression in which bigrams were coded as a) A - - B, where A and B are words and the dashes can be any intervening word: These are long distance word bigrams with two intervening items; and b) A - - - B, long distance word bigrams with three intervening items. If overall sensitivity to long distance dependencies remains, it shows such sensitivity was not based just on learning long distance word bigrams. In the same regression we considered another question. If people are sensitive to the phrase structure, they will be sensitive to phrases as such: Their sensitivity to long distance dependencies will not be for fixed lengths, but to lengths that vary from item to item depending on phrase length. Thus, as a stricter test of people learning long distance dependencies as a consequence of learning hierarchical embedding, we added two more variables to the regression: c) ) A - - B, where A and B are classes and the dashes can be any intervening class: These are long distance class bigrams with two intervening items; and d) A - - - B, long distance class bigrams with three intervening items. Note that c) and d) encode sensitivity to fixed length dependencies; if people have learnt the phrase structure per se, their sensitivity to long distance dependencies will exceed the variance accounted by these fixed-length predictors because there will be variance in when sensitivity is to 2 versus 3 words long not explained by either fixed-length variable. a)–d) could not be added to the regression already performed because there would be too many predictor variables and the regression becomes unstable. Thus we set up a new regression with a) to d) as predictors, as well as a control variable coding whether the item has short distance or long distance dependencies at all. The intercept codes whether there is sensitivity to the long distance dependencies when all these predictors are controlled. <xref ref-type="table" rid="pone-0045885-t004">Table 4</xref> displays the results. The long distance word bigrams had no predictive power; but long distance class bigrams (for fixed distances) had some. Crucially, the intercept was still significant controlling all of a) to d). Thus, sensitivity to long distance dependencies was not just based on sensitivity to long distance dependencies to words for fixed lengths (2 and 3 intervening items); this is evidence that there was sensitivity to word classes. Further, sensitivity to long distance dependencies was not just based on sensitivity to long distance dependencies to classes for fixed lengths (2 and 3 intervening items); this is consistent with people learning the phrase structure per se. Ideally, one regression would be performed with all predictors in, so our conclusion regarding learning phrase structure per se must await further testing; but we have at least found (unconscious) long distance dependency learning of classes (which goes beyond what has been previously demonstrated).</p>
        <table-wrap id="pone-0045885-t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0045885.t004</object-id><label>Table 4</label><caption>
            <title>Logistic Regression Analyses Regressing Participants' Responses Applying Surface and Deep-structure Nonlocal Chunks as Predictors.</title>
          </caption><alternatives>
            <graphic id="pone-0045885-t004-4" mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.t004" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" colspan="1" rowspan="1"/>
                  <td align="left" colspan="1" rowspan="1">Regression coefficient</td>
                  <td align="left" colspan="1" rowspan="1">
                    <italic>p</italic>
                  </td>
                  <td align="left" colspan="1" rowspan="1"><italic>t</italic> (79)</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Nonlocal dependency (intercept)</td>
                  <td align="left" colspan="1" rowspan="1">0.280<xref ref-type="table-fn" rid="nt102">*</xref></td>
                  <td align="left" colspan="1" rowspan="1">0.000</td>
                  <td align="left" colspan="1" rowspan="1">5.85</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Nonlocal bigram with two intervening (word)</td>
                  <td align="left" colspan="1" rowspan="1">0.003</td>
                  <td align="left" colspan="1" rowspan="1">0.660</td>
                  <td align="left" colspan="1" rowspan="1">0.44</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Nonlocal bigram with three intervening (word)</td>
                  <td align="left" colspan="1" rowspan="1">0.009</td>
                  <td align="left" colspan="1" rowspan="1">0.290</td>
                  <td align="left" colspan="1" rowspan="1">1.06</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Nonlocal bigram with two intervening (class)</td>
                  <td align="left" colspan="1" rowspan="1">0.006<xref ref-type="table-fn" rid="nt102">*</xref></td>
                  <td align="left" colspan="1" rowspan="1">0.000</td>
                  <td align="left" colspan="1" rowspan="1">4.49</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Nonlocal bigram with three intervening (class)</td>
                  <td align="left" colspan="1" rowspan="1">0.000</td>
                  <td align="left" colspan="1" rowspan="1">0.873</td>
                  <td align="left" colspan="1" rowspan="1">0.16</td>
                </tr>
                <tr>
                  <td align="left" colspan="1" rowspan="1">Local versus Nonlocal</td>
                  <td align="left" colspan="1" rowspan="1">0.019</td>
                  <td align="left" colspan="1" rowspan="1">0.757</td>
                  <td align="left" colspan="1" rowspan="1">0.31</td>
                </tr>
              </tbody>
            </table>
          </alternatives><table-wrap-foot>
            <fn id="nt102">
              <label>*</label>
              <p>: p&lt;.05.</p>
            </fn>
          </table-wrap-foot></table-wrap>
      </sec>
      <sec id="s4e">
        <title>Category identification test</title>
        <p>Overall the trained participants could not pick two out of three words of the same category at above baseline levels (baseline being .33; <italic>M</italic> = .31, <italic>SE</italic> = .01), <italic>t</italic> (79) = −1.79, <italic>p</italic> = .084, and similar to the control groups (<italic>M</italic> = .31, <italic>SE</italic> = .01 vs. <italic>M</italic> = .32, <italic>SE</italic> = .01), <italic>t</italic> (159) = −.84, <italic>p</italic> = .40. Further, no group individual was above baseline (all <italic>p</italic>s&gt;.27) and nor was better than their control group (all <italic>p</italic>s&gt;.05). The upper limit of the 95% confidence interval for trained participants is .33, so whatever knowledge trained participants have available for classifying this task, it is not enough to classify more than a percent above chance baseline. The test very sensitively rules out knowledge allowing discrimination of class. Thus, not only were participants not conscious of the grammatical classes they were sensitive to in parsing the structure of sentences, their knowledge of the classes was in such an implicit or embedded form it could not allow first order discrimination of what words had the same class.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Discussion</title>
      <p>The aim of the experiment was to investigate whether participants could implicitly acquire hierarchical recursive structures that resemble natural language word order on an abstract level. A second aim was to further explore the effects of branching-type and centre embedding. Our results showed that trained participants performed much better than the controls with respect to the layer-ungrammatical structures, including long-distance dependencies, under both inclusion and exclusion tests, suggesting that they did implicitly acquire knowledge that enabled them to distinguish the hierarchical structures. Importantly, people's unconscious knowledge of long-distance dependencies goes beyond the now common demonstration that people are sensitive to chunks of terminal elements. Based on the present results we cannot infer which mental representation participants had acquired, however, the findings suggest that it is a form of representation that incorporates long-distance dependencies and likely nested structures. Finally, participants learned better when the grammar featured tail rather than centre embedding (in the sense of <xref ref-type="bibr" rid="pone.0045885-Hawkins1">[103]</xref>–<xref ref-type="bibr" rid="pone.0045885-Hawkins2">[104]</xref>, <xref ref-type="bibr" rid="pone.0045885-Hawkins3">[126]</xref>), showing a variable argued to affect preferential learning in natural languages also affects learning of artificial languages in the lab.</p>
      <sec id="s5a">
        <title>Can people learn recursive structures?</title>
        <p>Our approach to determining whether people had acquired distinctively recursively embedded structures was to show that people could become sensitive to the long distance dependencies generated by context-free grammars; and further to show that this sensitivity remained after controlling for n-grams and repetition structure. Thus, in a sentence with multiple levels of recursive embedding, we showed participants could learn to become sensitive to violations of nested embedded structures. As the violation spans the embedding there is suggestive evidence of learning and representing recursively-generated long-distance dependencies. In this respect our results differ from the findings by <xref ref-type="bibr" rid="pone.0045885-Perruchet1">[77]</xref>. While <xref ref-type="bibr" rid="pone.0045885-Perruchet1">[77]</xref> found that people were not able to acquire indexed A<sup>n</sup>B<sup>n</sup> grammars, the grammar they employed was highly abstract (and requires indexing in order to avoid a counting confound). The grammar used in our study was modelled to be more complex than the simple A<sup>n</sup>B<sup>n</sup> grammar and to be a more general example of a context-free grammar, with further analogies to some realistic features of constituent order. The variety of structures and greater redundancy of our grammars rather than A<sup>n</sup>B<sup>n</sup> grammars may in fact render our grammar more learnable than the A<sup>n</sup>B<sup>n</sup> grammar.</p>
        <p>However, although we showed people can discriminate grammaticality, this finding alone does not demonstrate the learning of recursive rules or recursive parsing. We know that people are also sensitive to bigram and trigram frequencies <xref ref-type="bibr" rid="pone.0045885-Reber4">[105]</xref>–<xref ref-type="bibr" rid="pone.0045885-Knowlton1">[107]</xref> and repetition structure <xref ref-type="bibr" rid="pone.0045885-Brooks1">[82]</xref>, the latter being a type of long distance dependency which does not need to be recursively specified. To further explore whether participants' knowledge only included bigrams or trigrams of terminal elements or classes in the training phase, we determined for each test item the total (summed) frequency of its bigrams and trigrams of terminal elements (word chunk strength) as well as bigrams and trigrams of grammatical classes (class chunk strength). The performance for long-distance dependencies was significantly above chance when chunk and repetition structure knowledge was controlled for. We suggest the explanation is the acquisition of abstract knowledge at the complexity level of long-distance dependencies and hierarchical, embedded structures. The explanation is admittedly only indirectly supported by the evidence in that we have not decisively shown the psychological reality of the structural hierarchy per se. It should be further stressed that our results do not demand that the stimuli were parsed and processed recursively, as outlined in the introduction. The present training sequences (as all finite sets of sequences) could be entirely represented by a (much less parsimonious) finite-state grammar (encoding every single stimulus) or by mere whole-sequence memorisation. The fact, however, that the set used novel new-grammatical sequences (generated by the context-free rules) plausibly rules out a catch-all finite-state representation or whole-sequence memorisation. The fact that participants performed well for new-grammatical structures (and therefore were generalising) indicates that a more complex explanation for their learning behaviour is required. Accordingly, we controlled for other known alternative explanations of participants' response patterns and that they only account in a limited way for the performance. Nonetheless, if a potential explanation which involves learning and matching recursive structure is right, people trained on some depth of embeddings should be able to generalise to other novel structures and to other levels of embedding, to within the limit of the relevant memory buffer. Further research, for example click experiments or segmentation tasks in which participants are instructed to group word sequences that belong together, should also be able to provide evidence of levels of embedding being psychologically relevant structural units. Our paradigm provides an ideal starting point for such further research as well as neuroscientific research exploring whether the neural pathways involved in processing during this experiment resemble other results based on context-free structures.</p>
        <p>Many computational models of implicit learning tend to be good at learning chunks and associations <xref ref-type="bibr" rid="pone.0045885-Cleeremans2">[110]</xref>. For example, the SRN is good at learning conditional probabilities of successive elements <xref ref-type="bibr" rid="pone.0045885-Cleeremans3">[127]</xref>. Nonetheless it can learn the musical inversions of <xref ref-type="bibr" rid="pone.0045885-Kuhn1">[99]</xref>, but only by learning them as long-distance associations <xref ref-type="bibr" rid="pone.0045885-Kuhn2">[109]</xref> rather than as a recursively generated structure per se. How the SRN might cope with learning hierarchically embedded structures, as in the current material, remains to be determined in future work. Chunking models (e.g. <xref ref-type="bibr" rid="pone.0045885-ServanSchreiber1">[106]</xref>, <xref ref-type="bibr" rid="pone.0045885-Boucher1">[128]</xref>–<xref ref-type="bibr" rid="pone.0045885-Perruchet2">[129]</xref>) are challenged by the data because such models assume that learning involves chunking of adjacent elements. For example, the competitive chunk (CC) model assumes that the probability of a letter string is judged grammatical on the basis of the network of chunks acquired during the memorization task. Although the CC model can successfully reproduce some findings with the artificial finite grammar task, it is unlikely to learn the relations between grammatical classes over long distances when bigrams and trigrams are controlled. A further issue to be explored in future research is the impact of left or right branching and potential for centre embedding on model performance.</p>
      </sec>
      <sec id="s5b">
        <title>Is the learning unconscious?</title>
        <p>In order to demonstrate that people implicitly learnt the grammars, we need to show people acquired unconscious knowledge <xref ref-type="bibr" rid="pone.0045885-Dienes6">[116]</xref>. We employed both the PDP method and confidence ratings to determine people's awareness of knowing the grammaticality of items. As applied to this experiment, PDP is based on the assumption that if one consciously knows whether or not an item has the same structure as the training items, one should be able to control whether the item is endorsed as familiar or unfamiliar. Confidence ratings directly measure whether one consciously knows whether or not an item has the same structure as the training items. Both methods indicated substantial amounts of unconscious knowledge and some, but very limited, conscious knowledge. Specifically, people were quite likely to pick the grammatical item (and reject the non-grammatical) when deliberately trying to pick the item which violated the structure of the training items; and when people thought they were guessing and trying to pick the well structured item, they tended to pick the grammatical item (and reject the non-grammatical). These conclusions apply overall and for non-grammatical items violating only long distance dependencies. The finding that the learning outcome is in fact implicit (contrast e.g. <xref ref-type="bibr" rid="pone.0045885-Uddn1">[50]</xref>) is important since it demonstrates that the implicit learning mechanism can develop sensitivity to structures that are beyond mere chunks. Importantly, explicit learning of context-free structures would be less surprising since, for instance, the learning of a programming language involves dealing with an artificial language of this type of grammar. Thus testing for awareness of the learned structures is crucial in this context.</p>
      </sec>
      <sec id="s5c">
        <title>Did participants acquire word classes?</title>
        <p>An important contribution of the study is in providing evidence for the implicit learning of relations over classes. People were apparently processing more than just mere surface based features of the stimulus sentences and were able to infer some knowledge about syntactic categories from the surface word sequences in an unsupervised manner. The fact that they, on the other hand, were at chance at the word class tests at the end of each experiment shows that they could not directly or consciously access their knowledge of these classes even though their response patterns indicated they applied it. This finding takes previous findings that people can learn word classes in artificial grammar learning experiments <xref ref-type="bibr" rid="pone.0045885-Reeder1">[102]</xref> one step further by showing such knowledge can be unconscious.</p>
      </sec>
      <sec id="s5d">
        <title>Is the learning affected by the type of grammar?</title>
        <p>Natural grammars differ in a number of structural ways, some of which have been argued to affect their ease of learning (e.g. <xref ref-type="bibr" rid="pone.0045885-Hawkins1">[103]</xref>–<xref ref-type="bibr" rid="pone.0045885-Hawkins2">[104]</xref>, <xref ref-type="bibr" rid="pone.0045885-Hawkins3">[126]</xref>). One such structural feature is the extent to which the grammar produces centre embedded clauses. The less centre embedding the grammar has the potential to produce, the easier the grammar should be learn and use. On the other hand, from a theoretical perspective there is no structural difference between left and right branching per se in terms of complexity. Consistently and as one would expect, participants trained on the tail rather than centre embedding grammars performed better, which accords with our predictions. But to be more precise, participants performed best with the left-branching tail-embedding grammar than any of the other grammars. Strikingly, the participants' own native (left-branching) grammar is centre embedding, yet the easiest grammar was one with tail-embedding, suggesting that fundamental cognitive factors could override extensive experience with a centre embedding grammar.</p>
        <p>From a psycho-linguistic perspective, the findings regarding the performance interactions with branching type and centre embedding link with linguistic findings. The performance advantage for left-branching structures probably reflects the fact that their native language is left branching – a fact to be further explored with participants of different native language in future work. The advantage of tail versus centre embedding grammatical structures is, on a basic level, strongly related to Hawkins's performance and correspondence hypothesis for natural grammars <xref ref-type="bibr" rid="pone.0045885-Hawkins2">[104]</xref>. Typological studies find a preference towards tail embedding (or “consistent”) structures in languages of the world (cf <xref ref-type="bibr" rid="pone.0045885-Dryer1">[130]</xref>) and this difference seems to accord with our findings that in both, the left- and right-branching cases the structures which do not feature centre-embedding are better learned than the centre embedding ones. This seems to suggest a potential performative or structural effect that impacts on the learning of syntactic word order and might ultimately constitute a driving force in the way how grammars are selected or evolve (cf <xref ref-type="bibr" rid="pone.0045885-Hawkins2">[104]</xref>, <xref ref-type="bibr" rid="pone.0045885-Kirby1">[131]</xref>).</p>
        <p>The very fact that structural properties affected the learning of our artificial grammars in explicable ways given general cognitive constraints and the participant's native grammar supports our contention that the hierarchical structures of our grammars were learnt as such. This fact also illustrates how issues in linguistics can both motivate and be explored by the use of artificial grammars.</p>
      </sec>
    </sec>
    <sec id="s6">
      <title>Conclusion</title>
      <p>Overall, our findings suggest that people can implicitly acquire knowledge of tail- and centre-embedding structures (involving long-distance dependencies) as well as word classes drawn from recursive context-free grammars in the lab that are similar at an abstract level to those in natural language. The knowledge includes relations between grammatical classes even for dependencies over long distances, in ways that go beyond simple relations (e.g. n-grams) between individual words. Even though in real world, the interaction between syntax and semantics affects and facilitates the parsing as well as the acquisition of language, the finding of learning and processing of such hierarchical dependencies on a structural level is an important contribution. Our study shows how such complex forms of word sequences are potentially acquired incidentally from exposure and represented implicitly, i.e. based on unconscious knowledge. Notably, incidental learning is not the same as implicit learning; while reading this paper you incidentally acquired much conscious knowledge, e.g. on roughly which pages were certain points made. Implicit learning is the acquisition of unconscious knowledge, which can occur both incidentally and intentionally <xref ref-type="bibr" rid="pone.0045885-Dienes7">[117]</xref>. For an example of implicit learning that is intentional, the dynamic control tasks of Berry and Broadbent provide an example (see, for instance, <xref ref-type="bibr" rid="pone.0045885-Dienes10">[132]</xref>). Second language learning potentially provides another example of intentional implicit learning, albeit a (more) controversial one (cf. <xref ref-type="bibr" rid="pone.0045885-Williams2">[14]</xref>, <xref ref-type="bibr" rid="pone.0045885-Rebuschat2">[15]</xref>). While explicit knowledge of complex context-free languages (such as explicitly acquired knowledge of programming languages like ML or C) is less surprising, showing the ability that participants incidentally acquire implicit knowledge of context-free languages is novel and relates to natural acquisition processes like language or music acquisition (e.g. <xref ref-type="bibr" rid="pone.0045885-Williams2">[14]</xref>, <xref ref-type="bibr" rid="pone.0045885-Rebuschat2">[15]</xref>, <xref ref-type="bibr" rid="pone.0045885-Rebuschat1">[8]</xref>). We further found that the differences in grammatical complexity between tail- and centre-embedding and right or left branching affect the learning performance. This accords with Hawkins's performance and correspondence hypothesis for natural grammars and provides a hint towards a cognitive preference for adjacent tail- rather than centre-embedding structures which afford some ease of processing.</p>
    </sec>
    <sec id="s7">
      <title>Supporting Information</title>
      <supplementary-material id="pone.0045885.s001" mimetype="application/msword" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.s001" xlink:type="simple">
        <label>Appendix S1</label>
        <caption>
          <p><bold>Finite-state representations of formal grammars used in two studies </bold><xref ref-type="bibr" rid="pone.0045885-Saffran2">[<bold>34]</bold></xref>, <xref ref-type="bibr" rid="pone.0045885-Saffran4">[85]</xref><bold>.</bold></p>
          <p>(DOC)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pone.0045885.s002" mimetype="application/vnd.ms-excel" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0045885.s002" xlink:type="simple">
        <label>Appendix S2</label>
        <caption>
          <p>
            <bold>Stimulus materials used in the study.</bold>
          </p>
          <p>(XLS)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We would like to thank the editor and two anonymous reviewers for valuable and helpful comments on earlier versions of this manuscript.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pone.0045885-Reber1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reber</surname><given-names>AS</given-names></name> (<year>1989</year>) <article-title>Implicit learning and tacit knowledge</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>118</volume>: <fpage>219</fpage>–<lpage>235</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Land1">
        <label>2</label>
        <mixed-citation publication-type="other" xlink:type="simple">Land MF (1998) The visual control of steering. In: Harris LR, Jenkin K, editors. Vision and action. Cambridge University Press. pp. 163–180.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Reed1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reed</surname><given-names>N</given-names></name>, <name name-style="western"><surname>McLeod</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name> (<year>2010</year>) <article-title>Implicit knowledge and motor skill: What people who know how to catch don't know</article-title>. <source>Consciousness &amp; Cognition</source> <volume>19</volume>: <fpage>63</fpage>–<lpage>76</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Masters1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Masters</surname><given-names>RSW</given-names></name>, <name name-style="western"><surname>MacMahon</surname><given-names>KMA</given-names></name>, <name name-style="western"><surname>Pall</surname><given-names>HS</given-names></name> (<year>2004</year>) <article-title>Implicit motor learning in Parkinson's disease</article-title>. <source>Rehabilitation Psycholology</source> <volume>49</volume>: <fpage>79</fpage>–<lpage>82</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Saffran1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saffran</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Newport</surname><given-names>EL</given-names></name>, <name name-style="western"><surname>Aslin</surname><given-names>RN</given-names></name> (<year>1996</year>) <article-title>Word segmentation: The role of distributional cues</article-title>. <source>Journal of Memory and Language</source> <volume>35</volume>: <fpage>606</fpage>–<lpage>621</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Bigand1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bigand</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Poulin-Charronnat</surname><given-names>B</given-names></name> (<year>2006</year>) <article-title>Are we “experienced listeners”? a review of the musical capacities that do not depend on formal musical training</article-title>. <source>Cognition</source> <volume>100</volume>: <fpage>100</fpage>–<lpage>130</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Kenny1">
        <label>7</label>
        <mixed-citation publication-type="other" xlink:type="simple">Kenny BJ, Gellrich M (2002) Improvisation. In: Parncutt R, McPherson GE, editors. The science and psychology of music performance: Creative strategies for teaching and learning. Oxford: Oxford University Press. pp. 163–180.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Rebuschat1">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rebuschat</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Rohrmeier</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>Implicit learning and acquisition of music</article-title>. <source>Topics in Cognitive Science</source> E-pub ahead of print. DOI: 10.1111/j.1756-8765.2012.01223.x.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Rohrmeier1">
        <label>9</label>
        <mixed-citation publication-type="other" xlink:type="simple">Rohrmeier M (2010) Implicit learning of musical structure. Experimental and computational modelling approaches. PhD Dissertation, Centre for Music &amp; Science, Faculty of Music. Cambridge: University of Cambridge, UK.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Chomsky1">
        <label>10</label>
        <mixed-citation publication-type="other" xlink:type="simple">Chomsky N (1965) Aspects of the Theory of Syntax. Cambridge, MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Cleeremans1">
        <label>11</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cleeremans</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Destrebecqz</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Boyer</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Implicit learning: News from the front</article-title>. <source>Trends in Cognitive Sciences</source> <volume>2</volume>: <fpage>406</fpage>–<lpage>416</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Frensch1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frensch</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Rünger</surname><given-names>D</given-names></name> (<year>2003</year>) <article-title>Implicit Learning</article-title>. <source>Current Directions in Psychological Science</source> <volume>12</volume> (<issue>1</issue>)  <fpage>13</fpage>–<lpage>17</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Williams1">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname><given-names>JN</given-names></name> (<year>2005</year>) <article-title>Learning without awareness</article-title>. <source>Studies in Second Language Acquisition</source> <volume>27</volume> (<issue>2</issue>)  <fpage>269</fpage>–<lpage>304</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Williams2">
        <label>14</label>
        <mixed-citation publication-type="other" xlink:type="simple">Williams JN (2009) Implicit learning. In: Ritchie WC, Bhatia TK, editors. The New Handbook of Second Language Acquisition. Emerald Group Publishing Limited. pp. 319–353.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Rebuschat2">
        <label>15</label>
        <mixed-citation publication-type="other" xlink:type="simple">Rebuschat P (2008) Implicit learning of natural language syntax. PhD Dissertation. Research Centre for English and Applied Linguistics, University of Cambridge.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Altmann1">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Altmann</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Goode</surname><given-names>A</given-names></name> (<year>1995</year>) <article-title>On the modality independence of implicitly learned grammatical knowledge</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, &amp; Cognition</source> <volume>21</volume>: <fpage>899</fpage>–<lpage>912</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Bigand2">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bigand</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Perruchet</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Boyer</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Implicit learning of an artificial grammar of musical timbres</article-title>. <source>Cahiers De Psychologie Cognitive-Current Psychology of Cognition</source> <volume>17</volume> (<issue>3</issue>)  <fpage>577</fpage>–<lpage>600</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Altmann</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Kwan</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Goode</surname><given-names>A</given-names></name> (<year>1995</year>) <article-title>Unconscious knowledge of artificial grammars is applied strategically</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, &amp; Cognition</source> <volume>21</volume>: <fpage>1322</fpage>–<lpage>1338</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Loui1">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loui</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Wessel</surname><given-names>DL</given-names></name> (<year>2008</year>) <article-title>Effects of Set Size on the Learning and Liking of an Artificial Musical System</article-title>. <source>Musicae Scientiae</source> <volume>12</volume>: <fpage>201</fpage>–<lpage>218</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Pothos1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pothos</surname><given-names>EM</given-names></name> (<year>2007</year>) <article-title>Theories of Artificial Grammar Learning</article-title>. <source>Psychological Bulletin</source> <volume>133</volume>: <fpage>227</fpage>–<lpage>244</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Reber2">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reber</surname><given-names>AS</given-names></name> (<year>1967</year>) <article-title>Implicit learning of artificial grammars</article-title>. <source>Journal of Verbal Learning and Verbal Behavior</source> <volume>6</volume>: <fpage>855</fpage>–<lpage>863</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Reber3">
        <label>22</label>
        <mixed-citation publication-type="other" xlink:type="simple">Reber AS (1993) Implicit learning and tacit knowledge: An essay on the cognitive unconscious. New York: Oxford University Press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Redington1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Redington</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Chater</surname><given-names>N</given-names></name> (<year>1996</year>) <article-title>Transfer in artificial grammar learning: A reevaluation</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>125</volume>: <fpage>123</fpage>–<lpage>138</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Rohrmeier2">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohrmeier</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Rebuschat</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Cross</surname><given-names>I</given-names></name> (<year>2010</year>) <article-title>Incidental and online learning of melodic structure</article-title>. <source>Consciousness &amp; Cognition</source> <volume>20</volume> (<issue>2</issue>)  <fpage>214</fpage>–<lpage>222</lpage> In press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Braine1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Braine</surname><given-names>MDS</given-names></name> (<year>1963</year>) <article-title>On learning the grammatical order of words</article-title>. <source>Psychological Review</source> <volume>70</volume> (<issue>4</issue>)  <fpage>323</fpage>–<lpage>348</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Smith1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname><given-names>KH</given-names></name> (<year>1966</year>) <article-title>Grammatical intrusions in the free recall of structured letter pairs</article-title>. <source>Journal of Verbal Learning and Verbal Behavior</source> <volume>5</volume>: <fpage>447</fpage>–<lpage>454</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Moeser1">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moeser</surname><given-names>SD</given-names></name>, <name name-style="western"><surname>Bregman</surname><given-names>AS</given-names></name> (<year>1972</year>) <article-title>The role of reference in the acquisition of a miniature artificial language</article-title>. <source>Journal of Verbal Learning and Verbal Behavior</source> <volume>11</volume>: <fpage>759</fpage>–<lpage>769</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Gmez1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gómez</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Gerken</surname><given-names>LA</given-names></name> (<year>2000</year>) <article-title>Infant artificial language learning and language acquisition</article-title>. <source>Trends in Cognitive Sciences</source> <volume>4</volume>: <fpage>178</fpage>–<lpage>186</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Friederici1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Hahne</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mecklinger</surname><given-names>A</given-names></name> (<year>1996</year>) <article-title>Temporal structure of syntactic parsing: Early and late event-related brain potential effects elicited by syntactic anomalies</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>22</volume>: <fpage>1219</fpage>–<lpage>1248</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Friederici2">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Steinhauer</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Pfeifer</surname><given-names>E</given-names></name> (<year>2002</year>) <article-title>Brain signatures of artificial language processing: Evidence challenging the critical period hypothesis</article-title>. <source>Proceedings of the National Academy of Sciences of the USA</source> <volume>99</volume>: <fpage>529</fpage>–<lpage>534</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Opitz1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Opitz</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name> (<year>2003</year>) <article-title>Interactions of the hippocampal system and the prefrontal cortex in learning language-like rules</article-title>. <source>Neuro Image</source> <volume>19</volume>: <fpage>1730</fpage>–<lpage>1737</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Opitz2">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Opitz</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name> (<year>2007</year>) <article-title>Neural basis of processing sequential and hierarchical syntactic structures</article-title>. <source>Human Brain Mapping</source> <volume>28</volume>: <fpage>585</fpage>–<lpage>592</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Aslin1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aslin</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Saffran</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Newport</surname><given-names>EL</given-names></name> (<year>1998</year>) <article-title>Computation of conditional probability statistics by 8-month-old infants</article-title>. <source>Psychological Science</source> <volume>9</volume>: <fpage>321</fpage>–<lpage>324</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Saffran2">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saffran</surname><given-names>JR</given-names></name> (<year>2002</year>) <article-title>Constraints on statistical language learning</article-title>. <source>Journal of Memory and Language</source> <volume>47</volume>: <fpage>172</fpage>–<lpage>196</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Saffran3">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saffran</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Aslin</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Newport</surname><given-names>EL</given-names></name> (<year>1996</year>) <article-title>Statistical learning by 8-month-olds</article-title>. <source>Science</source> <volume>274</volume>: <fpage>1926</fpage>–<lpage>1928</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Thiessen1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thiessen</surname><given-names>ED</given-names></name>, <name name-style="western"><surname>Saffran</surname><given-names>JR</given-names></name> (<year>2007</year>) <article-title>Learning to learn: Acquisition of stress-based strategies for word segmentation</article-title>. <source>Language Learning and Development</source> <volume>3</volume>: <fpage>73</fpage>–<lpage>100</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Gmez2">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gómez</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Gerken</surname><given-names>LA</given-names></name> (<year>1999</year>) <article-title>Artificial grammar learning by one-year-olds leads to specific and abstract knowledge</article-title>. <source>Cognition</source> <volume>70</volume>: <fpage>109</fpage>–<lpage>135</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Finch1">
        <label>38</label>
        <mixed-citation publication-type="other" xlink:type="simple">Finch S, Chater N (1992) Bootstrapping syntactic categories using statistical methods. Proceedings of the 1st SHOE Workshop on Statistical Methods in Natural Language. pp. 229–235. ITK Proceedings 92/1, Institute for Language Technology and AI: The Netherlands: Tilburg University.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Redington2">
        <label>39</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Redington</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Chater</surname><given-names>N</given-names></name> (<year>1998</year>) <article-title>Connectionist and statistical approaches to language acquisition: A distributional perspective</article-title>. <source>Language and Cognitive Processes</source> <volume>13</volume>: <fpage>129</fpage>–<lpage>191</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Redington3">
        <label>40</label>
        <mixed-citation publication-type="other" xlink:type="simple">Redington M, Chater N, Finch S (1993) Distributional information and the acquisition of linguistic categories: A statistical approach. In: Proceedings of the 15th Annual Conference of the Cognitive Science Society. Hillsdale, NJ: Erlbaum. pp. 848–853.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Chomsky2">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chomsky</surname><given-names>N</given-names></name> (<year>1956</year>) <article-title>Three models for the description of language</article-title>. <source>IRE Transactions on Information Theory</source> <volume>IT-2</volume>: <fpage>113</fpage>–<lpage>124</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Chomsky3">
        <label>42</label>
        <mixed-citation publication-type="other" xlink:type="simple">Chomsky N (1957) Syntactic Structures. The Hague: Mouton.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Pullum1">
        <label>43</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pullum</surname><given-names>GK</given-names></name> (<year>1986</year>) <article-title>Footloose and context-free</article-title>. <source>Natural Language and Linguistic Theory</source> <volume>4</volume>: <fpage>409</fpage>–<lpage>414</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Lobina1">
        <label>44</label>
        <mixed-citation publication-type="other" xlink:type="simple">Lobina DJ (2011) Recursion and the competence/performance distinction in AGL tasks. Language and Cognitive Processes.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Lobina2">
        <label>45</label>
        <mixed-citation publication-type="other" xlink:type="simple">Lobina DJ (2010) Recursion and Linguistics: an addendum to Marcus Tomalin's Reconsidering Recursion in Syntactic Theory. Interlingüística XX.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Lobina3">
        <label>46</label>
        <mixed-citation publication-type="other" xlink:type="simple">Lobina DJ, García-Albea JE (2009) Recursion and cognitive science: data structures and mechanisms. In: Taatgen NA, van Rijn H, editors. In: Proceedings of the 31st Annual Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society. pp. 1347–1352.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes2">
        <label>47</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Longuet-Higgins</surname><given-names>HC</given-names></name> (<year>2004</year>) <article-title>Can musical transformations be implicitly learned?</article-title> <source>Cognitive Science</source> <volume>28</volume>: <fpage>531</fpage>–<lpage>558</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Fitch1">
        <label>48</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fitch</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Hauser</surname><given-names>M</given-names></name> (<year>2004</year>) <article-title>Computational constraints on syntactic processing in a nonhuman primate</article-title>. <source>Science</source> <volume>303</volume>: <fpage>377</fpage>–<lpage>380</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Friederici3">
        <label>49</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friederici</surname><given-names>A</given-names></name> (<year>2004</year>) <article-title>Processing local transitions versus long-distance syntactic hierarchies</article-title>. <source>Trends in Cognitive Sciences</source> <volume>8</volume> (<issue>6</issue>)  <fpage>245</fpage>–<lpage>247</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Uddn1">
        <label>50</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Uddén</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ingvar</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hagoort</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Petersson</surname><given-names>KM</given-names></name> (<year>2012</year>) <article-title>Implicit acquisition of grammars with crossed and nested non-adjacent dependencies: Investigating the push-down stack model</article-title>. <source>Cognitive Science</source> <comment>E-pub ahead of print. DOI: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1551-6709.2012.01235.x" xlink:type="simple">10.1111/j.1551-6709.2012.01235.x</ext-link>.</comment></mixed-citation>
      </ref>
      <ref id="pone.0045885-Jiang1">
        <label>51</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jiang</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Zhu</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Guo</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Ma</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>Z</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Unconscious structural knowledge of tonal symmetry: Tang poetry redefines limits of implicit learning</article-title>. <source>Consciousness &amp; Cognition</source> <volume>21</volume>: <fpage>476</fpage>–<lpage>486</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Tomalin1">
        <label>52</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tomalin</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Reconsidering recursion in syntactic theory</article-title>. <source>Lingua</source> <volume>117</volume> (<issue>10</issue>)  <fpage>1784</fpage>–<lpage>1800</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Martins1">
        <label>53</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martins</surname><given-names>MD</given-names></name> (<year>2012</year>) <article-title>Distinctive signatures of recursion</article-title>. <source>Phil Trans R Soc B</source> <volume>367</volume>: <fpage>2055</fpage>–<lpage>2064</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Fitch2">
        <label>54</label>
        <mixed-citation publication-type="other" xlink:type="simple">Fitch WT, Friederici AD (2012) Artificial grammar learning meets formal language theory: an overview.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Chomsky4">
        <label>55</label>
        <mixed-citation publication-type="other" xlink:type="simple">Chomsky N (1995) The Minimalist Program. MIT Press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Ferreira1">
        <label>56</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ferreira</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Patson</surname><given-names>ND</given-names></name> (<year>2007</year>) <article-title>The “Good Enough” Approach to Language Comprehension</article-title>. <source>Language and Linguistics Compass</source> <volume>1</volume>: <fpage>71</fpage>–<lpage>83</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Hauser1">
        <label>57</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hauser</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Chomsky</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Fitch</surname><given-names>T</given-names></name> (<year>2002</year>) <article-title>The faculty of language: what is it, who has it, and how did it evolve?</article-title> <source>Science</source> <volume>298</volume>: <fpage>1569</fpage>–<lpage>1579</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Zwart1">
        <label>58</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zwart</surname><given-names>JW</given-names></name> (<year>2011</year>) <article-title>Recursion in Language: A Layered-Derivation Approach</article-title>. <source>Biolinguistics</source> <volume>5</volume> (<issue>1–2</issue>)  <fpage>43</fpage>–<lpage>56</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-deVries1">
        <label>59</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Vries</surname><given-names>MH</given-names></name>, <name name-style="western"><surname>Christiansen</surname><given-names>MH</given-names></name>, <name name-style="western"><surname>Petersson</surname><given-names>KM</given-names></name> (<year>2011</year>) <article-title>Learning recursion: Multiple nested and crossed dependencies</article-title>. <source>Biolinguistics</source> <volume>5</volume>: <fpage>10</fpage>–<lpage>35</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Friederici4">
        <label>60</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Bahlmann</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Friedrich</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Makuuchi</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>The neural basis of recursion of complex syntactic hierarchy</article-title>. <source>Biolinguistics</source> <volume>5</volume> (<issue>1–2</issue>)  <fpage>87</fpage>–<lpage>104</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Pinker1">
        <label>61</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pinker</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Jackendoff</surname><given-names>R</given-names></name> (<year>2005</year>) <article-title>The faculty of language: what's special about it?</article-title> <source>Cognition</source> <volume>95</volume>: <fpage>201</fpage>–<lpage>236</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Jackendoff1">
        <label>62</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jackendoff</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Lerdahl</surname><given-names>F</given-names></name> (<year>2006</year>) <article-title>The Capacity for Music: What's Special about it?</article-title> <source>Cognition</source> <volume>100</volume>: <fpage>33</fpage>–<lpage>72</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Lerdahl1">
        <label>63</label>
        <mixed-citation publication-type="other" xlink:type="simple">Lerdahl F, Jackendoff R (1983) A generative theory of tonal music. Cambridge, MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Steedman1">
        <label>64</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steedman</surname><given-names>M</given-names></name> (<year>1984</year>) <article-title>A generative grammar for Jazz Chord sequences</article-title>. <source>Music Perception</source> <volume>2</volume> (<issue>1</issue>)  <fpage>52</fpage>–<lpage>77</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Rohrmeier3">
        <label>65</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rohrmeier</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Towards a generative syntax of tonal harmony</article-title>. <source>Journal of Mathematics and Music</source> <volume>5</volume> (<issue>1</issue>)  <fpage>35</fpage>–<lpage>53</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Patel1">
        <label>66</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Patel</surname><given-names>AD</given-names></name> (<year>2003</year>) <article-title>Language, music, syntax and the brain</article-title>. <source>Nature Neuroscience</source> <volume>6</volume>: <fpage>674</fpage>–<lpage>681</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Jackendoff2">
        <label>67</label>
        <mixed-citation publication-type="other" xlink:type="simple">Jackendoff R (2007) Language, Consciousness, Culture: Essays on Mental Structure. MIT Press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Jackendoff3">
        <label>68</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jackendoff</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>Parallels and nonparallels between language and music</article-title>. <source>Music Perception</source> <volume>26</volume>: <fpage>195</fpage>–<lpage>204</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Steedman2">
        <label>69</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steedman</surname><given-names>M</given-names></name> (<year>2002</year>) <article-title>Plans, affordances, and combinatory grammar</article-title>. <source>Linguistics and Philosophy</source> <volume>25</volume>: <fpage>723</fpage>–<lpage>753</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Chomsky5">
        <label>70</label>
        <mixed-citation publication-type="other" xlink:type="simple">Chomsky N, Schutzenberger MP (1963) The algebraic theory of context-free languages. In: Braffort P, Hirschberg D, editors. Computer programming and formal systems. North–Holland, Amsterdam. pp. 118–161.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Hopcroft1">
        <label>71</label>
        <mixed-citation publication-type="other" xlink:type="simple">Hopcroft JE, Ullman JD (1969) Formal Languages and Their Relation to Automata. Addison-Wesley.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Friederici5">
        <label>72</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Gunter</surname><given-names>TC</given-names></name>, <name name-style="western"><surname>Hahne</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mauth</surname><given-names>K</given-names></name> (<year>2004</year>) <article-title>The relative timing of syntactic and semantic processes in sentence comprehension</article-title>. <source>NeuroReport</source> <volume>15</volume> (<issue>1</issue>)  <fpage>165</fpage>–<lpage>169</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Friederici6">
        <label>73</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Bahlmann</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Heim</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Schubotz</surname><given-names>RI</given-names></name>, <name name-style="western"><surname>Anwander</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>The brain differentiates human and non-human grammars: Functional localization and structural connectivity</article-title>. <source>Proceedings of the National Academy of Sciences of the USA</source> <volume>103</volume> (<issue>7</issue>)  <fpage>2458</fpage>–<lpage>2463</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Bahlmann1">
        <label>74</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bahlmann</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Schubotz</surname><given-names>RI</given-names></name>, <name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name> (<year>2008</year>) <article-title>Hierarchical sequencing engages Broca's area</article-title>. <source>Neuro Image</source> <volume>42</volume>: <fpage>525</fpage>–<lpage>534</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Bahlmann2">
        <label>75</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bahlmann</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Schubotz</surname><given-names>RI</given-names></name>, <name name-style="western"><surname>Mueller</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Koester</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name> (<year>2009</year>) <article-title>Neural circuits of hierarchical visuo-spatial sequence processing</article-title>. <source>Brain Research</source> <volume>1298</volume>: <fpage>161</fpage>–<lpage>170</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Hochmann1">
        <label>76</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hochmann</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Azadpour</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Mehler</surname><given-names>J</given-names></name> (<year>2008</year>) <article-title>Do humans really learn a<sup>n</sup>b<sup>n</sup> artificial grammars from exemplars?</article-title> <source>Cognitive Science</source> <volume>32</volume>: <fpage>1021</fpage>–<lpage>1036</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Perruchet1">
        <label>77</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perruchet</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Rey</surname><given-names>A</given-names></name> (<year>2005</year>) <article-title>Does the mastery of hierarchical structures distinguishes humans from non-human primates?</article-title> <source>Psychonomic Bulletin &amp; Review</source> <volume>12</volume> (<issue>2</issue>)  <fpage>307</fpage>–<lpage>313</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-deVries2">
        <label>78</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Vries</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Monaghan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Knecht</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Zwitserlood</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>Syntactic structure and artificial grammar learning: The learnability of embedded hierarchical structures</article-title>. <source>Cognition</source> <volume>107</volume>: <fpage>763</fpage>–<lpage>774</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Lai1">
        <label>79</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lai</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Poletiek</surname><given-names>FH</given-names></name> (<year>2010</year>) <article-title>The impact of adjacent-dependencies and staged-input on the learnability of center-embedded hierarchical structures</article-title>. <source>Cognition</source> <volume>118</volume>: <fpage>265</fpage>–<lpage>273</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Poletiek1">
        <label>80</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poletiek</surname><given-names>FH</given-names></name> (<year>2011</year>) <article-title>What in the world makes recursion so easy to learn? A statistical account of the staged input effect on learning a centre embedded hierarchical structure in AGL</article-title>. <source>Biolinguistics</source> <volume>5</volume>: <fpage>1</fpage>–<lpage>2, 36–42</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Thompson1">
        <label>81</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thompson</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Newport</surname><given-names>EL</given-names></name> (<year>2007</year>) <article-title>Statistical Learning of Syntax: The Role of Transitional. Probability</article-title>. <source>Language Learning and Development</source> <volume>3</volume>: <fpage>1</fpage>–<lpage>42</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Brooks1">
        <label>82</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brooks</surname><given-names>LR</given-names></name>, <name name-style="western"><surname>Vokey</surname><given-names>JR</given-names></name> (<year>1991</year>) <article-title>Abstract analogies and abstracted grammars: Comments on Reber (1989) and Mathews et al. (1989)</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>120</volume>: <fpage>316</fpage>–<lpage>323</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Scott1">
        <label>83</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scott</surname><given-names>RB</given-names></name>, <name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name> (<year>2008</year>) <article-title>The conscious, the unconscious, and familiarity</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, &amp; Cognition</source> <volume>34</volume>: <fpage>1264</fpage>–<lpage>1288</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Tunney1">
        <label>84</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tunney</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Altmann</surname><given-names>GTM</given-names></name> (<year>2001</year>) <article-title>Two Modes of Transfer in Artificial Grammar Learning</article-title>. <source>Journal of Experimental Psychology: Learning Memory &amp; Cognition</source> <volume>27</volume>: <fpage>614</fpage>–<lpage>639</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Saffran4">
        <label>85</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saffran</surname><given-names>J</given-names></name> (<year>2001</year>) <article-title>The use of predictive dependencies in language learning</article-title>. <source>Journal of Memory and Language</source> <volume>44</volume>: <fpage>493</fpage>–<lpage>515</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Guo1">
        <label>86</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guo</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Zheng</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Zhu</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Chen</surname></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Acquisition of conscious and unconscious knowledge of semantic prosody</article-title>. <source>Consciousness &amp; Cognition</source> <publisher-name>In Press</publisher-name>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Leung1">
        <label>87</label>
        <mixed-citation publication-type="other" xlink:type="simple">Leung J (2007) Implicit Learning of Form-Meaning Connections. Doctoral Dissertation, University of Cambridge, UK.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Leung2">
        <label>88</label>
        <mixed-citation publication-type="other" xlink:type="simple">Leung J, Williams JN (2006) Implicit learning of form-meaning connections. In: Sun R, Miyake N, editors. Proceedings of the Annual Meeting of the Cognitive Science Society. Mahwah, N.J.: Lawrence Erlbaum. pp. 465–470.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Robinson1">
        <label>89</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robinson</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Cognitive abilities, chunk-strength, and frequency effects in implicit artificial grammar and incidental L2 learning: Replications of Reber, Walkenfeld, and Hernstadt (1991) and Knowlton and Squire (1996) and their relevance for SLA</article-title>. <source>Studies in Second Language Acquisition</source> <volume>27</volume>: <fpage>235</fpage>–<lpage>268</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Williams3">
        <label>90</label>
        <mixed-citation publication-type="other" xlink:type="simple">Williams JN (2004) Implicit learning of form-meaning connections. In: Van Patten B, Williams J, Rott S, Overstreet M, editors. Form-Meaning Connections in SLA. Mahwah, N.J.: Lawrence Erlbaum. pp. 203–18.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Williams4">
        <label>91</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Kuribara</surname><given-names>C</given-names></name> (<year>2008</year>) <article-title>Comparing a nativist and emergentist approach to the initial stage of SLA: An investigation of Japanese scrambling</article-title>. <source>Lingua</source> <volume>118</volume>: <fpage>522</fpage>–<lpage>53</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Vokey1">
        <label>92</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vokey</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Brooks</surname><given-names>JR</given-names></name> (<year>1992</year>) <article-title>The salience of item knowledge in learning artificial grammars</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>18</volume>: <fpage>328</fpage>–<lpage>344</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Johnstone1">
        <label>93</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnstone</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Shanks</surname><given-names>DR</given-names></name> (<year>1999</year>) <article-title>Two mechanisms in implicit artificial grammar learning? Comment on Meulemans and Van der Linden (1997)</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>25</volume>: <fpage>524</fpage>–<lpage>531</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Mathews1">
        <label>94</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mathews</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Buss</surname><given-names>RR</given-names></name>, <name name-style="western"><surname>Stanley</surname><given-names>WB</given-names></name>, <name name-style="western"><surname>Blanchard-Fields</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Cho</surname><given-names>JR</given-names></name>, <etal>et al</etal>. (<year>1989</year>) <article-title>Role of implicit and explicit processes in learning from examples: A synergistic effect</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>15</volume>: <fpage>1083</fpage>–<lpage>1100</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Newport1">
        <label>95</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Newport</surname><given-names>EL</given-names></name>, <name name-style="western"><surname>Aslin</surname><given-names>RN</given-names></name> (<year>2004</year>) <article-title>Learning at a distance: Statistical learning of nonadjacent dependencies</article-title>. <source>Cognitive Psychology</source> <volume>48</volume>: <fpage>127</fpage>–<lpage>162</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Creel1">
        <label>96</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Creel</surname><given-names>SC</given-names></name>, <name name-style="western"><surname>Newport</surname><given-names>EL</given-names></name>, <name name-style="western"><surname>Aslin</surname><given-names>RN</given-names></name> (<year>2004</year>) <article-title>Distant melodies: Statistical learning of nonadjacent dependencies in tone sequences</article-title>. <source>Journal of Experimental Psychology: Learning, Memory and Cognition</source> <volume>30</volume> (<issue>5</issue>)  <fpage>1119</fpage>–<lpage>1130</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Gebhart1">
        <label>97</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gebhart</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Newport</surname><given-names>EL</given-names></name>, <name name-style="western"><surname>Aslin</surname><given-names>RN</given-names></name> (<year>2009</year>) <article-title>Statistical learning of adjacent and non-adjacent dependencies among non-linguistic sounds</article-title>. <source>Psychonomic Bulletin &amp; Review</source> <volume>16</volume> (<issue>3</issue>)  <fpage>486</fpage>–<lpage>490</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Gomez1">
        <label>98</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gomez</surname><given-names>RL</given-names></name> (<year>2002</year>) <article-title>Variability and detection of invariant structure</article-title>. <source>Psychological Science</source> <volume>13</volume>: <fpage>431</fpage>–<lpage>436</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Kuhn1">
        <label>99</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kuhn</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name> (<year>2005</year>) <article-title>Implicit learning of non-local musical rules</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, &amp; Cognition</source> <volume>31</volume>: <fpage>1417</fpage>–<lpage>1432</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes3">
        <label>100</label>
        <mixed-citation publication-type="other" xlink:type="simple">Dienes Z, Kuhn G, Guo XY, Jones C (2012) Communicating structure, affect and movement: Commentary on Bharucha, Curtis &amp; Paroo. In: Rebuschat P, Rohrmeier M, Cross I, Hawkins J, editors. Language and Music as Cognitive System. Oxford: Oxford University Press. In press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes4">
        <label>101</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Kuhn</surname><given-names>G</given-names></name> (<year>2012</year>) <article-title>Implicitly learning to detect symmetries: Reply to Desmet et al</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, &amp; Cognition</source> In press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Reeder1">
        <label>102</label>
        <mixed-citation publication-type="other" xlink:type="simple">Reeder PA, Newport EL, Aslin RN (2009) The role of distributional information in linguistic category formation. In: Taatgen N, van Rijn H, editors. Proceedings of the 31st Annual Meeting of the Cognitive Science Society.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Hawkins1">
        <label>103</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hawkins</surname><given-names>J</given-names></name> (<year>1990</year>) <article-title>A Parsing theory of word order universals</article-title>. <source>Linguistic Inquiry</source> <volume>21</volume> (<issue>2</issue>)  <fpage>223</fpage>–<lpage>262</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Hawkins2">
        <label>104</label>
        <mixed-citation publication-type="other" xlink:type="simple">Hawkins J (2004) Efficiency and Complexity in Grammars. Oxford: Oxford University Press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Reber4">
        <label>105</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reber</surname><given-names>AS</given-names></name>, <name name-style="western"><surname>Lewis</surname><given-names>S</given-names></name> (<year>1977</year>) <article-title>Implicit learning: an analysis of the form and structure of a body of tacit knowledge</article-title>. <source>Cognition</source> <volume>114</volume>: <fpage>14</fpage>–<lpage>24</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-ServanSchreiber1">
        <label>106</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Servan-Schreiber</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>JR</given-names></name> (<year>1990</year>) <article-title>Learning artificial grammars with competitive chunking</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>16</volume> (<issue>4</issue>)  <fpage>592</fpage>–<lpage>608</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Knowlton1">
        <label>107</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knowlton</surname><given-names>BJ</given-names></name>, <name name-style="western"><surname>Squire</surname><given-names>LR</given-names></name> (<year>1994</year>) <article-title>The information acquired during artificial grammar learning</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>20</volume> (<issue>1</issue>)  <fpage>79</fpage>–<lpage>91</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Mueller1">
        <label>108</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mueller</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Bahlmann</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name> (<year>2010</year>) <article-title>Learnability of Embedded Syntactic Structures Depends on Prosodic Cues</article-title>. <source>Cognitive Science</source> <volume>34</volume>: <fpage>338</fpage>–<lpage>349</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Kuhn2">
        <label>109</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kuhn</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name> (<year>2008</year>) <article-title>Learning non-local dependencies</article-title>. <source>Cognition</source> <volume>106</volume>: <fpage>184</fpage>–<lpage>206</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Cleeremans2">
        <label>110</label>
        <mixed-citation publication-type="other" xlink:type="simple">Cleeremans A, Dienes Z (2008) Computational models of implicit learning. In: Sun R, editors, Cambridge Handbook of Computational Psychology. Cambridge: Cambridge University Press. pp. 396–421.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Jacoby1">
        <label>111</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jacoby</surname><given-names>LL</given-names></name> (<year>1991</year>) <article-title>A process dissociation framework: separating automatic from intentional use of memory</article-title>. <source>Journal of Memory and Language</source> <volume>30</volume>: <fpage>513</fpage>–<lpage>541</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Langendoen1">
        <label>112</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langendoen</surname><given-names>T</given-names></name> (<year>1975</year>) <article-title>Finite-state parsing of phrase-structure grammars and the status of readjustment rules in the lexicon</article-title>. <source>Linguistic Inquiry</source> <volume>6</volume>: <fpage>533</fpage>–<lpage>554</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Marcel1">
        <label>113</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marcel</surname><given-names>AJ</given-names></name> (<year>1983</year>) <article-title>Conscious and unconscious perception: Experiments on visual masking and word recognition</article-title>. <source>Cognitive Psychology</source> <volume>15</volume>: <fpage>197</fpage>–<lpage>237</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Fu1">
        <label>114</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fu</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Fu</surname><given-names>X</given-names></name> (<year>2010</year>) <article-title>Can unconscious knowledge allow control in sequence learning?</article-title> <source>Consciousness &amp; Cognition</source> <volume>19</volume>: <fpage>462</fpage>–<lpage>475</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes5">
        <label>115</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name> (<year>2004</year>) <article-title>Assumptions of subjective measures of unconscious mental states: Higher order thoughts and bias</article-title>. <source>Journal of Consciousness Studies</source> <volume>11</volume>: <fpage>25</fpage>–<lpage>45</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes6">
        <label>116</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name> (<year>2008a</year>) <article-title>Subjective measures of unconscious knowledge</article-title>. <source>Progress in Brain Research</source> <volume>168</volume>: <fpage>49</fpage>–<lpage>64</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes7">
        <label>117</label>
        <mixed-citation publication-type="other" xlink:type="simple">Dienes Z (2012) Conscious versus unconscious learning of structure. In: Rebuschat P, Williams J, editors. Statistical Learning and Language Acquisition. Mouton de Gruyter Publishers. pp. 337–364.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Jimenez1">
        <label>118</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jimenez</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Vaquero</surname><given-names>JMM</given-names></name>, <name name-style="western"><surname>Lupiáñez</surname><given-names>J</given-names></name> (<year>2006</year>) <article-title>Qualitative differences between implicit and explicit sequence learning</article-title>. <source>Journal of Experimental Psychology: Learnning, Memory &amp; Cognition</source> <volume>32</volume>: <fpage>475</fpage>–<lpage>90</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Wilkinson1">
        <label>119</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilkinson</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Shanks</surname><given-names>DR</given-names></name> (<year>2004</year>) <article-title>Intentional control and implicit sequence learning</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>30</volume>: <fpage>354</fpage>–<lpage>369</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dulan1">
        <label>120</label>
        <mixed-citation publication-type="other" xlink:type="simple">Dulan DE (1962) The place of hypotheses and intentions: an analysis of verbal control in verbal conditioning. In: Eriksen CW, editors. Behavior and awareness. Durham N.C: Duke University Press. pp 102–129.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Lotz1">
        <label>121</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lotz</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kinder</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lachnit</surname><given-names>H</given-names></name> (<year>2009</year>) <article-title>Multiple regression analyses in artificial grammar learning: The importance of control groups</article-title>. <source>The Quarterly Journal of Experimental Psychology</source> <volume>62</volume> (<issue>3</issue>)  <fpage>576</fpage>–<lpage>584</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Endress1">
        <label>122</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Endress</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Nespor</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Mehler</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Perceptual and memory constraints on language acquisition</article-title>. <source>Trends in Cognitive Sciences</source> <volume>13</volume> (<issue>8</issue>)  <fpage>348</fpage>–<lpage>353</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Mathews2">
        <label>123</label>
        <mixed-citation publication-type="other" xlink:type="simple">Mathews RC, Roussel LG (1997) Abstractness of implicit knowledge: A cognitive evolutionary perspective. In: Berry DC, editors. How implicit is implicit learning?. London: Oxford University Press. pp. 13–47.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes8">
        <label>124</label>
        <mixed-citation publication-type="other" xlink:type="simple">Dienes Z (2008b) Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference. Palgrave Macmillan</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes9">
        <label>125</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name> (<year>2011</year>) <article-title>Bayesian versus Orthodox statistics: Which side are you on?</article-title> <source>Perspectives on Psychological Sciences</source> <volume>6</volume> (<issue>3</issue>)  <fpage>274</fpage>–<lpage>290</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Hawkins3">
        <label>126</label>
        <mixed-citation publication-type="other" xlink:type="simple">Hawkins J (1994) A Performance Theory of Order and Constituency. Cambridge University Press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Cleeremans3">
        <label>127</label>
        <mixed-citation publication-type="other" xlink:type="simple">Cleeremans A (1993) Mechanisms of Implicit Learning: Connectionist Models of Sequence Processing. Cambridge, MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Boucher1">
        <label>128</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boucher</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name> (<year>2003</year>) <article-title>Two ways of learning associations</article-title>. <source>Cognitive Science</source> <volume>27</volume>: <fpage>807</fpage>–<lpage>842</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Perruchet2">
        <label>129</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perruchet</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Vinter</surname><given-names>A</given-names></name> (<year>1998</year>) <article-title>PARSER: A model for word segmentation</article-title>. <source>Journal of Memory and Language</source> <volume>39</volume>: <fpage>246</fpage>–<lpage>263</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dryer1">
        <label>130</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dryer</surname><given-names>MS</given-names></name> (<year>1992</year>) <article-title>The Greenbergian word order correlations</article-title>. <source>Language</source> <volume>68</volume> (<issue>1</issue>)  <fpage>81</fpage>–<lpage>138</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Kirby1">
        <label>131</label>
        <mixed-citation publication-type="other" xlink:type="simple">Kirby S (1999) Function selection and innateness: The emergence of language universals. Oxford University Press.</mixed-citation>
      </ref>
      <ref id="pone.0045885-Dienes10">
        <label>132</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dienes</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Fahey</surname><given-names>R</given-names></name> (<year>1998</year>) <article-title>The role of implicit memory in controlling a dynamic system</article-title>. <source>Quarterly Journal of Experimental Psychology</source> <volume>51A</volume>: <fpage>593</fpage>–<lpage>614</lpage>.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>