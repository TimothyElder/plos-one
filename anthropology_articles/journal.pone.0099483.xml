<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-44627</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0099483</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Biotechnology</subject><subj-group><subject>Bioengineering</subject><subj-group><subject>Biomedical engineering</subject><subject>Biomimetics</subject></subj-group></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subject>Cognitive psychology</subject></subj-group></subj-group></subj-group><subj-group><subject>Physical anthropology</subject><subj-group><subject>Anthropometry</subject></subj-group></subj-group><subj-group><subject>Psychology</subject><subj-group><subject>Experimental psychology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Engineering and technology</subject><subj-group><subject>Electrical engineering</subject></subj-group><subj-group><subject>Electronics engineering</subject><subj-group><subject>Computer engineering</subject></subj-group></subj-group><subj-group><subject>Signal processing</subject><subj-group><subject>Image processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social sciences</subject><subj-group><subject>Anthropology</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Geometric Facial Gender Scoring: Objectivity of Perception</article-title>
<alt-title alt-title-type="running-head">Geometric Facial Gender Scoring: Objectivity of Perception</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Gilani</surname><given-names>Syed Zulqarnain</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Rooney</surname><given-names>Kathleen</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Shafait</surname><given-names>Faisal</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Walters</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Mian</surname><given-names>Ajmal</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>School of Computer Science and Software Engineering, The University of Western Australia, Perth, Western Australia</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>School of Anatomy, Physiology and Human Biology, The University of Western Australia, Perth, Western Australia</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Cranio-MaxilloFacial Unit, Princess Margaret Hospital for Children, Perth, Western Australia</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>D'Ausilio</surname><given-names>Alessandro</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>IIT - Italian Institute of Technology, Italy</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">zulqarnain.gilani@uwa.edu.au</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: SZG KR FS AM. Performed the experiments: SZG KR. Analyzed the data: SZG KR FS MW. Contributed reagents/materials/analysis tools: MW. Wrote the paper: SZG KR AM.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>12</day><month>6</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>6</issue>
<elocation-id>e99483</elocation-id>
<history>
<date date-type="received"><day>29</day><month>10</month><year>2013</year></date>
<date date-type="accepted"><day>15</day><month>5</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Gilani et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Gender score is the cognitive judgement of the degree of masculinity or femininity of a face which is considered to be a continuum. Gender scores have long been used in psychological studies to understand the complex psychosocial relationships between people. Perceptual scores for gender and attractiveness have been employed for quality assessment and planning of cosmetic facial surgery. Various neurological disorders have been linked to the facial structure in general and the facial gender perception in particular. While, subjective gender scoring by human raters has been a tool of choice for psychological studies for many years, the process is both time and resource consuming. In this study, we investigate the geometric features used by the human cognitive system in perceiving the degree of masculinity/femininity of a 3D face. We then propose a mathematical model that can mimic the human gender perception. For our experiments, we obtained 3D face scans of 64 subjects using the 3dMDface scanner. The textureless 3D face scans of the subjects were then observed in different poses and assigned a gender score by 75 raters of a similar background. Our results suggest that the human cognitive system employs a combination of Euclidean and geodesic distances between biologically significant landmarks of the face for gender scoring. We propose a mathematical model that is able to automatically assign an objective gender score to a 3D face with a correlation of up to 0.895 with the human subjective scores.</p>
</abstract>
<funding-group><funding-statement>This research was partly supported by Australian Research Council Discovery Grant DP110102399 and UWA Faculty of Engineering, Computing and Mathematics Development Grant. Syed Zulqarnain Gilani is supported by International Postgraduate Research Scholarship, and Faisal Shafait was supported by Australian Research Council grant LP110201008. The funding agencies had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="12"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Cognitive judgements of facial attractiveness, gender and the degree of masculinity/femininity are found to be universally reproducible in people of varied cultural and ethnic backgrounds <xref ref-type="bibr" rid="pone.0099483-Little1">[1]</xref>, <xref ref-type="bibr" rid="pone.0099483-Leopold1">[2]</xref>. The Human mind has the capability to assess facial masculinity/femininity and this gender attribute plays an important role in social behaviours. Psychologists and cognitive scientists have extensively analysed the role of perceived gender (masculinity/femininity) on various socio-psychological behaviours in a number of studies (see Table. 1 for a summary).</p>
<p>A subjective gender score is a tangible metric that human raters assign to the degree of masculinity/femininity of a face. This is because, though sex is binary, gender is understood to be a continuum. For example, <xref ref-type="fig" rid="pone-0099483-g001">Figure 1</xref> shows synthetic images of the same individual by varying its gender from very male to very female. In the literature these scores have also been referred to as perceptual gender scores, masculinity/femininity scores or masculinity/femininity index (referred later as masculinity index for brevity).</p>
<fig id="pone-0099483-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g001</object-id><label>Figure 1</label><caption>
<title>Facial gender is considered to be a continuum over masculinity or femininity.</title>
<p>Figure shows morphed 3D images of the same individual with gender varying from highly masculine to highly feminine. Which geometric features do human observers employ for ascribing a score to this variation and can such scores be replicated by computer algorithms? (Note: These images have been created from a model <xref ref-type="bibr" rid="pone.0099483-Singular1">[45]</xref>, <xref ref-type="bibr" rid="pone.0099483-Blanz1">[46]</xref> as we are barred from publishing images of actual subjects under ethics approval.)</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g001" position="float" xlink:type="simple"/></fig>
<p>Subjective gender scoring has been widely used by researchers in Psychology to study the relationship between sexual dimorphism and facial attractiveness <xref ref-type="bibr" rid="pone.0099483-Rhodes1">[3]</xref>, <xref ref-type="bibr" rid="pone.0099483-Little2">[4]</xref>, mate choice <xref ref-type="bibr" rid="pone.0099483-Rhodes2">[5]</xref>, <xref ref-type="bibr" rid="pone.0099483-Lee1">[8]</xref>, personal character traits <xref ref-type="bibr" rid="pone.0099483-Perrett1">[9]</xref> as well as perceived and actual health <xref ref-type="bibr" rid="pone.0099483-Rhodes3">[10]</xref>. Applications of subjective gender scores in medical and health care include analysis of the effects of syndromes (e.g. Autism Spectrum Disorder) on facial masculinity/femininity <xref ref-type="bibr" rid="pone.0099483-Bejerot1">[11]</xref>, relationship between sexual dimorphism and semen quality <xref ref-type="bibr" rid="pone.0099483-Peters1">[12]</xref>/facial symmetry <xref ref-type="bibr" rid="pone.0099483-Koehler1">[13]</xref>. Other uses include evaluation of the outcome of facial cosmetic surgery <xref ref-type="bibr" rid="pone.0099483-Dey1">[14]</xref>, <xref ref-type="bibr" rid="pone.0099483-Chung1">[15]</xref>. A comprehensive overview of the applications of subjective gender scoring is given in <xref ref-type="table" rid="pone-0099483-t001">Table 1</xref>. In these studies, a number of human raters are asked to judge the masculinity/femininity of the subjects.</p>
<table-wrap id="pone-0099483-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.t001</object-id><label>Table 1</label><caption>
<title>Application of masculinity/femininity ratings in various fields of research.</title>
</caption><alternatives><graphic id="pone-0099483-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Study</td>
<td align="left" rowspan="1" colspan="1">Reference</td>
<td align="left" rowspan="1" colspan="1">Subjects</td>
<td align="left" rowspan="1" colspan="1">Raters</td>
<td align="left" rowspan="1" colspan="1">Ratings</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Correlation between masculinity and trustworthiness/emotions</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Perrett1">[9]</xref></td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">40</td>
<td align="left" rowspan="1" colspan="1">480</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Relationship between masculinity/femininity and attractiveness as well as masculinity and distinctiveness</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Rhodes1">[3]</xref> <xref ref-type="bibr" rid="pone.0099483-Little2">[4]</xref></td>
<td align="left" rowspan="1" colspan="1">71</td>
<td align="left" rowspan="1" colspan="1">204</td>
<td align="left" rowspan="1" colspan="1">5036</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Relationship between masculinity/femininity and health</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Rhodes3">[10]</xref></td>
<td align="left" rowspan="1" colspan="1">310</td>
<td align="left" rowspan="1" colspan="1">37</td>
<td align="left" rowspan="1" colspan="1">11470</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Relationship between masculinity/femininity and symmetry.</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Koehler1">[13]</xref></td>
<td align="left" rowspan="1" colspan="1">194</td>
<td align="left" rowspan="1" colspan="1">39</td>
<td align="left" rowspan="1" colspan="1">5599</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Role of gender scores in sex classification of faces.</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Hoss1">[47]</xref></td>
<td align="left" rowspan="1" colspan="1">200</td>
<td align="left" rowspan="1" colspan="1">40</td>
<td align="left" rowspan="1" colspan="1">8000</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Relationship between sexual behaviour and masculinity/femininity</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Rhodes2">[5]</xref></td>
<td align="left" rowspan="1" colspan="1">362</td>
<td align="left" rowspan="1" colspan="1">109</td>
<td align="left" rowspan="1" colspan="1">40952</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Womens' preference and mate choice based on masculinity of men</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-DeBruine1">[48]</xref> <xref ref-type="bibr" rid="pone.0099483-Jones1">[7]</xref></td>
<td align="left" rowspan="1" colspan="1">40</td>
<td align="left" rowspan="1" colspan="1">20</td>
<td align="left" rowspan="1" colspan="1">800</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Relationship between masculinity and semen quality in men</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Peters1">[12]</xref></td>
<td align="left" rowspan="1" colspan="1">118</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">1416</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Relationship between sociosexuality and gender ratings</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Boothroyd1">[6]</xref></td>
<td align="left" rowspan="1" colspan="1">8+50</td>
<td align="left" rowspan="1" colspan="1">195+17</td>
<td align="left" rowspan="1" colspan="1">2410</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Role of masculinity in the functioning of a male endocrine system</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Pound1">[19]</xref></td>
<td align="left" rowspan="1" colspan="1">57</td>
<td align="left" rowspan="1" colspan="1">72</td>
<td align="left" rowspan="1" colspan="1">4104</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Role of masculinity and femininity in distinguishing homosexuals</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Rieger1">[49]</xref></td>
<td align="left" rowspan="1" colspan="1">95</td>
<td align="left" rowspan="1" colspan="1">58</td>
<td align="left" rowspan="1" colspan="1">5510</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Effects of syndrome on masculinity/femininity</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Bejerot1">[11]</xref></td>
<td align="left" rowspan="1" colspan="1">103</td>
<td align="left" rowspan="1" colspan="1">8</td>
<td align="left" rowspan="1" colspan="1">824</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Comparison between masculinity (attractiveness) and intelligence as cues for health and provision of resources in mate selection</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Lee1">[8]</xref></td>
<td align="left" rowspan="1" colspan="1">32</td>
<td align="left" rowspan="1" colspan="1">689</td>
<td align="left" rowspan="1" colspan="1">22048</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Evaluating the outcome of facial cosmetic surgery in terms of perceptual attractiveness; pre and post surgery</td>
<td align="left" rowspan="1" colspan="1"><xref ref-type="bibr" rid="pone.0099483-Chung1">[15]</xref> <xref ref-type="bibr" rid="pone.0099483-Dey1">[14]</xref></td>
<td align="left" rowspan="1" colspan="1">32; 20</td>
<td align="left" rowspan="1" colspan="1">163; 90</td>
<td align="left" rowspan="1" colspan="1">5216; 1800</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>Applications of perceptual gender ratings by employing human raters. Notice the huge number of ratings performed in case. References are provided for interested readers.</p></fn></table-wrap-foot></table-wrap>
<p>The process of perceptual gender rating in itself is both time and resource consuming and a challenging problem is to identify the nature of predictors or features that are employed by the human mind for this task. Some researchers have also investigated objective scores for sexual dimorphism (masculinity/femininity) using morphometric analysis <xref ref-type="bibr" rid="pone.0099483-Scheib1">[16]</xref>–<xref ref-type="bibr" rid="pone.0099483-Thornhill1">[18]</xref>. The key idea behind calculating objective masculinity index is to use facial measurements, like distances between biologically significant landmarks or ratios of these distances, for obtaining a score of facial masculinity/femininity. For each face these measures can be used individually or collectively by adding their standardised measures or their Z-scores.</p>
<p>Scheib et al. <xref ref-type="bibr" rid="pone.0099483-Scheib1">[16]</xref> obtained masculinity indices by summing up the standardized facial measures of the cheek-bone prominence and relative lower face length from grayscale pictures of 40 male subjects. The authors then asked 12 female participants to rate these faces for attractiveness. Interestingly, the masculinity index correlated positively with facial attractiveness (more masculine males were more attractive) which is against the established norms <xref ref-type="bibr" rid="pone.0099483-Rhodes1">[3]</xref>. In a similar study, Penton et al. <xref ref-type="bibr" rid="pone.0099483-PentonVoak1">[17]</xref> calculated five separate masculinity indices for each face using measures related to eye size, ratio of lower face height to total face height, cheek bone prominence, ratio of face width to lower face height and mean eyebrow height. Two dimensional pictures of 60 male and 49 female faces were used in this study. The authors did not find a correlation between these five dimorphic measurements and female-rated facial attractiveness. However, the rated attractiveness correlated positively with a composite masculinity index found by summing up the standardized Z-scores of the five individual measures. In a later study, Pound et al. <xref ref-type="bibr" rid="pone.0099483-Pound1">[19]</xref> used the same approach to calculate a composite facial masculinity index from 2D photographs of 57 male subjects. The study aimed at analysing the correlation between circulating testosterone levels and masculinity in males. Fifty seven male subjects were first asked to predict, by seeing the photographs, the outcome of a particular wrestler in six wrestling bouts. Subjects were then shown videos of the bouts allocated to a “winning” and “loosing” condition and pre/post task testosterone levels were measured. A group of 72 participants was then asked to rate the subjects for their perceived masculinity. The authors did not find any correlation between perceived masculinity and pre/post task testosterone levels. However, post task increase in testosterone levels correlated positively with the facial masculinity index. Note that, none of these studies explored the relationship between the perceived/rated masculinity and the objective facial masculinity index.</p>
<p>A more sophisticated method of obtaining the masculinity index is to first perform sex classification using discriminant analysis and then use the discriminant scores associated with each face as its masculinity index. One of the earlier attempts in that direction was made by Burton et al. <xref ref-type="bibr" rid="pone.0099483-Burton1">[20]</xref>. The authors performed sex classification on 179 faces using a set of 16 2D and 3D Euclidean facial distances as well as their ratios and angles. The discriminant function score of each face was taken as its masculinity index and the reported sex classification accuracy using Discriminant Function Analysis (DFA) was 94%. However, the authors could not find a positive correlation between their objective scores and the perceptual subjective scores obtained by asking 13 participants to rate the subjects' faces for masculinity/femininity. The correlation coefficient was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e001" xlink:type="simple"/></inline-formula> for male faces and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e002" xlink:type="simple"/></inline-formula> for female faces. In another study, Thornhill and Gangestad <xref ref-type="bibr" rid="pone.0099483-Thornhill1">[18]</xref> used DFA based on five measures of masculinity (chin length, jaw width, lip width, eye width, and eye height) to yield <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e003" xlink:type="simple"/></inline-formula> sex classification accuracy on 2D images of 295 subjects. Discriminant function scores were then used to measure facial masculinity. The authors then analysed the relationship between these masculinity scores and health in terms of respiratory diseases and their duration. There was a significant negative correlation for men and positive for women, between health and facial masculinity. Note that Rhodes et al. <xref ref-type="bibr" rid="pone.0099483-Rhodes3">[10]</xref> did not find any such correlation between perceived masculinity and the actual health of female subjects.</p>
<p>A similar technique was employed by Scott et al. <xref ref-type="bibr" rid="pone.0099483-Scott1">[21]</xref> to obtain a morphometric masculinity index. Two datasets of textured images of 20 male faces and 150 (75 male/75 female) faces were used for this purpose. Principle Component Analysis (PCA) was performed on 129 landmarks duly registered using Procrustes analysis and only 11 Principle Components (PCs) were retained. Using DFA, the authors classified facial sex with an accuracy of 96.8% in the first dataset and 98.7% in the second dataset. Discriminant function scores were used as the masculinity index. The relationship between these objective scores and perceived attractiveness was then analysed. The authors did not find any correlation between the male facial masculinity index and perceived attractiveness. However, the relationship between masculinity and attractiveness in female faces was significant and negative. Using the same approach, Stephen et al. <xref ref-type="bibr" rid="pone.0099483-Stephen1">[22]</xref> measured the masculinity index of 34 male participants using their 2D images. Interestingly, the authors found no correlation between their objective measure of sexual dimorphism and perceived attractiveness. Perhaps the absence of correlation is due to the fact that the authors have used 2D texture images in their experiments. Distances on 2D images are unable to model the facial surface accurately.</p>
<p>The above mentioned studies, on the one hand, highlight the importance of gender rating in evaluating various psychological and medical aspects in humans, and on the other hand, present the obvious difficulty in obtaining these scores. Our literature review shows that, so far, the methods employed for measuring objective masculinity/femininity scores fail to explain the underlying processes in perceptual gender scoring. That is why the objective scores obtained using these methods do not correlate well with subjective perceptual scores, making it difficult to use them instead of, or in combination with, perceptual scores in different studies. Note that, the main aim of these studies was to find relationship between different characteristics/attributes of the face with perceived (or objective) facial gender scores instead of looking for a direct relationship between their perceptual and objective facial masculinity/femininity. The requirement, therefore, is to understand the facial features used by humans to score the masculinity/femininity from faces and to evaluate the plausibility of reproducing these scores using objective measures. Once reliable objective measures are established, computer algorithms can be used to predict the perceived masculinity/femininity of a face with high confidence.</p>
<p>Understanding human perception or Human Visual System (HVS) for particular tasks has been of great interest to researchers (Note that, “Human Visual System” also refers to the anatomical structure of the visual system. However, throughout this paper we have used this term to refer to the cognitive mechanism employed by the human mind to perceptually asses and analyse visual information). Bruce et al. <xref ref-type="bibr" rid="pone.0099483-Bruce1">[23]</xref> performed Discriminant Function Analysis (DFA) for sex classification using 2D and 3D Euclidean distances extracted from 73 landmarks, the ratios of these distances and angles between them. The authors suggested that perhaps the human visual system takes into account a subset of 16 measurements to classify facial sex, since these features result in a classification accuracy of 94%. Similarly, to understand human and machine sex classification behaviour, Graf et al. <xref ref-type="bibr" rid="pone.0099483-Graf1">[24]</xref> used 2D images as stimuli to perform perceptual as well as computational sex classification. The authors asked human subjects to visually classify the 2D images for sex. Next, they used the Principle Components of the images and several state of the art classifiers to understand human internal decision space for sex classification.</p>
<p>To the best of our knowledge, there is no exclusive work on understanding the broad features used by HVS to give a measure to the degree of masculinity/femininity of the face. In the absence of such an understanding, the objective scores calculated by researchers, as evident from our survey, either do not correlate significantly with the perceptual scores or go against the established findings on relationship between perceived sexual dimorphism and other facial traits. This research gap has also resulted in the lack of development of robust algorithms for objective scoring of masculinity/femininity.</p>
<p>There are two major cues used by humans for facial sex classification: shape and appearance. Given the 3D nature of the face, a large amount of shape information gets lost in the 2D images of the face. On the contrary, a 3D face image, although more difficult to capture, has more shape-rich information. O'Toole et al. <xref ref-type="bibr" rid="pone.0099483-OToole1">[25]</xref> showed that 3D geometric information outperforms the texture in classifying sex of a face. Similarly, Bruce et al. <xref ref-type="bibr" rid="pone.0099483-Bruce2">[26]</xref> claimed that visually-derived semantic information like age, expression, gender etc. depend mainly on the geometric form of the perceived face. Therefore, we focus on using 3D geometric faces in this work to capture human perceptual ratings on gender. The main research questions that we want to address are the following:</p>
<list list-type="bullet"><list-item>
<p>Which geometric features are used by the HVS in perceiving the degree of gender of a 3D face?</p>
</list-item><list-item>
<p>Can a mathematical model mimic human performance and objectively rate the gender of a 3D face?</p>
</list-item></list>
<p>The answers to these questions will help in understanding facial sexual dimorphism and the diagnosis of related syndromes. In this study, we present 3D face models of 64 subjects in frontal, oblique and profile views to 75 raters to obtain perceptual ratings and analyse the physical features used by the raters to rate the faces. Next, we build a computational model based on the results of the perceptual study to objectively rate the gender using 3D Euclidean and geodesic features and their combinations. Using this model, we present our findings on the nature of geometric features used by the HVS in rating gender. Our results suggest that humans take into account a combination of 3D Euclidean and geodesic distances while perceiving the amount of sexual dimorphism in a face.</p>
</sec><sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>This study was performed at University of Western Australia (UWA) and Princess Margaret Hospital (PMH). All participants completed an informed consent form having been given written and verbal details of the tasks to be completed. The study was approved by the Princess Margaret Hospital Ethics Committee vide Approval Reference Number: 1532/EP. For developing the mathematical model for objective gender scores, the digital data was analysed anonymously. All identification features like the meta-data, texture etc. were stripped from the 3D images before hand.</p>
<sec id="s2a">
<title>Subjects</title>
<p>Images were obtained from participants recruited from the student body of UWA. 3D images of a total of 64 participants between the ages of 18 and 25, of varying population affinities, who had not undergone significant craniofacial surgery, and had no craniofacial abnormalities or injuries were captured for the current study. The self-reported population affinities were grouped into two categories of Europeans (Caucasian) and non-Europeans (‘Other’).</p>
<p>Fifty two percent of 64 subjects were females and 48% were males. 80% of the faces were Caucasian/European. The remaining 20% were allocated to the ethnicity category “other” which included Asians (n = 6), Blacks(n = 1), Anglo-Indian (n = 1), Eurasian (n = 2) and Indo-Chinese (n = 1). The majority (78%) of rated faces were of people between the ages of 18 and 21. Sixty eight percent of the rated subjects were born in Australia. Fourteen percent of these identified themselves as having an ethnicity other than Caucasian. The majority of the “other” group were born in Australia (46%), or in Asia (38%), the remainder having been born in Africa (n = 2). Caucasians born outside of Australia were born in Africa (n = 2), New Zealand (n = 6), and the UK (n = 6).</p>
</sec><sec id="s2b">
<title>Raters</title>
<p>Raters of a similar background to the imaged subjects were recruited from within and outside the student body at The University of Western Australia. These raters were also categorised as European/Caucasian or non-European/Other.</p>
<p>The panel of raters (n = 75) was composed of 40 females (53%) and 35 males (47%). Sixty four of the raters were Caucasian/European (84%). The majority, n = 48 (64%), of raters were aged between twenty one and twenty three, although the full age range extended from eighteen to twenty five. The mean age of the raters (21.9 years) was greater than that of the rated image subjects (19.9 years) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e004" xlink:type="simple"/></inline-formula>. Seventy seven percent of all raters were born in Australia. Seven percent of these identified themselves as having an ethnicity other than Caucasian/European. The majority of the ethnic group Other/non-European was born in Asia (58%), or in Australia (33%), the remainder having been born in Africa (n = 1). Europeans born outside of Australia were born in Asia (n = 2), New Zealand (n = 2), and the UK (n = 4).</p>
</sec><sec id="s2c">
<title>3D Facial Stereophotogrammetry</title>
<p>Three dimensional (3D) images of the faces of participants were captured using the 3dMDface 3D stereophotogrammetry system (3dMD LCC, Atlanta Georgia, USA). The 3dMDface system generates 180 degree (ear to ear) 3D images by employing the technique of triangulation. These high-resolution images are captured within 1.5 milliseconds (ms) <xref ref-type="bibr" rid="pone.0099483-Weinberg1">[27]</xref>. Image capture was undertaken in an office environment under standard clinic/office lighting conditions. Subjects were positioned so that imaging of the full face from ear to ear could be achieved. Images were taken of participants with faces holding a neutral expression, and jaws in centric relation with temporomandibular joint seated and natural dental contact without clenching force.</p>
</sec><sec id="s2d">
<title>Stimuli Preparation for Perceptual Scoring</title>
<p>Texture maps were stripped from the 3D images to remove features such as eyebrow shape and skin colour. Facial surface was smoothed to diminish the effects of skin texture and eyebrow coarseness. This is done in order to ensure that the raters' perceptions are based solely on facial geometry.</p>
<p>Processed images were prepared into individual packages of 20 randomly chosen faces for viewing on a visual display unit by each individual rater. Packages comprised equal number of males and females, drawn randomly from sex and population subgroups.</p>
</sec><sec id="s2e">
<title>Stimuli Preparation for Objective Scoring</title>
<p>We annotated 23 biologically significant landmarks <xref ref-type="bibr" rid="pone.0099483-Farkas1">[28]</xref> on each image as shown in <xref ref-type="fig" rid="pone-0099483-g002">Figure 2</xref>. The motivation for using these landmarks comes from the fact that they represent the sexual dimorphism of the face <xref ref-type="bibr" rid="pone.0099483-Farkas2">[29]</xref>. These landmarks and Euclidean distances measured from them are used to measure a quantitative dimension for the morphological deviation from the normal face <xref ref-type="bibr" rid="pone.0099483-Farkas1">[28]</xref>, to delineate syndromes <xref ref-type="bibr" rid="pone.0099483-Aldridge1">[30]</xref> and to measure objective masculinity/femininity <xref ref-type="bibr" rid="pone.0099483-Scott1">[21]</xref>. We have selected the facial landmarks that relate to the bony structure of the face which is effected by the ratio of testosterone to estrogen (oestrogen) during adolescence <xref ref-type="bibr" rid="pone.0099483-Bardin1">[31]</xref>. It is believed that facial masculinity is associated with levels of circulating testosterone in men <xref ref-type="bibr" rid="pone.0099483-Pound1">[19]</xref>. Hence it is intuitive to use features extracted from these bony landmarks for facial gender scoring.</p>
<fig id="pone-0099483-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g002</object-id><label>Figure 2</label><caption>
<title>Landmarks used in our algorithm.</title>
<p>23 landmarks annotated on a shaded textureless 3D image. The image is the average face of 10 male subjects from our database.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g002" position="float" xlink:type="simple"/></fig>
<p>The pose of each 3D face is corrected to a canonical form based on four landmarks (Ex(L), Ex(R), N and Prn). This step is required to eliminate any error due to pose in the extraction of geodesic distances which will be discussed in detail in the Study 2 of the Experiments Section. Holes are filled and noise removed by re-sampling the 3D face on a uniform grid using the gridfit <xref ref-type="bibr" rid="pone.0099483-DErico1">[32]</xref> algorithm. Since some portions of the face are expected to be self occluded (e.g. region around Ac) when re-sampled on a grid, we bisect the 3D face along the vertical axis at the nose tip and rotate each half by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e005" xlink:type="simple"/></inline-formula> before re-sampling to mitigate this problem. Besides hole filling, another advantage of bisecting and rotating the halves before re-sampling is that the resulting 3D face has a more uniform sampling in the 3D space. The processed halves are then rotated back and stitched seamlessly to form a single mesh. <xref ref-type="fig" rid="pone-0099483-g003">Figure 3</xref> shows the different preprocessing steps.</p>
<fig id="pone-0099483-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g003</object-id><label>Figure 3</label><caption>
<title>Different steps in preprocessing.</title>
<p>(A) The raw input face. (B) Bisected raw face rotated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e006" xlink:type="simple"/></inline-formula>. Notice the holes in the eye region. (C) Processed face. (D) Processed face stitched back seamlessly.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g003" position="float" xlink:type="simple"/></fig></sec><sec id="s2f">
<title>Evaluation Criteria</title>
<p>The main focus of this paper is to find geometric features that are used by HVS for rating gender. Since it is well known that texture itself is very informative on sex classification <xref ref-type="bibr" rid="pone.0099483-Burton1">[20]</xref>, we used textureless 3D rendered images to avoid any bias in the results due to texture. Abdi et al. <xref ref-type="bibr" rid="pone.0099483-Abdi1">[33]</xref> show that hair is one of the major contributors in sex classification. To avoid bias resulting from this feature, ratings were obtained on 3D images with the hair concealed or cropped.</p>
<p>Consequent to the above considerations, raters were asked to rate each of the 64 faces for perceived masculinity/femininity and nominate the facial regions they used for this judgement. A computational model was then developed based on this study to objectively score the gender. Our evaluation criterion is the correlation between perceptual ratings and objective scores from the model. Given two random variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e007" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e008" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e009" xlink:type="simple"/></inline-formula> samples each, their correlation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e010" xlink:type="simple"/></inline-formula> is defined as,<disp-formula id="pone.0099483.e011"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e011" xlink:type="simple"/><label>(1)</label></disp-formula></p>
<p>In each study, we depict the correlation for males and females in a plot. We also project the objective and subjective perceptual scores on a Bland-Altman plot <xref ref-type="bibr" rid="pone.0099483-MartinBland1">[34]</xref>. Bland et al. <xref ref-type="bibr" rid="pone.0099483-MartinBland1">[34]</xref> proposed a technique for comparing the outcome of two methods in clinical practice. They argue that a comparison between the average of the outcomes to the difference is a better way of assessing the agreement between two different methods. The Cartesian coordinates of the Bland-Altman plot <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e012" xlink:type="simple"/></inline-formula> are given by,<disp-formula id="pone.0099483.e013"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e013" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e014" xlink:type="simple"/></inline-formula> are the samples of each observation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e015" xlink:type="simple"/></inline-formula> belonging to male/female class.</p>
</sec><sec id="s2g">
<title>Perceptual Scoring</title>
<p>As mentioned earlier, the stimuli were prepared into individual packages of 20 randomly chosen faces for viewing on a visual display unit by each individual rater. The rater was unaware of the sex and population composition of the package. As shown in <xref ref-type="fig" rid="pone-0099483-g004">Figure 4</xref>, a series of five facial views: left profile, left oblique, straight, right oblique, right profile, were prepared for each subject and displayed on the screen. Raters were able to toggle between these images in making their ratings.</p>
<fig id="pone-0099483-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g004</object-id><label>Figure 4</label><caption>
<title>Facial views for perceptual rating.</title>
<p>Series of facial views of each subject shown to raters. From left to right: left profile, left oblique, straight, right oblique, right profile.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g004" position="float" xlink:type="simple"/></fig>
<p>Questionnaires were presented and filled out electronically while viewing the images on a second computer screen. Raters were asked to do the following</p>
<list list-type="bullet"><list-item>
<p>Fill out a personal information questionnaire detailing age, sex and population affinity.</p>
</list-item><list-item>
<p>View each face and rate the degree of masculinity or femininity of the face on a 20 point scale.</p>
</list-item><list-item>
<p>Nominate the facial regions that they used to make their judgement. The options available were forehead, eyes, nose, cheeks, mouth, chin, jaw and no specific features.</p>
</list-item><list-item>
<p>Identify the sex of the individual depicted.</p>
</list-item></list>
</sec><sec id="s2h">
<title>Objective Scoring</title>
<p>An overview of our gender scoring algorithm is given in <xref ref-type="fig" rid="pone-0099483-g005">Figure 5</xref>. Gender classification is an important prerequisite for obtaining objective gender scores. Using the annotated landmarks, 44 distances (22 each of the 3D Euclidean and geodesic) related to the regions indicated in <xref ref-type="table" rid="pone-0099483-t002">Table 2</xref> were extracted as features. <xref ref-type="fig" rid="pone-0099483-g006">Figure 6</xref> shows some of the features used. Further details on these features are given in the Experiments Section.</p>
<fig id="pone-0099483-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g005</object-id><label>Figure 5</label><caption>
<title>Block Diagram.</title>
<p>Block diagram of the proposed gender classification and scoring algorithm. For details see the Objective Scoring Section. The synthetic images are from <xref ref-type="bibr" rid="pone.0099483-Singular1">[45]</xref>, <xref ref-type="bibr" rid="pone.0099483-Blanz1">[46]</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g005" position="float" xlink:type="simple"/></fig><fig id="pone-0099483-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g006</object-id><label>Figure 6</label><caption>
<title>Features used in our algorithm.</title>
<p>Some of the 3D Euclidean (left) and geodesic (right) distances used in gender scoring algorithm.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g006" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0099483-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.t002</object-id><label>Table 2</label><caption>
<title>Significant facial features in perceptual gender scoring.</title>
</caption><alternatives><graphic id="pone-0099483-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Feature</td>
<td align="left" rowspan="1" colspan="1"><italic>x</italic><sup>2</sup></td>
<td align="left" rowspan="1" colspan="1">p</td>
<td align="left" rowspan="1" colspan="1">Masculinity/Femininity association</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Forehead</bold></td>
<td align="left" rowspan="1" colspan="1">5.28</td>
<td align="left" rowspan="1" colspan="1">0.071</td>
<td align="left" rowspan="1" colspan="1">No particular association</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Eyes</bold></td>
<td align="left" rowspan="1" colspan="1">23.69</td>
<td align="left" rowspan="1" colspan="1">&lt;0.001</td>
<td align="left" rowspan="1" colspan="1">Femininity</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Nose</bold></td>
<td align="left" rowspan="1" colspan="1">3.08</td>
<td align="left" rowspan="1" colspan="1">0.214</td>
<td align="left" rowspan="1" colspan="1">No particular association</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Cheeks</bold></td>
<td align="left" rowspan="1" colspan="1">36.39</td>
<td align="left" rowspan="1" colspan="1">&lt;0.001</td>
<td align="left" rowspan="1" colspan="1">Femininity</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Mouth</bold></td>
<td align="left" rowspan="1" colspan="1">23.63</td>
<td align="left" rowspan="1" colspan="1">&lt;0.001</td>
<td align="left" rowspan="1" colspan="1">Femininity</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Chin</bold></td>
<td align="left" rowspan="1" colspan="1">19.38</td>
<td align="left" rowspan="1" colspan="1">&lt;0.001</td>
<td align="left" rowspan="1" colspan="1">Masculinity</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Jaw</bold></td>
<td align="left" rowspan="1" colspan="1">58.29</td>
<td align="left" rowspan="1" colspan="1">&lt;0.001</td>
<td align="left" rowspan="1" colspan="1">Masculinity</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>No Spec</bold></td>
<td align="left" rowspan="1" colspan="1">2.97</td>
<td align="left" rowspan="1" colspan="1">0.227</td>
<td align="left" rowspan="1" colspan="1">No particular association</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p>Chi-square and propability values for the correlation between facial features and their use in rating masculinity/femininity.</p></fn></table-wrap-foot></table-wrap>
<p>We begin with feature selection which is a process of selecting the most relevant features for classification while removing the redundant ones. For this purpose we use the minimal redundancy maximal relevance (mRMR) algorithm packed in a forward-selection wrapper <xref ref-type="bibr" rid="pone.0099483-Peng1">[35]</xref>. The algorithm first calculates the intrinsic information (relevance) within a feature and also the mutual information (redundancy) among the features to segregate different classes. Then it maximizes the relevance and minimizes the redundancy simultaneously. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e016" xlink:type="simple"/></inline-formula> be the feature matrix with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e017" xlink:type="simple"/></inline-formula> observations and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e018" xlink:type="simple"/></inline-formula> features, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e019" xlink:type="simple"/></inline-formula> be the target reduced feature set and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e020" xlink:type="simple"/></inline-formula> be any arbitrary class from the set of classes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e021" xlink:type="simple"/></inline-formula>, then relevance is defined by,<disp-formula id="pone.0099483.e022"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e022" xlink:type="simple"/><label>(3)</label></disp-formula>and redundancy is defined by,<disp-formula id="pone.0099483.e023"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e023" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e024" xlink:type="simple"/></inline-formula> is the mutual information between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e025" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e026" xlink:type="simple"/></inline-formula>. Maximal relevance and minimal redundancy is obtained by taking the maximum and minimum values of (3) and (4) respectively. The goal of simultaneously maximizing the relevance and minimising the redundancy is achieved by maximizing the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e027" xlink:type="simple"/></inline-formula> where,<disp-formula id="pone.0099483.e028"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e028" xlink:type="simple"/><label>(5)</label></disp-formula>or<disp-formula id="pone.0099483.e029"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e029" xlink:type="simple"/><label>(6)</label></disp-formula>where <xref ref-type="disp-formula" rid="pone.0099483.e028">equation (5</xref>) is the Mutual Information Difference and <xref ref-type="disp-formula" rid="pone.0099483.e029">equation (6</xref>) is the Mutual Information Quotient formulation of mRMR algorithm. Since our feature set is small, we find the classification accuracy yielded by both formulations and use only the one giving the maximum accuracy on training data. The reduced number of candidate features <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e030" xlink:type="simple"/></inline-formula> is selected by first obtaining <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e031" xlink:type="simple"/></inline-formula> feature sets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e032" xlink:type="simple"/></inline-formula> using the mRMR sequential search (<xref ref-type="disp-formula" rid="pone.0099483.e028">Eq. 5</xref> or <xref ref-type="disp-formula" rid="pone.0099483.e029">6</xref> depending on which one gives better accuracy). More specifically <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e033" xlink:type="simple"/></inline-formula>. Next we compare the classification accuracy for all feature subsets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e034" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e035" xlink:type="simple"/></inline-formula> to find a range for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e036" xlink:type="simple"/></inline-formula> where the classification accuracy is maximum. Finally, we select a compact set of features by exploiting the forward-selection wrapper <xref ref-type="bibr" rid="pone.0099483-Kohavi1">[36]</xref>. The wrapper first searches for a single feature <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e037" xlink:type="simple"/></inline-formula> from the feature set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e038" xlink:type="simple"/></inline-formula> which gives the maximum classification accuracy. Then, from the subset <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e039" xlink:type="simple"/></inline-formula> we search for another feature such that the subset <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e040" xlink:type="simple"/></inline-formula> gives the maximum accuracy irrespective of the previous one. This is a deviation from the original mRMR algorithm <xref ref-type="bibr" rid="pone.0099483-Peng1">[35]</xref> which desires a feature subset that produces better or equal accuracy than the previous subset in order to minimize the number of evaluations due to the greater number of candidate features in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e041" xlink:type="simple"/></inline-formula>. Since our original feature set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e042" xlink:type="simple"/></inline-formula> contains fewer than 50 features and the size of candidate feature set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e043" xlink:type="simple"/></inline-formula> is even smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e044" xlink:type="simple"/></inline-formula>, therefore, we let the wrapper evaluate all possible subsets of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e045" xlink:type="simple"/></inline-formula> in a forward selection scheme enabling us to find the reduced feature subset that gives the best accuracy. Consequently, we obtain a feature set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e046" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e047" xlink:type="simple"/></inline-formula> and we select the feature subset <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e048" xlink:type="simple"/></inline-formula> which corresponds to the highest accuracy. Note that this is the most compact feature subset as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e049" xlink:type="simple"/></inline-formula>.</p>
<p>We train a Linear Discriminant Analysis (LDA) classifier using an exclusive set of training data. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e050" xlink:type="simple"/></inline-formula> be the matrix of features of class <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e051" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e052" xlink:type="simple"/></inline-formula> samples. LDA maximizes the ratio of <italic>between-class scatter</italic> to <italic>within-class scatter</italic>. Between-class scatter is defined as<disp-formula id="pone.0099483.e053"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e053" xlink:type="simple"/><label>(7)</label></disp-formula>and within-class scatter is defined as<disp-formula id="pone.0099483.e054"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e054" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e055" xlink:type="simple"/></inline-formula> is the mean of all classes, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e056" xlink:type="simple"/></inline-formula> is the mean of class <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e057" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e058" xlink:type="simple"/></inline-formula> is the number of samples in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e059" xlink:type="simple"/></inline-formula>. Fisher <xref ref-type="bibr" rid="pone.0099483-Duda1">[37]</xref> proposed to maximise the ratio between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e060" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e061" xlink:type="simple"/></inline-formula> relative to the projection direction by solving<disp-formula id="pone.0099483.e062"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e062" xlink:type="simple"/><label>(9)</label></disp-formula></p>
<p>By differentiating the equation with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e063" xlink:type="simple"/></inline-formula> and equating it to zero, we get <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e064" xlink:type="simple"/></inline-formula>, which is a generalized eigenvalue problem and the eigenvector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e065" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e066" xlink:type="simple"/></inline-formula> is the desired optimal direction. Given the learnt LDA projection <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e067" xlink:type="simple"/></inline-formula>, a query face is classified into one of the two genders. The projection of feature vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e068" xlink:type="simple"/></inline-formula> (of a face with unknown gender) on the LDA space is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e069" xlink:type="simple"/></inline-formula>.</p>
<p>Gender classification is performed based on the distance between the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e070" xlink:type="simple"/></inline-formula> and the means of the projected classes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e071" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e072" xlink:type="simple"/></inline-formula> such that<disp-formula id="pone.0099483.e073"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e073" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e074" xlink:type="simple"/></inline-formula></p>
<p>Interestingly, the directional distance of a projected test face from the center of the projected means of the two classes gives an intuitive insight into the amount of masculinity or femininity of the face. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e075" xlink:type="simple"/></inline-formula> be the center of the projected means.The gender score <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e076" xlink:type="simple"/></inline-formula> of a test face <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e077" xlink:type="simple"/></inline-formula>, whose gender has already been determined with <xref ref-type="disp-formula" rid="pone.0099483.e073">Eqn. 10</xref>, is defined as<disp-formula id="pone.0099483.e078"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e078" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e079" xlink:type="simple"/></inline-formula> is the projected mean of either class (1 or 2) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e080" xlink:type="simple"/></inline-formula> is a scaling factor for comparability with the available human perceptual ratings. In our case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e081" xlink:type="simple"/></inline-formula>. Hence we score the gender on a scale of 0 to 20 (0 being most masculine and 20 being most feminine). <xref ref-type="fig" rid="pone-0099483-g007">Figure 7</xref> illustrates the process of scoring the gender of a query face in the LDA projected space.</p>
<fig id="pone-0099483-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g007</object-id><label>Figure 7</label><caption>
<title>Gender scoring in LDA projected space.</title>
<p>Diagram depicting the process of objectively scoring the gender in LDA space to assign a metric for masculinity/femininity of the test face.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g007" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Results and Analysis</title>
<sec id="s3a">
<title>Perceptual Scoring</title>
<p>While ratings of masculinity/femininity were clearly bimodal (<xref ref-type="fig" rid="pone-0099483-g008">Figure 8</xref>) with most males rated at the lower one third of the scale, and most females in the upper one third, a substantial proportion of images (29%) were rated in the middle one third, or perceived to be ambiguously masculine/feminine. The ratings from all the 75 raters were found to be significantly consistent (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e082" xlink:type="simple"/></inline-formula>) using the Fleiss Agreement Test <xref ref-type="bibr" rid="pone.0099483-Fleiss1">[38]</xref>.</p>
<fig id="pone-0099483-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g008</object-id><label>Figure 8</label><caption>
<title>The perceptual subjective gender scores.</title>
<p>A histogram of mean perceptual masculinity and femininity ratings obtained from 75 raters.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g008" position="float" xlink:type="simple"/></fig>
<p>The sex and ethnicity of the person represented in images had a significant influence on how they were rated by all groups <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e083" xlink:type="simple"/></inline-formula>. In general the perceived masculinity or femininity of the imaged subject was independent of the background of the person doing the rating. Both European male and female faces were considered to be more masculine than their non-European counterparts.</p>
<p>There was a strong tendency for the chin and jaw to be nominated as significant indicators in judgements of faces rated as extremely masculine (ratings 0 to 4), while the eyes, cheeks and mouth were the most frequently nominated features used in judgements of faces receiving high femininity ratings (ratings 15–20). <xref ref-type="table" rid="pone-0099483-t002">Table 2</xref> gives the detailed test values for each feature.</p>
<p>Gender was correctly identified in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e084" xlink:type="simple"/></inline-formula> of the instances. All sex and ethnic groups had the same ability to identify gender overall <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e085" xlink:type="simple"/></inline-formula>. Raters were adept at correctly identifying sex for their own ethnic group (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e086" xlink:type="simple"/></inline-formula> correct). Raters were slightly better at identifying the sex of the dominant culture when they were a minority born amongst the dominants than if they were a member of the dominant culture trying to identify the sex of one of the minorities (Europeans<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e087" xlink:type="simple"/></inline-formula> correct; non-Europeans<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e088" xlink:type="simple"/></inline-formula>). Europeans were better at classifying the sex of non-Europeans (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e089" xlink:type="simple"/></inline-formula> correct) than non-Europeans were at classifying the sex of Europeans (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e090" xlink:type="simple"/></inline-formula>) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e091" xlink:type="simple"/></inline-formula>. Gender identification errors were more likely to be made amongst female faces (23% wrong) than amongst male faces (10% wrong) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e092" xlink:type="simple"/></inline-formula>. In particular, there was a strong tendency for female Europeans to be wrongly identified as males (29% wrong), while male Europeans (5% wrong) were very unlikely to be mistaken for females <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e093" xlink:type="simple"/></inline-formula>. Correctly identified females were perceived to be significantly more feminine than those that were mistaken for males <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e094" xlink:type="simple"/></inline-formula>. Correctly identified males were perceived as more masculine than those mistaken for females <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e095" xlink:type="simple"/></inline-formula>. The ability to identify sex did not improve with the number of faces that were viewed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e096" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s3b">
<title>Objective Scoring</title>
<sec id="s3b1">
<title>Study 1: Euclidean Measurements.</title>
<p>Our first study constitutes obtaining objective gender scores using 3D Euclidean distances. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e097" xlink:type="simple"/></inline-formula> be the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e098" xlink:type="simple"/></inline-formula> landmark. The 3D Euclidean distance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e099" xlink:type="simple"/></inline-formula> between landmarks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e100" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e101" xlink:type="simple"/></inline-formula> is defined as,<disp-formula id="pone.0099483.e102"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0099483.e102" xlink:type="simple"/><label>(12)</label></disp-formula></p>
<p><xref ref-type="fig" rid="pone-0099483-g006">Figure 6(Left)</xref> shows some of the 3D Euclidean distances used in this experiment.</p>
<p>Using 3D Euclidean distances as features, our proposed algorithm classifies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e103" xlink:type="simple"/></inline-formula> subjects correctly as males or females. The correlation between objective gender scores and the perceptual scores is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e104" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e105" xlink:type="simple"/></inline-formula> for males and females respectively. <xref ref-type="fig" rid="pone-0099483-g009">Figure 9(a &amp; b</xref>, first row) show the correlation and best fit line for males and females while <xref ref-type="fig" rid="pone-0099483-g009">Figure 9(c, first row)</xref> shows the Bland-Altman plot between the objective and perceptual subjective scores.</p>
<fig id="pone-0099483-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g009</object-id><label>Figure 9</label><caption>
<title>Results of objective gender scoring.</title>
<p>(A) Correlation for males. (B) Correlation for females. (C) Cumulative Bland-Altman plot. Correlation and Bland-Altman plots between objective and subjective gender scores for males and females using only 3D Euclidean distances (First Row), only geodesic distances (second row) and combination of Euclidean and geodesic distances (third row).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g009" position="float" xlink:type="simple"/></fig>
<p>It is evident that objective scores for masculinity and femininity do not correlate well with the perceptual subjective scores. In <xref ref-type="fig" rid="pone-0099483-g009">Figure 9(c, first row)</xref> ideally the mean of the difference of objective and subjective gender scores should have been zero. However, we can see that the mean difference line is well above zero and the width of the limits of agreement in this case is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e106" xlink:type="simple"/></inline-formula>.</p>
<p>Clearly, 3D Euclidean distances do not seem to be the features that HVS concentrates on while scoring the facial gender. However, it is interesting to note that the forehead width (Ft-Ft), nasal bridge length (N-Prn), nasal tip protrusion (Sn-Prn), nasal width (Al-Al) and chin height (Sto-Pg) are selected as the most differentiating features by our algorithm (see <xref ref-type="fig" rid="pone-0099483-g010">Figure 10(a)</xref>). This is in line with the findings of Burton et al. <xref ref-type="bibr" rid="pone.0099483-Burton1">[20]</xref> who performed experiments on a subset of 2D and 3D Euclidean distances. Note that the authors handpicked these features based on knowledge from existing literature, whereas our approach relies on a mathematical feature selection algorithm. This endorses the mathematical model we use for obtaining discriminant features.</p>
<fig id="pone-0099483-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g010</object-id><label>Figure 10</label><caption>
<title>Most discriminating features between males and females found in the three experiments.</title>
<p>(A) Euclidean distances only. (B) Geodesic distances only. (C) Combined Euclidean and geodesic distances.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g010" position="float" xlink:type="simple"/></fig>
</sec>
<sec id="s3b2">
<title>Study 2: Geodesic Measurements.</title>
<p>In the second study, we use geodesic distances to predict the facial gender scores. Some examples of the geodesics can be seen in <xref ref-type="fig" rid="pone-0099483-g006">Figure 6(Right)</xref>. We define geodesic distance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e107" xlink:type="simple"/></inline-formula> between points <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e108" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e109" xlink:type="simple"/></inline-formula> as the length of the curve <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e110" xlink:type="simple"/></inline-formula> generated by orthogonal projection of the Euclidean line <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e111" xlink:type="simple"/></inline-formula> on the 3D facial surface. This is precisely the reason for normalising the pose of each 3D face as variation in pose can present a different surface to the viewing angle. Less curved distances like the upper lip height (Sn-Sto) are modelled by a second order polynomial while more curved distances with multiple inflection points, like the biocular width (Ex-Ex) are modelled by higher order polynomials. Studies suggest that geodesic distances may represent 3D models in a better way as compared to 3D Euclidean distances <xref ref-type="bibr" rid="pone.0099483-Hamza1">[39]</xref>. Gupta et al. <xref ref-type="bibr" rid="pone.0099483-Gupta1">[40]</xref> argue that algorithms based on geodesic distances are likely to be robust to changes in facial expressions. In support of this argument Bronstein et al.<xref ref-type="bibr" rid="pone.0099483-Bronstein1">[41]</xref> have suggested that facial expressions can be modelled as isometric deformations of the 3D surface where intrinsic properties of the surface like geodesic distances are preserved. <xref ref-type="fig" rid="pone-0099483-g011">Figure 11</xref> depicts the variation in 3D Euclidean and geodesic distances in biocular width on two models. The left model has a protuberant nose and hence a larger geodesic distance than the right model which has a flatter nose. Euclidean distance in both the models is similar. <xref ref-type="fig" rid="pone-0099483-g012">Figure 12(a)</xref> shows some of the extracted geodesic features and <xref ref-type="fig" rid="pone-0099483-g012">Figure 12(b–c)</xref> show the process of fitting a polynomial to these features.</p>
<fig id="pone-0099483-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g011</object-id><label>Figure 11</label><caption>
<title>Robustness of geodesic distances to facial expression.</title>
<p>Geodesic and 3D Euclidean distances of biocular width shown on two models. Left model has a protuberant nose and hence a greater geodesic distance than the right model which has a flatter nose. Euclidean distance in both the models is similar.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g011" position="float" xlink:type="simple"/></fig><fig id="pone-0099483-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0099483.g012</object-id><label>Figure 12</label><caption>
<title>Modelling of geodesic curves.</title>
<p>(A) Geodesic curves for nasal bridge length(N-Prn) and biocular width(Ex-Ex). (B–C) Fitting polynomials to these curves. Notice that N-Prn is modelled by a fourth order curve while Ex-Ex is modelled by a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e112" xlink:type="simple"/></inline-formula> order curve.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0099483.g012" position="float" xlink:type="simple"/></fig>
<p>Geodesic distances classify facial sex with an accuracy of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e113" xlink:type="simple"/></inline-formula>. The correlation between objective gender scores and the perceptual subjective scores also increases to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e114" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e115" xlink:type="simple"/></inline-formula> for males and females respectively. <xref ref-type="fig" rid="pone-0099483-g009">Figure 9(a &amp; b</xref>, second row) show the correlation and best fit line for males and females while <xref ref-type="fig" rid="pone-0099483-g009">Figure 9(c</xref>, second row) shows the Bland-Altman plot between the objective and perceptual subjective scores.</p>
<p>Even though the correlation has improved, the geodesic distances alone do not seem to be the features of choice for HVS while ascribing a score to facial gender. <xref ref-type="fig" rid="pone-0099483-g009">Figure 9(c</xref>, second row) shows that the mean of the difference is still well above zero and the width of limits of agreement in this case is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e116" xlink:type="simple"/></inline-formula>. Once again the forehead width (Ft-Ft), nasal bridge length (N-Prn), nasal width (Al-Prn-Al) and chin height (Sto-Pg) are amongst the most differentiating features. However, with geodesic distances, the upper lip height (Sn-Sto), eye fissure length (Ex-En) and intracanthal width (En-En) are added as the most discriminating sex classification features (see <xref ref-type="fig" rid="pone-0099483-g010">Figure 10(b)</xref>).</p>
</sec>
<sec id="s3b3">
<title>Study 3: Combined Measurements.</title>
<p>In the last experiment, we use a combination of 3D Euclidean and geodesic distances as our features for gender scoring. Since most of the gender discriminating features are common between the two families of distances, it seems intuitive to combine them and analyse their effect.</p>
<p>Equipped with a combination of 3D Euclidean and geodesic distances, our algorithm classifies facial sex with an accuracy of 99.93%. There is also a significant boost in the correlation between the objective and subjective gender scores which now is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e117" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e118" xlink:type="simple"/></inline-formula> for males and females respectively. The Bland-Altman plot shows the mean of the difference between the two scoring methods to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e119" xlink:type="simple"/></inline-formula> while the width of limits of agreement is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e120" xlink:type="simple"/></inline-formula>. This is a significant improvement as compared to the previous experiments. <xref ref-type="fig" rid="pone-0099483-g009">Figure 9(a &amp; b</xref>, third row) show the correlation and best fit line for males and females while <xref ref-type="fig" rid="pone-0099483-g009">Figure 9(c, third row)</xref> shows the Bland-Altman plot between the objective and perceptual subjective scores.</p>
<p>The most differentiating features between the two sexes are once again common between the two families of distances. The Euclidean and geodesic distances for forehead width (Ft-Ft), nasal bridge length (N-Prn), nasal width (Al-Al), eye fissure length (Ex-En), chin height (Sto-Pg) and upper lip height (Sn-Sto) are the most discriminating features in our algorithm (see <xref ref-type="fig" rid="pone-0099483-g010">Figure 10(c)</xref>). However, this time the forehead height (Tr-G) is added to the list of discriminating features.</p>
<p>The above results suggest that the human visual system looks at the combination of Euclidean and geodesic distances between certain features on the face to give a gender score.</p>
</sec></sec></sec><sec id="s4">
<title>General Discussion</title>
<p>In the three studies involving various families of features, we have tried to find the predictors that the human visual system uses to attribute a measure to the facial gender. Beginning with 3D Euclidean distances alone, we see that there is little correlation between objective gender scores and subjective scores. This situation improves slightly when geodesic distances are used. The reason is straight forward as geodesic distances can model the facial surface curvature better than the Euclidean distances. However, the results are still below an acceptable significance threshold. Finally, when we use a combination of Euclidean and geodesic distances we see that the correlation between the two methods of scoring improves significantly and so does the agreement between them. This seems to corroborate the claim of Bruce et al. <xref ref-type="bibr" rid="pone.0099483-Bruce1">[23]</xref> that humans use a combination of predictors to perceive the sex of a face. Furthermore, as is evident from <xref ref-type="fig" rid="pone-0099483-g010">Figure 10(c)</xref>, the most discriminating features from both families of distances seem to be common. This indicates that HVS might actually be taking into consideration the ratio between 3D Euclidean and geodesic distances while making a decision on the gender score of a face.</p>
<p>Relating the sex classification results to the gender scores in the three studies gives us a very interesting clue. In all three studies, sex classification results are very impressive. In fact, the base accuracy of 94.21% using only the 3D Euclidean distances tends to agree with the findings of Burton et al.<xref ref-type="bibr" rid="pone.0099483-Burton1">[20]</xref> who classified facial sex with 94% accuracy using 2D and 3D Euclidean distances. However, the objective gender scores obtained with this family of distances do not significantly agree with the perceptual scores. Even when the classification results improve to 98.57% using the geodesics, the correlation between the objective and subjective gender scores remains below an acceptable significance threshold. This trend changes significantly when a combination of the two families of distances is used as predictors even though the sex classification results improve by 1.36% only. It shows that even though facial sex can be classified accurately using only the 3D Euclidean or geodesic distances, perfect and more meaningful gender scores can only be obtained when a combination or ratio of these distances are taken as features for gender scoring.</p>
<p>Commenting on the method of obtaining gender scores, it is observed that a classification algorithm is a necessary prerequisite. However, the scoring result itself is invariant to the sex classification accuracy. This is evident from the gender scores obtained for females in the three experiments. There are a few female subjects who score below the boundary line of 10 giving them a more masculine gender score. This is indicative of a failure in classifying their sex but correlates very well with the perceptual subjective scores. Therefore, even though the algorithm misclassifies their sex, it still gives them a meaningful gender score which tends to agree with the subjective scores. Hence, our proposed algorithm puts the facial gender in the category of a continuum rather than binary.</p>
<p>From the Categorical Perception (CP) point of view, our results corroborate the findings of Armann and Bülthoff <xref ref-type="bibr" rid="pone.0099483-Armann1">[42]</xref>, that there is no evidence for naturally occurring CP for the sex of faces. Results of perceptual scoring, although bimodal, show that the gender ratings are on a continuum and do not follow a decision boundary. Consequently, a few female subjects were rated more masculine, hence crossing the decision boundary. This trend was replicated by our proposed computational model which ascribes the correct gender scores to even those subjects which fall on the other side of the decision boundary. Furthermore, the participants in Armann and Bülthoff's study <xref ref-type="bibr" rid="pone.0099483-Armann1">[42]</xref> show a consistent bias to judge faces as male rather than female. Our findings from perceptual sex classification replicated this observation as we found a strong tendency for female Europeans to be wrongly identified as males (29% wrong), while male Europeans (5% wrong) were very unlikely to be mistaken for females <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e121" xlink:type="simple"/></inline-formula>.</p>
<p>Our choice of features was motivated by the results from perceptual scoring. Instead of taking <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e122" xlink:type="simple"/></inline-formula> combinations of distances, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0099483.e123" xlink:type="simple"/></inline-formula> is the number of landmarks, we developed our model around the facial features that our raters indicated were instrumental in giving a score. It is evident from <xref ref-type="fig" rid="pone-0099483-g010">Figure 10(c)</xref> that our algorithm also selects the features that were significant in subjective perceptual scoring. However, distances relating to the jaw (Go-Go) and mouth (Ch-Ch) were not highly discriminating. While there is no plausible reason for the mouth width (Ch-Ch) to be excluded from the list, mandible width (Go-Go) may have been excluded due to localization error of the related landmarks. Gonions (Go,L and Go,R) are a palpable landmarks indicating the extremes of the jaw and as such are very difficult to annotate consistently on 3D images.</p>
<p>Facial rating for attractiveness and sexual dimorphism plays an important role in planning reconstructive and cosmetic surgery. This procedure depends on a number of physiological and psychological constraints, like, age, sex, health state, structure, shape of the face and patient's needs and expectations. Patients who undergo such procedure are rated by human observers pre and post surgery to assess any improvement in perceptual attractiveness <xref ref-type="bibr" rid="pone.0099483-Dey1">[14]</xref>, <xref ref-type="bibr" rid="pone.0099483-Chung1">[15]</xref>. With the development of 3D simulation techniques to preview the aesthetical results of facial cosmetic surgery <xref ref-type="bibr" rid="pone.0099483-Gao1">[43]</xref>, our proposed algorithm can assist in predicting the attractiveness of the surgical outcome as it correlates significantly with human perceptual results. For example, secondary rhinoplasty is a nose operation carried out to correct or revise an unsatisfactory outcome from a previous rhinoplasty <xref ref-type="bibr" rid="pone.0099483-Bracaglia1">[44]</xref>. Lee et al. <xref ref-type="bibr" rid="pone.0099483-Lee1">[8]</xref> have proposed a three-dimensional (3D) surgical simulation system, which can assist surgeons in planning rhinoplasty procedures. Our proposed algorithm can be used in such cases to assess the improvement in facial attractiveness of the resulting rhinoplasty through gender scoring, thus reducing the chances of further secondary procedures.</p>
<p>We can conclude by claiming that our proposed algorithm helps us in a better understanding of the Human Visual System. It is the first algorithm that has such a significantly high correlation with the mean perceptual scores given by 75 raters on 64 subjects. Hence, it may be possible to use these gender scores in a myriad of applications in medical and psychological fields where human raters are employed to obtain these scores.</p>
</sec></body>
<back><ref-list>
<title>References</title>
<ref id="pone.0099483-Little1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Little</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>BC</given-names></name>, <name name-style="western"><surname>DeBruine</surname><given-names>LM</given-names></name> (<year>2011</year>) <article-title>The many faces of research on face perception</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>366</volume>: <fpage>1634</fpage>–<lpage>1637</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Leopold1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leopold</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Rhodes</surname><given-names>G</given-names></name> (<year>2010</year>) <article-title>A comparative view of face perception</article-title>. <source>Journal of Comparative Psychology</source> <volume>124</volume>: <fpage>233</fpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Rhodes1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rhodes</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Hickford</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Jeffery</surname><given-names>L</given-names></name> (<year>2000</year>) <article-title>Sex-typicality and attractiveness: Are supermale and superfemale faces super-attractive</article-title>? <source>British Journal of Psychology</source> <volume>91</volume>: <fpage>125</fpage>–<lpage>140</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Little2"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Little</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Hancock</surname><given-names>PJ</given-names></name> (<year>2002</year>) <article-title>The role of masculinity and distinctiveness in judgments of human male facial attractiveness</article-title>. <source>British Journal of Psychology</source> <volume>93</volume>: <fpage>451</fpage>–<lpage>464</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Rhodes2"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rhodes</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Simmons</surname><given-names>LW</given-names></name>, <name name-style="western"><surname>Peters</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Attractiveness and sexual behavior: Does attractiveness enhance mating success</article-title>? <source>Evolution and Human Behavior</source> <volume>26</volume>: <fpage>186</fpage>–<lpage>201</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Boothroyd1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boothroyd</surname><given-names>LG</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>BC</given-names></name>, <name name-style="western"><surname>Burt</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>DeBruine</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Perrett</surname><given-names>DI</given-names></name> (<year>2008</year>) <article-title>Facial correlates of sociosexuality</article-title>. <source>Evolution and Human Behavior</source> <volume>29</volume>: <fpage>211</fpage>–<lpage>218</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Jones1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jones</surname><given-names>BC</given-names></name>, <name name-style="western"><surname>Feinberg</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Watkins</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Fincher</surname><given-names>CL</given-names></name>, <name name-style="western"><surname>Little</surname><given-names>AC</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Pathogen disgust predicts womens preferences for masculinity in mens voices, faces, and bodies</article-title>. <source>Behavioral Ecology</source> <volume>24</volume>: <fpage>373</fpage>–<lpage>379</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Lee1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Dubbs</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Kelly</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>von Hippel</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Brooks</surname><given-names>RC</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Human facial attributes, but not perceived intelligence, are used as cues of health and resource provision potential</article-title>. <source>Behavioral Ecology</source> <volume>24</volume>: <fpage>779</fpage>–<lpage>787</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Perrett1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perrett</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Penton-Voak</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Rowland</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Yoshikawa</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>1998</year>) <article-title>Effects of sexual dimorphism on facial attractiveness</article-title>. <source>Nature</source> <volume>394</volume>: <fpage>884</fpage>–<lpage>887</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Rhodes3"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rhodes</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Chan</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zebrowitz</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Simmons</surname><given-names>LW</given-names></name> (<year>2003</year>) <article-title>Does sexual dimorphism in human faces signal health</article-title>? <source>Proceedings of the Royal Society of London B: Biological Sciences</source> <volume>270</volume>: <fpage>S93</fpage>–<lpage>S95</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Bejerot1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bejerot</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Eriksson</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Bonde</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Carlström</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Humble</surname><given-names>MB</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>The extreme male brain revisited: gender coherence in adults with autism spectrum disorder</article-title>. <source>The British Journal of Psychiatry</source> <volume>201</volume>: <fpage>116</fpage>–<lpage>123</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Peters1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peters</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Rhodes</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Simmons</surname><given-names>L</given-names></name> (<year>2008</year>) <article-title>Does attractiveness in men provide clues to semen quality</article-title>? <source>Journal of Evolutionary Biology</source> <volume>21</volume>: <fpage>572</fpage>–<lpage>579</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Koehler1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koehler</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Simmons</surname><given-names>LW</given-names></name>, <name name-style="western"><surname>Rhodes</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Peters</surname><given-names>M</given-names></name> (<year>2004</year>) <article-title>The relationship between sexual dimorphism in human faces and fluctuating asymmetry</article-title>. <source>Proceedings of the Royal Society of London B: Biological Sciences</source> <volume>271</volume>: <fpage>S233</fpage>–<lpage>S236</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Dey1"><label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Dey JK, Ishii M, Boahene K, Byrne PJ, Ishii LE (2013) Changing perception: Facial reanimation surgery improves attractiveness and decreases negative facial perception. The Laryngoscope.</mixed-citation>
</ref>
<ref id="pone.0099483-Chung1"><label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Chung EH, Borzabad-Farahani A, Yen SLK (2013) Clinicians and laypeople assessment of facial attractiveness in patients with cleft lip and palate treated with lefort i surgery or late maxillary protraction. International Journal of Pediatric Otorhinolaryngology.</mixed-citation>
</ref>
<ref id="pone.0099483-Scheib1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scheib</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Gangestad</surname><given-names>SW</given-names></name>, <name name-style="western"><surname>Thornhill</surname><given-names>R</given-names></name> (<year>1999</year>) <article-title>Facial attractiveness, symmetry and cues of good genes</article-title>. <source>Proceedings of the Royal Society of LondonB: Biological Sciences</source> <volume>266</volume>: <fpage>1913</fpage>–<lpage>1917</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-PentonVoak1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Penton-Voak</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Little</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Baker</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Tiddeman</surname><given-names>B</given-names></name>, <etal>et al</etal>. (<year>2001</year>) <article-title>Symmetry, sexual dimorphism in facial proportions and male facial attractiveness</article-title>. <source>Proceedings of the Royal Society of London B: Biological Sciences</source> <volume>268</volume>: <fpage>1617</fpage>–<lpage>1623</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Thornhill1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thornhill</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gangestad</surname><given-names>SW</given-names></name> (<year>2006</year>) <article-title>Facial sexual dimorphism, developmental stability, and susceptibility to disease in men and women</article-title>. <source>Evolution and Human Behavior</source> <volume>27</volume>: <fpage>131</fpage>–<lpage>144</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Pound1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pound</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Penton-Voak</surname><given-names>IS</given-names></name>, <name name-style="western"><surname>Surridge</surname><given-names>AK</given-names></name> (<year>2009</year>) <article-title>Testosterone responses to competition in men are related to facial masculinity</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source> <volume>276</volume>: <fpage>153</fpage>–<lpage>159</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Burton1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burton</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Bruce</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Dench</surname><given-names>N</given-names></name> (<year>1993</year>) <article-title>What's the difference between men and women? Evidence from facial measurement</article-title>. <source>Perception</source> <volume>22</volume>: <fpage>153</fpage>–<lpage>176</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Scott1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scott</surname><given-names>IM</given-names></name>, <name name-style="western"><surname>Pound</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Stephen</surname><given-names>ID</given-names></name>, <name name-style="western"><surname>Clark</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Penton-Voak</surname><given-names>IS</given-names></name> (<year>2010</year>) <article-title>Does masculinity matter? the contribution of masculine face shape to male attractiveness in humans</article-title>. <source>PLoS one</source> <volume>5</volume>: <fpage>e13585</fpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Stephen1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephen</surname><given-names>ID</given-names></name>, <name name-style="western"><surname>Scott</surname><given-names>IM</given-names></name>, <name name-style="western"><surname>Coetzee</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Pound</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Perrett</surname><given-names>DI</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Cross-cultural effects of color, but not morphological masculinity, on perceived attractiveness of men's faces</article-title>. <source>Evolution and Human Behavior</source> <volume>33</volume>: <fpage>260</fpage>–<lpage>267</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Bruce1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bruce</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Burton</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Hanna</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Healey</surname><given-names>P</given-names></name> (<year>1993</year>) <article-title>Sex discrimination: how do we tell the difference between male and female faces</article-title>? <source>Perception</source> <volume>22</volume>: <fpage>131</fpage>–<lpage>152</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Graf1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graf</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Wichmann</surname><given-names>FA</given-names></name>, <name name-style="western"><surname>Bülthoff</surname><given-names>HH</given-names></name>, <name name-style="western"><surname>Schölkopf</surname><given-names>BH</given-names></name> (<year>2006</year>) <article-title>Classification of faces in man and machine</article-title>. <source>Neural Computation</source> <volume>18</volume>: <fpage>143</fpage>–<lpage>165</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-OToole1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Toole</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Vetter</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Troje</surname><given-names>NF</given-names></name>, <name name-style="western"><surname>Bülthoff</surname><given-names>HH</given-names></name> (<year>1997</year>) <article-title>Sex classification is better with three-dimensional head structure than with image intensity information</article-title>. <source>Perception</source> <volume>26</volume>: <fpage>75</fpage>–<lpage>84</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Bruce2"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bruce</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Young</surname><given-names>A</given-names></name> (<year>1986</year>) <article-title>Understanding face recognition</article-title>. <source>British journal of psychology</source> <volume>77</volume>: <fpage>305</fpage>–<lpage>327</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Weinberg1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weinberg</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Naidoo</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Govier</surname><given-names>DP</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Kane</surname><given-names>AA</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Anthropometric precision and accuracy of digital three-dimensional photogrammetry: comparing the genex and 3dmd imaging systems with one another and with direct anthropometry</article-title>. <source>Journal of Craniofacial Surgery</source> <volume>17</volume>: <fpage>477</fpage>–<lpage>483</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Farkas1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Farkas L (1994) Anthropometry of the head and face in clinical practice. Anthropometry of the Head and Face, 2nd Ed: 71–111.</mixed-citation>
</ref>
<ref id="pone.0099483-Farkas2"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Farkas</surname><given-names>LG</given-names></name>, <name name-style="western"><surname>Kolar</surname><given-names>JC</given-names></name> (<year>1987</year>) <article-title>Anthropometrics and art in the aesthetics of women's faces</article-title>. <source>Clinics in Plastic Surgery</source> <volume>14</volume>: <fpage>599</fpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Aldridge1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aldridge</surname><given-names>K</given-names></name>, <name name-style="western"><surname>George</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Cole</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Austin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Takahashi</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Facial phenotypes in subgroups of prepubertal boys with autism spectrum disorders are correlated with clinical phenotypes</article-title>. <source>Molecular Autism</source> <volume>2</volume>: <fpage>15</fpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Bardin1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bardin</surname><given-names>CW</given-names></name>, <name name-style="western"><surname>Catterall</surname><given-names>JF</given-names></name> (<year>1981</year>) <article-title>Testosterone: A major determinant of extragenital sexual dimorphism</article-title>. <source>Science</source> <volume>211</volume>: <fpage>1285</fpage>–<lpage>1294</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-DErico1"><label>32</label>
<mixed-citation publication-type="other" xlink:type="simple">DErico J (2008) Surface fitting using gridfit. Technical report, MATLAB Central File Exchange.</mixed-citation>
</ref>
<ref id="pone.0099483-Abdi1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abdi</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Valentin</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Edelman</surname><given-names>B</given-names></name>, <name name-style="western"><surname>O'Toole</surname><given-names>AJ</given-names></name> (<year>1995</year>) <article-title>More about the difference between men and women: evidence from linear neural network and the principal-component approach</article-title>. <source>Perception</source> <volume>24</volume>: <fpage>539</fpage>–<lpage>539</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-MartinBland1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martin Bland</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Altman</surname><given-names>D</given-names></name> (<year>1986</year>) <article-title>Statistical methods for assessing agreement between two methods of clinical measurement</article-title>. <source>The Lancet</source> <volume>327</volume>: <fpage>307</fpage>–<lpage>310</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Peng1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peng</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Long</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Ding</surname><given-names>C</given-names></name> (<year>2005</year>) <article-title>Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</source> <volume>27</volume>: <fpage>1226</fpage>–<lpage>1238</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Kohavi1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kohavi</surname><given-names>R</given-names></name>, <name name-style="western"><surname>John</surname><given-names>GH</given-names></name> (<year>1997</year>) <article-title>Wrappers for feature subset selection</article-title>. <source>Artificial Intelligence</source> <volume>97</volume>: <fpage>273</fpage>–<lpage>324</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Duda1"><label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Duda R, Hart P, Stork D (2001) Pattern Classification and Scene Analysis 2nd ed.</mixed-citation>
</ref>
<ref id="pone.0099483-Fleiss1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fleiss</surname><given-names>JL</given-names></name> (<year>1971</year>) <article-title>Measuring nominal scale agreement among many raters</article-title>. <source>Psychological Bulletin</source> <volume>76</volume>: <fpage>378</fpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Hamza1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hamza</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Krim</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>Geodesic matching of triangulated surfaces</article-title>. <source>IEEE Transactions on Image Processing</source> <volume>15</volume>: <fpage>2249</fpage>–<lpage>2258</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Gupta1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gupta</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Markey</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bovik</surname><given-names>A</given-names></name> (<year>2010</year>) <article-title>Anthropometric 3D face recognition</article-title>. <source>International Journal of Computer Vision</source> <volume>90</volume>: <fpage>331</fpage>–<lpage>349</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Bronstein1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bronstein</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bronstein</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kimmel</surname><given-names>R</given-names></name> (<year>2005</year>) <article-title>Three-dimensional face recognition</article-title>. <source>International Journal of Computer Vision</source> <volume>64</volume>: <fpage>5</fpage>–<lpage>30</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Armann1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Armann</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bülthoff</surname><given-names>I</given-names></name> (<year>2012</year>) <article-title>Male and female faces are only perceived categorically when linked to familiar identities–and when in doubt, he is a male</article-title>. <source>Vision research</source> <volume>63</volume>: <fpage>69</fpage>–<lpage>80</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Gao1"><label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Gao J, Zhou M, Wang H, Zhang C (2001) Three dimensional surface warping for plastic surgery planning. In: IEEE International Conference on Systems, Man, and Cybernetics. IEEE, volume 3, pp. 2016–2021.</mixed-citation>
</ref>
<ref id="pone.0099483-Bracaglia1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bracaglia</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Fortunato</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gentileschi</surname><given-names>S</given-names></name> (<year>2005</year>) <article-title>Secondary rhinoplasty</article-title>. <source>Aesthetic Plastic Surgery</source> <volume>29</volume>: <fpage>230</fpage>–<lpage>239</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Singular1"><label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Singular Inversions Facegen Modeller. Available: <ext-link ext-link-type="uri" xlink:href="http://www.facegen.com/" xlink:type="simple">http://www.facegen.com/</ext-link>. Accessed 26 May 2014.</mixed-citation>
</ref>
<ref id="pone.0099483-Blanz1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blanz</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Vetter</surname><given-names>T</given-names></name> (<year>2003</year>) <article-title>Face recognition based on fitting a 3D morphable model</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <volume>25</volume>: <fpage>1063</fpage>–<lpage>1074</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Hoss1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoss</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Ramsey</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Griffin</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Langlois</surname><given-names>JH</given-names></name> (<year>2005</year>) <article-title>The role of facial attractiveness and facial masculinity/femininity in sex classification of faces</article-title>. <source>Perception</source> <volume>34</volume>: <fpage>1459</fpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-DeBruine1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeBruine</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>BC</given-names></name>, <name name-style="western"><surname>Little</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Boothroyd</surname><given-names>LG</given-names></name>, <name name-style="western"><surname>Perrett</surname><given-names>DI</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Correlated preferences for facial masculinity and ideal or actual partner's masculinity</article-title>. <source>Proceedings of the Royal Society B: Biological Sciences</source> <volume>273</volume>: <fpage>1355</fpage>–<lpage>1360</lpage>.</mixed-citation>
</ref>
<ref id="pone.0099483-Rieger1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rieger</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Linsenmeier</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Gygax</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Garcia</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Bailey</surname><given-names>JM</given-names></name> (<year>2010</year>) <article-title>Dissecting “gaydar”: Accuracy and the role of masculinity–femininity</article-title>. <source>Archives of Sexual Behavior</source> <volume>39</volume>: <fpage>124</fpage>–<lpage>140</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>